{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b24c-9882-492e-9af8-69d824226db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import shutil\n",
    "import warnings\n",
    "import sys\n",
    "# import glob\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from scipy import stats\n",
    "# import scipy.io\n",
    "import csv\n",
    "import xlsxwriter\n",
    "from statistics import mode\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import openpyxl as xl\n",
    "from copy import copy\n",
    "# from copy import deepcopy\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# import statsmodels.api as sm\n",
    "# from bioinfokit.analys import stat\n",
    "# from statsmodels.formula.api import ols\n",
    "# import pingouin as pg\n",
    "# from statsmodels.graphics.factorplots import interaction_plot\n",
    "# from brokenaxes import brokenaxes\n",
    "# import matplotlib.patheffects as pe\n",
    "from matplotlib import container\n",
    "# import concurrent.futures\n",
    "# from itertools import starmap\n",
    "# import multiprocessing as mp\n",
    "# from brokenaxes import brokenaxes\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "import tkinter as tk\n",
    "import tkinter.filedialog as fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6e3df-8182-4f68-83c2-a229760def4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hide these warnings\n",
    "warnings.filterwarnings(action='ignore', message='.*converting a masked element to nan.*')\n",
    "\n",
    "# Some plotting presets\n",
    "# plt.rc(\"errorbar\", capsize=3)\n",
    "plt.rc(\"figure\", dpi=70)\n",
    "plt.rc(\"savefig\", dpi=40, facecolor=\"white\", bbox=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab1536-3f37-438c-a4d9-b3cbeaaccab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print python version\n",
    "python_version = sys.version\n",
    "print(python_version)\n",
    "\n",
    "## Choose analysis files\n",
    "initial_dir = \"C:\\\\\"\n",
    "print(\"Please select the files to analyze:\")\n",
    "\n",
    "time.sleep(0.25)\n",
    "root = tk.Tk()\n",
    "root.filenames = fd.askopenfilenames(initialdir=initial_dir,\n",
    "                                     title=\"Please select the files to analyze\",\n",
    "                                     filetypes=((\"csv files\", \"*.csv\"), (\"all files\", \"*.*\")))\n",
    "input_files = [os.path.basename(f) for f in root.filenames]\n",
    "input_dir = os.path.dirname(root.filenames[0])\n",
    "root.destroy()\n",
    "\n",
    "print(\"Please choose the output folder:\")\n",
    "time.sleep(0.25)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.filenames = fd.askdirectory(initialdir=input_dir,\n",
    "                                 title=\"Please select the output folder\")\n",
    "output_dir = root.filenames\n",
    "root.destroy()\n",
    "\n",
    "os.chdir(input_dir)\n",
    "print('Working directory is:', input_dir, '\\n')\n",
    "print('Output folder is:', output_dir,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ef523-b85d-4d72-bee5-46c521419b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Couple each file with its date of creation\n",
    "messydatesdict = OrderedDict()\n",
    "for fl in input_files:\n",
    "    if '_spike_' not in fl:\n",
    "        with open(fl, encoding='UTF-8') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            row = list(csv_reader)\n",
    "            for lin in row[75:76]:\n",
    "                if any('Analysis Duration (s):' in e.strip() for e in lin):\n",
    "                    duration = round(float(lin[1])/2)\n",
    "            for lin in row[5:7]:\n",
    "                if any('Original File Time' in e.strip() for e in lin):\n",
    "                    stamp = lin[1].split(' ')[0]\n",
    "                    clock = lin[1].split(' ')[1]\n",
    "                    year = int(stamp.split('/')[2])\n",
    "                    month = int(stamp.split('/')[0])\n",
    "                    day = int(stamp.split('/')[1])\n",
    "                    hour = int(clock.split(':')[0])\n",
    "                    minute = int(clock.split(':')[1])\n",
    "                    second = int(clock.split(':')[2])\n",
    "                    messydatesdict[fl] = datetime(year,month,day,hour,minute,second) + timedelta(seconds=duration)\n",
    "\n",
    "# Sort file names by their date of creation\n",
    "messydatesdict = dict(sorted(messydatesdict.items(), key=lambda x: datetime.strptime(str(x[1]), '%Y-%m-%d %H:%M:%S')))\n",
    "for k, v in messydatesdict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646461e-b861-4740-9606-da1cafd06a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"clean\" file names\n",
    "cleandatesdict = OrderedDict()\n",
    "datesforaveragemerging = OrderedDict()\n",
    "for key, value in messydatesdict.items():\n",
    "    new_key = key.split('.')[0].replace('(000)(000)', '').replace('(000)', '').replace('(00', ' ').replace('(0', ' ').replace(')', '')\n",
    "    cleandatesdict[new_key] = str(value)\n",
    "    datesforaveragemerging[new_key] = value\n",
    "acutedatesdict = {}\n",
    "for k, v in cleandatesdict.items():\n",
    "    if 'bl' in k or 'ac' in k:\n",
    "        acutedatesdict[k] = v \n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe7015-f61e-45cf-93b3-e9799e20b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# append desired parameters' data to dataList from the advanced metrics files\n",
    "rowsChulas = np.r_[67,69,71,72,168:186 + 1] - 1\n",
    "dataList = []\n",
    "duration_in_sec = False\n",
    "durations = []\n",
    "for fl in messydatesdict.keys():\n",
    "    with open(fl, encoding='UTF-8') as csv_file:\n",
    "        temporary = csv.reader(csv_file, delimiter=',')\n",
    "        for i, row_content in enumerate(temporary):\n",
    "            if 'Analysis Duration (s): ' in row_content:\n",
    "                durations.append(float(row_content[1]))\n",
    "print(durations)\n",
    "for fl in messydatesdict.keys():\n",
    "#     if fl.split('.')[-1] == 'csv' and '_' not in fl:            \n",
    "    with open(fl, encoding='UTF-8') as csv_file:\n",
    "        singleCSV = []\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for i, row_content in enumerate(csv_reader):\n",
    "            if 'Analysis Duration (s): ' in row_content:\n",
    "                duration_in_sec = float(row_content[1])\n",
    "            if i in rowsChulas:\n",
    "                if row_content[0] == 'Treatment':\n",
    "                    row_content[0] = 'Treatment - Well'\n",
    "                    singleCSV.append(row_content)\n",
    "                elif (row_content[0] == 'Number of Spikes' or row_content[0] == 'Number of Bursts' or\n",
    "                row_content[0] == 'Number of Network Bursts' or row_content[0] == 'Inter-Burst Interval - Avg (s)' or\n",
    "                row_content[0] == 'Inter-Burst Interval - Std (s)'):\n",
    "                    if duration_in_sec >= mode(durations):\n",
    "                        # pseudo-normalize by time span of the recording\n",
    "                        singleCSV.append([round(float(x) / (duration_in_sec/mode(durations))) if x != '' else '' for x in row_content[1:]]) # divide by regular time span of the recordings\n",
    "                        singleCSV[-1].insert(0, row_content[0])\n",
    "                    else:\n",
    "                        singleCSV.append(row_content)\n",
    "                elif row_content[0] == 'Measurement':\n",
    "                    row_content[0] = 'Electrode'\n",
    "                    singleCSV.append(row_content)\n",
    "                else:\n",
    "                    singleCSV.append(row_content)\n",
    "        dataList.append(singleCSV)\n",
    "\n",
    "for i, row_content in enumerate(dataList):\n",
    "    dataList[i][2][1:] = [c + ' ' + t if c != '' else t for t, c in zip(dataList[i][2][1:], dataList[i][3][1:])]\n",
    "    dataList[i].insert(3, ['Treatment - Electrode'] + [dataList[i][2][a+1] for a, w in enumerate(dataList[i][0][1:]) for e in dataList[i][4][1:] if e[:2] == w])\n",
    "    dataList[i].insert(2, ['Electrode Coloring'] + [dataList[i][1][a+1] for a, w in enumerate(dataList[i][0][1:]) for e in dataList[i][5][1:] if e[:2] == w])\n",
    "    \n",
    "'''\n",
    "In the future extract the concentration of the second compound used in the well from the name too\\n\n",
    "(use \"after the + symbol\" regular expression to find it) and add it to the \"concentration\" row in dataList\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e12d9-916d-493c-8b3a-0ab657c1b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually define a dictionary of shortened parameter names for the excel sheet names\n",
    "\n",
    "excel_friendly = {\n",
    "                'Well':'Well',\n",
    "                'Well Coloring':'Well Coloring',\n",
    "                'Electrode Coloring':'Electrode Coloring',\n",
    "                'Treatment - Well':'Treatment - Well',\n",
    "                'Treatment - Electrode':'Treatment - Electrode',\n",
    "                'Concentration':'Concentration',\n",
    "                'Electrode':'Electrode',\n",
    "                'Number of Spikes':'Number_of_Spikes', # getting normalization and acute treatment values from average for all 11 minutes periods\n",
    "                'Mean Firing Rate (Hz)':'Mean_Firing_Rate_Hz',\n",
    "                'ISI Coefficient of Variation':'ISI_Coefficient_of_Variation',\n",
    "                'Number of Bursts':'Number_of_Bursts', # getting normalization and acute treatment values from average for all 11 minutes periods\n",
    "                'Burst Duration - Avg (s)':'Burst_Duration_Avg_s',\n",
    "                'Burst Duration - Std (s)':'Burst_Duration_Std_s',\n",
    "                'Number of Spikes per Burst - Avg':'Spikes_per_Burst_Avg',\n",
    "                'Number of Spikes per Burst - Std':'Spikes_per_Burst_Std',\n",
    "                'Mean ISI within Burst - Avg':'Mean_ISI_within_Burst_Avg',\n",
    "                'Mean ISI within Burst - Std':'Mean_ISI_within_Burst_Std',\n",
    "                'Median ISI within Burst - Avg':'Median_ISI_within_Burst_Avg',\n",
    "                'Median ISI within Burst - Std':'Median_ISI_within_Burst_Std',\n",
    "                'Inter-Burst Interval - Avg (s)':'Inter_Burst_Interval_Avg_s',\n",
    "                'Inter-Burst Interval - Std (s)':'Inter_Burst_Interval_Std_s',\n",
    "                'Burst Frequency (Hz)':'Burst_Frequency_Hz',\n",
    "                'IBI Coefficient of Variation':'IBI_Coefficient_of_Variation',\n",
    "                'Normalized Duration IQR':'Normalized Duration IQR',\n",
    "                'Burst Percentage':'Burst_Percentage',\n",
    "                }\n",
    "\n",
    "# Check all names are <=31 chars long\n",
    "for i in excel_friendly.values():\n",
    "    if len(i) > 31:\n",
    "        print(i+ ' is too long of an excel worksheet name')\n",
    "print('Number of electrode advanced metrics =', len(excel_friendly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc5a3d-726b-4e54-89a9-2d02b8539130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign each row in dataList to its day in cleandatesdict, then load it into the \"data\" dictionary\n",
    "data = defaultdict(dict)\n",
    "\n",
    "for day, ds in zip(cleandatesdict.keys(), dataList):\n",
    "    for row in ds:\n",
    "        key = excel_friendly[row[0]]\n",
    "        data[day][key] = row[1:]\n",
    "\n",
    "for day in cleandatesdict.keys():\n",
    "#     keys = list(data[day].keys())\n",
    "#     vals = list(data[day].values())\n",
    "    idx = np.argsort(data[day]['Treatment - Electrode'])#merging treatment and concentration values within Treatment key\n",
    "print('\\nIndex of sorted electrode treatments is: ', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebc7d1-375d-4e6d-bdee-9fd55797d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first time point that received a treatment and the last that did not, and save their names as \"baseline\" and \"acute\", respectively\n",
    "# Use when there is only one baseline and one acute time point\n",
    "baseline = False\n",
    "day_of_treatment = False\n",
    "if any(\"bl\" in k.lower() for k in data.keys()) or any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "    for i, day in enumerate(data):\n",
    "        for hexcolor in data[day]['Well Coloring']:\n",
    "            if hexcolor != '#00FF00' and 'bl' in day:\n",
    "                baseline = day\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "if any(\"ac\" in k.lower() for k in data.keys()):\n",
    "    acute = [k for k in list(cleandatesdict.keys()) if 'ac' in k and 'ac(' not in k][0]\n",
    "    day_of_treatment = acute.split(' ')[1].replace('div','')\n",
    "    print('Acute treatment administered in:', acute)\n",
    "print('Baseline is:', baseline, '\\nDay of treatment is:', day_of_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a44208-bd7d-4f8b-94d2-8f9ee023fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative time from each time point to the acute treatment (time 0) if present\n",
    "def_labels = {}\n",
    "realistic_x_axis = {}\n",
    "if any(\"ac\" in k.lower() for k in data.keys()):\n",
    "    act = datetime.strptime(cleandatesdict[acute], '%Y-%m-%d %H:%M:%S')\n",
    "else:\n",
    "    act = datetime.strptime(list(cleandatesdict.values())[0], '%Y-%m-%d %H:%M:%S')\n",
    "for key in cleandatesdict.keys():\n",
    "    now = datetime.strptime(cleandatesdict[key], '%Y-%m-%d %H:%M:%S')\n",
    "    diff = now - act\n",
    "    hours = diff.days*24 + diff.seconds/3600\n",
    "    minutes = abs(hours*60) % 60 # abs not working the same for pos and neg values, that is why abs(hours). No negative sign in the pre-acute time points for this reason\n",
    "    seconds = abs(hours*3600) % 60\n",
    "    def_labels[key] = [\"{}: {}\".format(key, \"%d%s%02d%s%02d%s\" % (hours,'h ',minutes,'min ',seconds,'sec'))]\n",
    "    realistic_x_axis[key] = hours\n",
    "\n",
    "print('def_labels:\\n')\n",
    "for k, v in def_labels.items():\n",
    "    print(k+':\\t', \",\".join(v))\n",
    "\n",
    "print('\\nrealistic_x_axis:\\n')\n",
    "for k, v in realistic_x_axis.items():\n",
    "    print(k+':\\t', v, 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bd620-a901-46ef-97a6-65949fc963f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the average of all baseline timepoints and append baseline values for normalization to norms dictionary if there is at least one 'bl' item in \"data\".\n",
    "if any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "    baselines = [key for key in data.keys() if 'pretreatment' in key.lower()]\n",
    "else:\n",
    "    baselines = [key for key in data.keys() if 'bl' in key.lower()]\n",
    "for element in baselines: # This is the place to do the averaging of all the baseline time points\n",
    "    norms = dict()\n",
    "    for parameter, quantity in data[element].items(): \n",
    "        if parameter not in ['Well', 'Well Coloring', 'Electrode Coloring', 'Treatment - Well', 'Treatment - Electrode', 'Concentration', 'Electrode']:\n",
    "            norms[parameter] = [] # As it is written now, \"norms\" will contain the data from the last baseline timepoint (usually bl(011) in a batch of 12)\n",
    "            for val in quantity:\n",
    "                try:\n",
    "                    norms[parameter].append(float(val))\n",
    "                except ValueError:\n",
    "                    norms[parameter].append(None)\n",
    "\n",
    "# Then calculate the average of all acute treatment timepoints\n",
    "acutes = [key for key in data.keys() if 'ac' in key.lower()]\n",
    "# average and save as a unique \"ac\" point\n",
    "baselines, acutes, day_of_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6213e3-ab96-441c-9190-3ae202a4dc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create excel workbook\n",
    "excelName = os.path.join(output_dir, 'Tables_elec_single.xlsx')\n",
    "with xlsxwriter.Workbook(excelName, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(total=len(data.values()), desc=\"Writing advanced metrics workbook rows on \" + str(time.ctime(time.time()))) as pbar:\n",
    "        styles = dict()\n",
    "        params = data[list(cleandatesdict.keys())[0]]\n",
    "        sheets_names = []\n",
    "        for key in params.keys():\n",
    "            if key not in ['Well', 'Well Coloring', 'Electrode Coloring', 'Treatment - Well', 'Treatment - Electrode', 'Concentration', 'Electrode']:\n",
    "                sheets_names.append(key)\n",
    "        for name in sheets_names:\n",
    "            workbook.add_worksheet(name)\n",
    "            workbook.add_worksheet(name='n{}'.format(name))\n",
    "        sheets = [sh for sh in workbook.worksheets() if sh.name in sheets_names]\n",
    "        norm_sheets = [sh for sh in workbook.worksheets() if sh.name not in sheets_names]\n",
    "        for i, day_data in enumerate(data.values()):\n",
    "            pbar.update(1)\n",
    "            for color, treat in zip(day_data['Electrode Coloring'], day_data['Treatment - Electrode']):\n",
    "                if not treat in styles:\n",
    "                    styles[treat] = workbook.add_format({'bg_color': color})\n",
    "                #styles[''] = workbook.add_format({'bg_color': '#00FF00'})#define not-treated condition and color\n",
    "            for sh in sheets:\n",
    "                sh.write(i + 2, 0, list(cleandatesdict.keys())[i])\n",
    "                sh.write(1, 0, 'Treatment')\n",
    "                sh.write(0, 0, 'Electrode')\n",
    "                for j in range(len(day_data['Electrode'])):\n",
    "                    sh.write(0, j + 1, day_data['Electrode'][idx[j]])\n",
    "                    treatment = day_data['Treatment - Electrode'][idx[j]]\n",
    "                    sh.write(1, j + 1, treatment)\n",
    "                    raw = day_data[sh.name][idx[j]]\n",
    "                    try:\n",
    "                        val = float(raw)\n",
    "                        if val == 0:\n",
    "                            val = 0\n",
    "                    except ValueError as e:\n",
    "                        # print(str(e).replace(':', ' in') + \" {}, {}, {}\".format(list(cleandatesdict.keys())[i],\n",
    "                        #                                     sh.name,\n",
    "                        #                                     day_data['Well'][idx[j]]))\n",
    "                        val = 0\n",
    "                    sh.write(i + 2, j + 1, val, styles[day_data['Treatment - Electrode'][idx[j]]])\n",
    "            \n",
    "            # write normalized values\n",
    "            for sh in norm_sheets:\n",
    "                sh.write(i + 2, 0, list(cleandatesdict.keys())[i])\n",
    "                sh.write(1, 0, 'Treatment')\n",
    "                sh.write(0, 0, 'Electrode')\n",
    "                for j in range(len(day_data['Electrode'])):\n",
    "                    sh.write(0, j + 1, day_data['Electrode'][idx[j]])\n",
    "                    treatment = day_data['Treatment - Electrode'][idx[j]]\n",
    "                    sh.write(1, j + 1, treatment)\n",
    "                    raw = day_data[sh.name[1:]][idx[j]]\n",
    "                    try:\n",
    "                        if \"pretreatment\" in baselines[0].lower() and 'bl' not in list(cleandatesdict.keys())[i]:\n",
    "                            val = float(raw) / norms[sh.name[1:]][idx[j]]\n",
    "                        elif \"pretreatment\" in baselines[0].lower() and 'bl' in list(cleandatesdict.keys())[i]:\n",
    "                            val = float(raw)\n",
    "                        else: # if no pretreatment present\n",
    "                            val = float(raw) / norms[sh.name[1:]][idx[j]]                            \n",
    "                        if val == 0 or val > 4:\n",
    "                            val = 0\n",
    "                    except ValueError:\n",
    "                        val = 0\n",
    "                    except TypeError:\n",
    "                        val = 0\n",
    "                    except ZeroDivisionError: # if a well has active electrodes\n",
    "#                          in the baseline, a ZeroDivisioError will prompt when\n",
    "#                          trying to normalize (will try to divide float by 0)\n",
    "#                          print(\"{} {} {} {}\".format('Cannot divide',str(raw),'by',str(norms[sh.name[1:]][idx[j]])))\n",
    "                        val = 0\n",
    "                    sh.write(i + 2, j + 1, val, styles[day_data['Treatment - Electrode'][idx[j]]])\n",
    "print('Created', excelName)\n",
    "# \"\"\"A FileNotFound error prompting here means the name is too long for an excel workbook and it could not be initialized\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47701f8f-f7d8-4dc2-98bd-004b9dde89a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Now working on the spike amplitude files...'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfd673-d4a7-4de9-b123-b3b1513a0e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Couple each spike_list file with its creation date. messydatesdict_amp needs to be a dict in order to sort the file names by date\n",
    "messydatesdict_amp = {}\n",
    "for fl in tqdm_notebook([f for f in input_files if '_spike_list' in f],\n",
    "                        desc = \"Retrieving timestamp from spike_list files on \" + str(time.ctime(time.time()))):\n",
    "        with open(fl) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file) # execution time in this step is limited by disk data transfer speed,\n",
    "            # specially by the size of the bl and ac spike_list files\n",
    "            rows = list(csv_reader)\n",
    "            for lin in rows[3:6]:\n",
    "                if lin[0] == '   Original File Time':\n",
    "                    messydatesdict_amp[fl]= lin[1]\n",
    "\n",
    "# Sort full file names by date of creation\n",
    "filenames = []\n",
    "for n, d in sorted(messydatesdict_amp.items(), key=lambda x: datetime.strptime(x[1], '%m/%d/%Y %H:%M:%S')):\n",
    "    filenames.append(n)\n",
    "\n",
    "for i in list(filenames[:6]):\n",
    "    print(i)\n",
    "print('\\netc...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2933fd8-f1cc-41bb-8a08-78c00de3b710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create data structures\n",
    "nombresListAmps = []\n",
    "ampdata = dict()\n",
    "electrode_info = dict()\n",
    "electrodes = dataList[0][6][1:]\n",
    "\n",
    "# add ampdata to structures\n",
    "for fname in tqdm_notebook(filenames, desc = \"Coupling amplitude values with their day, electrode, treatment and treatment concentration on \" + str(time.ctime(time.time()))):\n",
    "    # append short file names to \"nombresListAmps\"\n",
    "    if 'on' in fname and not bool(re.search('\\(00[1-9]\\)|\\([0-9][1-9][0-9]\\)|\\([1-9][0-9][0-9]\\)', fname)):\n",
    "        day = fname.split('.')[0].replace('(000)', ' 0').replace('_spike_list', '')\n",
    "    else:    \n",
    "        day = fname.split('.')[0].replace('(000)', '').replace('_spike_list', '').replace('(00', ' ').replace('(0', ' ').replace(')', '')\n",
    "    print(day)\n",
    "    nombresListAmps.append(day)\n",
    "    with open(fname, encoding='UTF-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',') # execution time in this step is limited by disk data transfer speed,\n",
    "            # specially by the size of the bl and ac spike_list files\n",
    "        ampdata[day] = dict()\n",
    "        ampdata[day]['Electrode'] = []\n",
    "        ampdata[day]['Amplitude'] = []\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i > 0:\n",
    "                try:\n",
    "                    ampdata[day]['Electrode'].append(row[3]) # retrieve full electrode names\n",
    "                    ampdata[day]['Amplitude'].append(float(row[4]))\n",
    "                except IndexError:\n",
    "                    break\n",
    "\n",
    "        wells, coloring, treatment, concentration = None, None, None, None # well info is at the bottom of the table, below the last amplitude values\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            try:\n",
    "                if row[0] == 'Well':\n",
    "                    wells = row[1:]\n",
    "                if row[0] == 'Well Coloring':\n",
    "                    well_coloring = row[1:]\n",
    "                if row[0] == 'Treatment':\n",
    "                    well_treatments = row[1:]\n",
    "                if row[0] == 'Concentration':\n",
    "                    well_concentrations = row[1:]\n",
    "            except IndexError:\n",
    "                break\n",
    "            \n",
    "        # coloring = [well_coloring[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        # treatment = [well_treatments[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        # concentration = [well_concentrations[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "            \n",
    "        for i, row in enumerate(csv_reader): # try again, necessary in files with very few amplitude values (headers are longer than values list)\n",
    "            try:\n",
    "                if row[0] == 'Well':\n",
    "                    wells = row[1:]\n",
    "                if row[0] == 'Well Coloring':\n",
    "                    well_coloring = row[1:]\n",
    "                if row[0] == 'Treatment':\n",
    "                    well_treatments = row[1:]\n",
    "                if row[0] == 'Concentration':\n",
    "                    well_concentrations = row[1:]\n",
    "            except IndexError:\n",
    "                break\n",
    "                \n",
    "        coloring = [well_coloring[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        treatment = [well_treatments[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        concentration = [well_concentrations[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        \n",
    "        electrode_info[day] = dict(zip(electrodes, zip(coloring, treatment, concentration)))\n",
    "print('\\nCreated \"ampdata\" and \"electrode_info\" dictionaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06d343-6c7f-4168-8751-10cd001fdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting amplitude values from mV to μV and assigning them to their electrode of origin\n",
    "electrode_data = dict()\n",
    "for day, amp_day_data in tqdm_notebook(ampdata.items(), desc = \"Converting each day\\'s amplitude values to μV and assigning them to each electrode on \" + str(time.ctime(time.time()))):\n",
    "    electrode_data[day] = defaultdict(list, {e:[] for e in electrodes})\n",
    "    for el, amp in zip(amp_day_data['Electrode'], amp_day_data['Amplitude']):\n",
    "        electrode_data[day][el].append(amp*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d31b0-90aa-40bd-a8e8-f592018208d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting columns by treatment and concentration\n",
    "if any(\"ac\" in k.lower() for k in electrode_data.keys()): # if acute treatment is present in days list\n",
    "    treatments = np.array([(e, conc+' '+t) if conc != '' else (e, t) for e, (color, t, conc) in electrode_info[nombresListAmps[-1]].items()], dtype=str).T # retrieve info from the last recording\n",
    "    idx = np.argsort(treatments)[1]\n",
    "else:\n",
    "    print('This experiment does not have an acute treatment point, must be a development experiment or badly named')\n",
    "    treatments = np.array([(e, t) for e, (color, t, conc) in electrode_info[list(electrode_info.items())[0][0]].items()], dtype=str).T\n",
    "    idx = np.argsort(treatments)[1]\n",
    "\n",
    "print('\\nEach electrode\\'s treatments are: ',  treatments[1])\n",
    "print('\\nIndex of sorted electrode treatments is: ', idx)\n",
    "print('\\nElectrode treatments sorted by final table arrangement: ', treatments[1][idx])\n",
    "\n",
    "col_numbers = dict(zip(treatments[0][idx], range(len(idx))))\n",
    "print('Sorted columns by treatment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ba046-a3d5-4369-bb2a-134eb2bb1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define excel workbook name and directory\n",
    "excelNameAmps = os.path.join(output_dir, 'amps.xlsx')\n",
    "counter = 0\n",
    "# Create excel workbook\n",
    "start = time.time()\n",
    "with xlsxwriter.Workbook(excelNameAmps, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(ncols=800, desc=\"Writing amplitude cells to a workbook on \" + str(time.ctime(time.time()))) as pbar:\n",
    "    # Write raw worksheets\n",
    "        sh = workbook.add_worksheet('Mean_Amplitude_μV')\n",
    "        sh.write(1, 0, 'Treatment')\n",
    "        sh.write(0, 0, 'Electrode')\n",
    "        styles = dict()\n",
    "        for electrode, (color, treatment, concentration) in electrode_info[nombresListAmps[-1]].items():\n",
    "            styles[treatment] = workbook.add_format({'bg_color': color}) ## pick treatments and colors from last time point\n",
    "        styles[''] = workbook.add_format({'bg_color': '#00FF00'})## define not-treated condition and color\n",
    "        for electrode, j in col_numbers.items():\n",
    "            sh.write(0, j + 1, electrode)\n",
    "            t,c = electrode_info[nombresListAmps[-1]][electrode][1:]\n",
    "            sh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "        for i, amp_day_data in enumerate(electrode_data.values()):\n",
    "            sh.write(i + 2, 0, nombresListAmps[i])\n",
    "            for electrode, amps in amp_day_data.items():\n",
    "                pbar.update(0.5)      # execution time in this step is limited by disk data transfer speed,\n",
    "                counter += 0.5        # specially the size of the 'bl', 'pretreatment' and 'ac' spike_list files\n",
    "                if amps != []:\n",
    "                    sh.write(i + 2, col_numbers[electrode] + 1, np.nanmean(amps),\n",
    "                         styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                else:\n",
    "                    sh.write(i + 2, col_numbers[electrode] + 1, 0,\n",
    "                         styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "    # Write normalized worksheets\n",
    "        nsh = workbook.add_worksheet('nMean_Amplitude_μV')\n",
    "        nsh.write(1, 0, 'Treatment')\n",
    "        nsh.write(0, 0, 'Electrode')\n",
    "        for electrode, j in col_numbers.items():\n",
    "            nsh.write(0, j + 1, electrode)\n",
    "            t,c = electrode_info[nombresListAmps[-1]][electrode][1:]\n",
    "            nsh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "        for i, amp_day_data in enumerate(electrode_data.values()):\n",
    "            nsh.write(i + 2, 0, nombresListAmps[i])\n",
    "            for electrode, amps in amp_day_data.items():\n",
    "                pbar.update(0.5)\n",
    "                counter += 0.5\n",
    "                if amps != [] and electrode_data[baselines[0]][electrode] != []:\n",
    "                        # if numerator or denominator ARE NOT empty...    \n",
    "                    if \"pretreatment\" in baselines[0].lower() and 'bl' in list(cleandatesdict.keys())[i]:\n",
    "                        nsh.write(i + 2, col_numbers[electrode] + 1, np.nanmean(amps),\n",
    "                                styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                    else:\n",
    "                        if np.nanmean(amps)/np.nanmean(electrode_data[baselines[0]][electrode]) > 4:\n",
    "                            nsh.write(i + 2, col_numbers[electrode] + 1, 0,\n",
    "                                styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                        else:\n",
    "                            nsh.write(i + 2, col_numbers[electrode] + 1, np.nanmean(amps)/np.nanmean(electrode_data[baselines[0]][electrode]),\n",
    "                                styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                        \n",
    "\n",
    "                else:   # if numerador or denominador ARE empty...\n",
    "                    nsh.write(i + 2, col_numbers[electrode] + 1, 0,\n",
    "                         styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "end = time.time()\n",
    "print('Created \"amps.xlsx\" Workbook,',f\"Runtime of the program was {end - start} seconds\")\n",
    "## No RuntimeWarnings means no division of zero or by zero has been performed and excel file will be clean from formula errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0d7a1-f77a-490f-88f1-914cea57c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move amps.xlsx sheets to final Tables_elec_single.xlsx\n",
    "\n",
    "wb1 = xl.load_workbook(filename=excelNameAmps)\n",
    "meanamps = wb1.worksheets[0]\n",
    "if any(\"bl\" in k.lower() for k in electrode_data.keys()) or any(\"pretreatment\" in k.lower() for k in electrode_data.keys()):\n",
    "    nmeanamps = wb1.worksheets[1]\n",
    "\n",
    "merged = xl.load_workbook(filename=excelName)\n",
    "copymeanamps = merged.create_sheet(meanamps.title, index=0)\n",
    "if any(\"bl\" in k.lower() for k in electrode_data.keys()) or any(\"pretreatment\" in k.lower() for k in electrode_data.keys()):\n",
    "    copyNmeanamps = merged.create_sheet(nmeanamps.title, index=1)\n",
    "\n",
    "with tqdm_notebook(ncols=800, total = counter,\n",
    "                   desc=\"Moving amplitude cells to final excel \" + str(time.ctime(time.time()))) as pbar:\n",
    "    for row in meanamps:\n",
    "        for cell in row:\n",
    "            new_cell = copymeanamps.cell(row=cell.row, column=cell.col_idx,\n",
    "                    value= cell.value)\n",
    "            if cell.has_style:\n",
    "                new_cell.font = copy(cell.font)\n",
    "                new_cell.border = copy(cell.border)\n",
    "                new_cell.fill = copy(cell.fill)\n",
    "                new_cell.number_format = copy(cell.number_format)\n",
    "                new_cell.protection = copy(cell.protection)\n",
    "                new_cell.alignment = copy(cell.alignment)\n",
    "                pbar.update(0.5)\n",
    "                \n",
    "    if any(\"bl\" in k.lower() for k in electrode_data.keys()) or any(\"pretreatment\" in k.lower() for k in electrode_data.keys()):\n",
    "        for row in nmeanamps:\n",
    "            for cell in row:\n",
    "                new_cell = copyNmeanamps.cell(row=cell.row, column=cell.col_idx,\n",
    "                        value= cell.value)\n",
    "                if cell.has_style:\n",
    "                    new_cell.font = copy(cell.font)\n",
    "                    new_cell.border = copy(cell.border)\n",
    "                    new_cell.fill = copy(cell.fill)\n",
    "                    new_cell.number_format = copy(cell.number_format)\n",
    "                    new_cell.protection = copy(cell.protection)\n",
    "                    new_cell.alignment = copy(cell.alignment)\n",
    "                    pbar.update(0.5)\n",
    "                    \n",
    "for sheet in merged:\n",
    "    sheet.sheet_view.tabSelected = False\n",
    "merged.active = 0\n",
    "merged.save(excelName)\n",
    "os.remove(excelNameAmps)\n",
    "print('Moved amplitude worksheets to {} workbook and deleted {} workbook'.format(excelName, 'amps.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545af20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"\\n\\nPlease, check the Test.xlsx file in the output folder before proceeding with the plots. Press Enter to continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dcd1c-105f-4af8-9572-d013911d1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel file as pandas dataframe\n",
    "df = pd.read_excel(excelName, index_col=0, header=1, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb28c0-08da-4de5-a5d4-d1324a323004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Match treatments with their color using the last time point's color-treatment key-value pair.\n",
    "# Check the result is correct and use next code cell after manually correcting the excel workbook info if not\n",
    "try:\n",
    "    co = {}\n",
    "    for color, treatm in zip(day_data['Well Coloring'], day_data['Treatment']):\n",
    "        co[treatm] = color\n",
    "    temp = {v : k for k, v in co.items()}\n",
    "    res = {v : k for k, v in temp.items()}\n",
    "    print(res,'\\n\\nNumber of treatments is: {}\\n'.format(len(res)))\n",
    "    \n",
    "# Solution for when the treatment names are wrong in the .csv files but correct in the DataFrame after correction\n",
    "# and the treatment-color pairs must be gotten from the dataframe (using 'Number of Spikes' sheet)\n",
    "except KeyError:\n",
    "    \n",
    "    tem = list(df['Number_of_Spikes'].columns)\n",
    "    col_headers = []\n",
    "    # ran = [format(x, '0') for x in range(0,10)]\n",
    "    for e in tem:\n",
    "        if e.count('.') > 1:\n",
    "            col_headers.append(e.split(' ', maxsplit=1)[0]+ ' ' + e.split(' ', maxsplit=1)[1].split('.')[0])\n",
    "        elif '.' not in e:\n",
    "            col_headers.append(e)\n",
    "        elif '.' in e[:3] and '.' not in e[-1:-3]:\n",
    "            col_headers.append(e)\n",
    "        else:\n",
    "            col_headers.append(e.split('.')[0])\n",
    "\n",
    "    print(col_headers)\n",
    "    wob = xl.load_workbook(excelName, data_only = True)\n",
    "    sh = wob['Number_of_Spikes']\n",
    "    hex_colors = []\n",
    "    for column in range(2, sh.max_column+1):\n",
    "        hex_colors.append(sh.cell(3,column).fill.start_color.index.replace('FF','#',1))\n",
    "    co = {}\n",
    "    for color, treatm in zip(hex_colors, col_headers):\n",
    "        co[treatm] = color\n",
    "    temp = {v : k for k, v in co.items()}\n",
    "    res = {v : k for k, v in temp.items()}\n",
    "    print(res)\n",
    "    print ('HEX =', hex_colors) \n",
    "    #print('RGB =', tuple(int(hex_colors[i:i+2], 16) for i in (0, 2, 4))) # Color in RGB\n",
    "\n",
    "# Create parameter data and SEM averages for each treatment directly from pandas dataframe content\n",
    "'''These calculations are also performed within the descriptive plots' loop'''\n",
    "parameters = dict()\n",
    "sem = dict()\n",
    "for metric_name, df_metric in tqdm_notebook(df.items(), desc = 'Creating \"parameters\" and \"sem\" data dictionaries on ' + str(time.ctime(time.time()))):\n",
    "    parameters[metric_name] = {k: [] for k in res}\n",
    "    sem[metric_name] = {k: [] for k in res}\n",
    "    for day in df_metric.index:\n",
    "        for trt in parameters[metric_name]:\n",
    "            vals_over_trt = np.array([df_metric.loc[day][key] for key in df_metric.keys() if key.startswith(trt)])\n",
    "            with np.errstate(invalid='ignore'):\n",
    "                parameters[metric_name][trt].append(np.nanmean(vals_over_trt))\n",
    "                sem[metric_name][trt].append(stats.sem(vals_over_trt, axis=0, ddof=1, nan_policy='omit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7052d-31e3-4043-8a47-85b587165fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my_functions import statistics_chart\n",
    "# Create a composition with all the charts using add_gridspec and add_subplot\n",
    "# statistics_chart(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a16af-ab16-42c7-a403-026a390f3343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some plot presets\n",
    "\n",
    "titles_and_axes_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=48)\n",
    "legends_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=42)\n",
    "\n",
    "devel = []\n",
    "overnight = []\n",
    "after = []\n",
    "post_on = []\n",
    "for k in realistic_x_axis.keys():\n",
    "    if 'bl' in k:\n",
    "        break\n",
    "    devel.append(k)\n",
    "\n",
    "last_overnight = None\n",
    "first_div = None\n",
    "for i, (k, v) in enumerate(realistic_x_axis.items()):\n",
    "    if 'bl' in k or 'ac' in k or 'on' in k or 'pretreatment' in k or 'Pretreatment' in k:\n",
    "        overnight.append(k)\n",
    "    else:\n",
    "        last_overnight = list(realistic_x_axis.values())[i-1]\n",
    "        first_div = v\n",
    "        break\n",
    "\n",
    "if first_div is not None and last_overnight is not None and abs(last_overnight - first_div) < 3:\n",
    "    overnight.append(df['Number_of_Spikes'].index.values[len(overnight)]) # this line applies to the last timepoint of the on whose name is changed to 'div' something.\n",
    "devel = np.array(devel)                                         # Sometimes a separate 11 min segment is recorded a few hours after the last 11 min of overnight recording\n",
    "after = df['Number_of_Spikes'].index.values[len(devel):]\n",
    "post_on = df['Number_of_Spikes'].index.values[len(devel) + len(overnight):]\n",
    "print('Stablished line plot presets on ' + str(time.ctime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492cd74-0ebe-4e7f-9da7-37d745ed0e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot post-treatment values with SEM whiskers and a broken (split) x-axis\n",
    "workbook = xl.load_workbook(excelName)\n",
    "prs = Presentation()\n",
    "blank_slide_layout = prs.slide_layouts[6]\n",
    "r=0\n",
    "with plt.ion():\n",
    "    for name, mean_metr in tqdm_notebook(parameters.items(), total=((len(parameters.keys()))) , desc='Plotting and saving graphs on ' + str(time.ctime(time.time()))):\n",
    "        if post_on.size == 0:\n",
    "            fig1, axs1 = plt.subplots(1, 1, figsize=(40, 15), gridspec_kw=dict(hspace=0, wspace=0), sharex='none', sharey=True)\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                if name[0] != 'n':\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after]\n",
    "                elif name[0] == 'n' and any('pretreatment' in ind.lower() for ind in after):\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after if 'bl' not in ind.lower()]\n",
    "                realistic_x = [list(realistic_x_axis.values())[i] for i in x]\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, capthick=2, markeredgecolor='black',\n",
    "                              markerfacecolor=res[trt], fmt = ' ', capsize=18, ecolor=res[trt])\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False), label=trt,\n",
    "                              elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, capthick=2.8, markeredgecolor='black',\n",
    "                              markerfacecolor=res[trt], capsize=18.5, ecolor='black')\n",
    "                handles, labels = axs1.get_legend_handles_labels()\n",
    "                handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "                by_label = dict(zip(labels, handles))\n",
    "                axs1.legend(by_label.values(), by_label.keys(), ncol=len(res), labelspacing=1, columnspacing=2,\n",
    "                            bbox_to_anchor=(0.20, 1.05), loc=\"lower left\", framealpha=0, prop=legends_on)\n",
    "        else:\n",
    "            custom_fig_size=(40, 15)\n",
    "            fig1, (axs1,axs2) = plt.subplots(1, 2, figsize=custom_fig_size, gridspec_kw=dict({'width_ratios': [len(overnight), len(post_on)]}, hspace=0, wspace=0.03),\n",
    "                                             sharex='none', sharey=True, facecolor='w')\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            axs2.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                if name[0] != 'n':\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after]\n",
    "                elif name[0] == 'n' and any('pretreatment' in ind.lower() for ind in after):\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after if 'bl' not in ind.lower()]\n",
    "                x2 = [df[name].index.get_loc(ind) for ind in post_on]\n",
    "                realistic_x = [list(realistic_x_axis.values())[i] for i in x]\n",
    "                realistic_x2 = [list(realistic_x_axis.values())[i] for i in x2]\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt], fmt = ' ',\n",
    "                              capsize=18.4, capthick=1.2, ecolor=res[trt])\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=18.8, capthick=2.6, ecolor='black')\n",
    "                axs2.errorbar(realistic_x2, np.nan_to_num(np.array(row)[x2], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x2], copy=False),\n",
    "                              label=trt, elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt], fmt = ' ',\n",
    "                              capsize=18.4, capthick=1.2, ecolor=res[trt])\n",
    "                axs2.errorbar(realistic_x2, np.nan_to_num(np.array(row)[x2], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x2], copy=False),\n",
    "                              label=trt, elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=18.8, capthick=2.6, ecolor='black')\n",
    "            \n",
    "            axs2.set_xlim(realistic_x_axis[post_on[0]]-6, realistic_x_axis[post_on[-1]] + 1)\n",
    "            plt.setp(axs2, xticks=np.arange(round(realistic_x_axis[post_on[0]]), round(realistic_x_axis[post_on[-1]] + 50), 50))\n",
    "            axs2.tick_params(axis='x', length=25, width=3.5, pad=12)\n",
    "            plt.setp(axs2.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "            axs2.spines['left'].set_visible(False)\n",
    "            axs2.spines['bottom'].set_linewidth(3.5)\n",
    "            axs2.spines['top'].set_visible(False)\n",
    "            axs2.spines['right'].set_visible(False)\n",
    "            axs2.yaxis.set_visible(False)\n",
    "            axs2.grid(False)\n",
    "            d = .025 # how big to make the diagonal lines in axes coordinates\n",
    "            kwargs = dict(transform=axs1.transAxes, color='k', clip_on=False, linewidth=3.5)\n",
    "            axs1.plot((1,1), (-d,+d), **kwargs)\n",
    "            kwargs.update(transform=axs2.transAxes)\n",
    "            axs2.plot((0,0), (-d,+d), **kwargs)\n",
    "            handles, labels = axs1.get_legend_handles_labels()\n",
    "            handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            axs1.legend(by_label.values(), by_label.keys(), ncol=len(res), labelspacing=1, columnspacing=2,\n",
    "                        bbox_to_anchor=(0.6, 1.05), loc=\"lower center\", framealpha=0, prop=legends_on)\n",
    "\n",
    "        # Settings common to both cases\n",
    "        axs1.set_xlim(realistic_x_axis[overnight[0]]-0.5, realistic_x_axis[overnight[-1]]+0.5)\n",
    "        plt.setp(axs1, xticks=np.arange(0, round(realistic_x_axis[overnight[-1]]+1.51), 1))\n",
    "        plt.setp(axs1.yaxis.get_majorticklabels(), fontproperties=titles_and_axes_on)\n",
    "        plt.setp(axs1.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "        axs1.tick_params(axis='both', labelright='off', length=25, width=3.5, pad=12)\n",
    "        axs1.spines['left'].set_linewidth(3.5)\n",
    "        axs1.spines['bottom'].set_linewidth(3.5)\n",
    "        axs1.spines['top'].set_visible(False)\n",
    "        axs1.spines['right'].set_visible(False)\n",
    "        axs1.yaxis.tick_left()\n",
    "        axs1.grid(False)\n",
    "        oldname = name\n",
    "        if name[0] == 'n':\n",
    "            name = re.sub('^n', 'Normalized ', name) # use '^n' to replace the first lowercase n by 'Normalized',there is no length limit unlike for excel sheet names\n",
    "            name = re.sub(r' \\([^()]*\\)', '', name) # use ' (*' as a marker to delete units (all units in names are inside parentheses)\n",
    "        fig1.text(0.5, -0.02, 'Time (hours)', ha='center', fontproperties=titles_and_axes_on)\n",
    "        axs1.set_ylabel(name, labelpad=35, fontproperties=titles_and_axes_on)\n",
    "        # Set bottom ylimit to 0 if ylim[0] > 0\n",
    "        ymin, ymax = axs1.get_ylim()\n",
    "        if ymin > 0:\n",
    "            axs1.set_ylim(ymin=0, ymax=ymax*1.1)\n",
    "        else:\n",
    "            pass\n",
    "        axs1.annotate('', xy=(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[0]),\n",
    "                      xytext=(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[1]), rotation=0,\n",
    "                      ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                      arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "        axs1.text(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]]+0.15, axs1.get_ylim()[1]*0.89,\n",
    "                  'DIV'+str(day_of_treatment), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline') # if text separates more than 0.2 from the line is because of autoshrink\n",
    "        if post_on.size > 0:\n",
    "            axs2.annotate('', xy=(realistic_x_axis[post_on[0]], axs2.get_ylim()[0]),\n",
    "                          xytext=(realistic_x_axis[post_on[0]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(realistic_x_axis[post_on[0]]+0.3, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[0].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "            axs2.annotate('', xy=(realistic_x_axis[post_on[-1]], axs2.get_ylim()[0]),\n",
    "                          xytext=(realistic_x_axis[post_on[-1]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(realistic_x_axis[post_on[-1]]+0.3, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[-1].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "\n",
    "        # Save the figures as .png to folder within the loop\n",
    "        if os.path.isdir(os.path.join(output_dir,'Graphs per electrode')) == False:\n",
    "            os.mkdir(os.path.join(output_dir,'Graphs per electrode'))\n",
    "        fig1.savefig(os.path.join(output_dir,'Graphs per electrode\\\\') + name, edgecolor='none', transparent=False)\n",
    "        \n",
    "        # Insert figures on the fly on their corresponding worksheet\n",
    "        sheet = workbook[oldname]\n",
    "        img = xl.drawing.image.Image(os.path.join(output_dir,'Graphs per electrode\\\\') + name + '.png')\n",
    "        img.width = 23*custom_fig_size[0]\n",
    "        img.height = 30*custom_fig_size[1]\n",
    "        sheet.add_image(img, '{}{}'.format('A', len(cleandatesdict.keys())+4))\n",
    "        \n",
    "        # Save figures to a .pptx within the loop\n",
    "        left=Inches(2.9)\n",
    "        top=Inches(0.05)\n",
    "        width=Inches(6.75)\n",
    "        img_path = os.path.join(output_dir,'Graphs per electrode\\\\') + name + '.png'\n",
    "        if r == 0:\n",
    "            slide = prs.slides.add_slide(blank_slide_layout)\n",
    "            pic = slide.shapes.add_picture(img_path, left, top, width)\n",
    "            txBox = slide.shapes.add_textbox(Inches(0.1), Inches(0.1), Inches(3), Inches(0.5))\n",
    "            tf = txBox.text_frame\n",
    "            tf.text = name\n",
    "            tf.paragraphs[0].font.bold = True\n",
    "            tf.paragraphs[0].font.size = Pt(20)\n",
    "            r=1\n",
    "        elif r==1:\n",
    "            top=Inches(3.75)\n",
    "            pic = slide.shapes.add_picture(img_path, left, top, width)\n",
    "            r=0\n",
    "        \n",
    "#         plt.show(close=None, block=None) # Inline plotting consumes many resources and hugely increases running time\n",
    "#         prs.save('Presentation.pptx') # Save after each iteration so I can see the live evolution of the file\n",
    "\n",
    "plt.close('all') # Close all figures and do not display any inline after running the loop (inline plotting is time-consuming)\n",
    "prs.save(os.path.join(output_dir,'Graphs per electrode.pptx')) # Save pptx after the loop is done to save time\n",
    "workbook.save(excelName)\n",
    "\n",
    "# Move all .csv files to \"analysis files\" folder\n",
    "# if os.path.isdir('Unsorted analysis files') == False:\n",
    "#     os.mkdir('Unsorted analysis files')\n",
    "# for fl in input_files:\n",
    "#     if '.csv' in fl or '.txt' in fl:\n",
    "#         shutil.move(fl, os.path.join('Unsorted analysis files', fl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
