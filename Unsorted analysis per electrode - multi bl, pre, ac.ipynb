{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b24c-9882-492e-9af8-69d824226db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import shutil\n",
    "import warnings\n",
    "import sys\n",
    "# import glob\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from scipy import stats\n",
    "# import scipy.io\n",
    "import csv\n",
    "import xlsxwriter\n",
    "from statistics import mode\n",
    "import math\n",
    "import time\n",
    "from mea_functions import averageMetricsAndDates\n",
    "from mea_functions import data_and_date_reconstruction\n",
    "from mea_functions import averageAmps\n",
    "# from mea_functions import statistics_chart\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import openpyxl as xl\n",
    "from copy import copy\n",
    "# from copy import deepcopy\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# import statsmodels.api as sm\n",
    "# from bioinfokit.analys import stat\n",
    "# from statsmodels.formula.api import ols\n",
    "# import pingouin as pg\n",
    "# from statsmodels.graphics.factorplots import interaction_plot\n",
    "# from brokenaxes import brokenaxes\n",
    "# import matplotlib.patheffects as pe\n",
    "from matplotlib import container\n",
    "# import concurrent.futures\n",
    "# from itertools import starmap\n",
    "# import multiprocessing as mp\n",
    "# from brokenaxes import brokenaxes\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "import tkinter as tk\n",
    "import tkinter.filedialog as fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6e3df-8182-4f68-83c2-a229760def4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hide these warnings\n",
    "warnings.filterwarnings(action='ignore', message='.*converting a masked element to nan.*')\n",
    "\n",
    "# Some plotting presets\n",
    "# plt.rc(\"errorbar\", capsize=3)\n",
    "plt.rc(\"figure\", dpi=70)\n",
    "plt.rc(\"savefig\", dpi=40, facecolor=\"white\", bbox=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab1536-3f37-438c-a4d9-b3cbeaaccab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print python version\n",
    "python_version = sys.version\n",
    "print(python_version)\n",
    "\n",
    "## Choose analysis files\n",
    "initial_dir = \"C:\\\\\"\n",
    "print(\"Please select the files to analyze:\")\n",
    "\n",
    "time.sleep(0.25)\n",
    "root = tk.Tk()\n",
    "root.filenames = fd.askopenfilenames(initialdir=initial_dir,\n",
    "                                     title=\"Please select the files to analyze\",\n",
    "                                     filetypes=((\"csv files\", \"*.csv\"), (\"all files\", \"*.*\")))\n",
    "input_files = [os.path.basename(f) for f in root.filenames]\n",
    "input_dir = os.path.dirname(root.filenames[0])\n",
    "root.destroy()\n",
    "\n",
    "print(\"Please choose the output folder:\")\n",
    "time.sleep(0.25)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.filenames = fd.askdirectory(initialdir=input_dir,\n",
    "                                 title=\"Please select the output folder\")\n",
    "output_dir = root.filenames\n",
    "root.destroy()\n",
    "\n",
    "os.chdir(input_dir)\n",
    "print('Working directory is:', input_dir, '\\n')\n",
    "print('Output folder is:', output_dir,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ef523-b85d-4d72-bee5-46c521419b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Couple each file with its date of creation\n",
    "messydatesdict = OrderedDict()\n",
    "for fl in input_files:\n",
    "    if '_spike_' not in fl:\n",
    "        with open(fl, encoding='UTF-8') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            row = list(csv_reader)\n",
    "            for lin in row[75:76]:\n",
    "                if any('Analysis Duration (s):' in e.strip() for e in lin):\n",
    "                    duration = round(float(lin[1])/2) # Use the middle of the recording as its new timestamp\n",
    "            for lin in row[5:7]:\n",
    "                if any('Original File Time' in e.strip() for e in lin):\n",
    "                    stamp = lin[1].split(' ')[0]\n",
    "                    clock = lin[1].split(' ')[1]\n",
    "                    year = int(stamp.split('/')[2])\n",
    "                    month = int(stamp.split('/')[0])\n",
    "                    day = int(stamp.split('/')[1])\n",
    "                    hour = int(clock.split(':')[0])\n",
    "                    minute = int(clock.split(':')[1])\n",
    "                    second = int(clock.split(':')[2])\n",
    "                    messydatesdict[fl] = datetime(year,month,day,hour,minute,second) + timedelta(seconds=duration)\n",
    "\n",
    "# Sort file names by their date of creation\n",
    "messydatesdict = dict(sorted(messydatesdict.items(), key=lambda x: datetime.strptime(str(x[1]), '%Y-%m-%d %H:%M:%S')))\n",
    "for k, v in messydatesdict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646461e-b861-4740-9606-da1cafd06a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"clean\" file names\n",
    "cleandatesdict = OrderedDict()\n",
    "datesforaveragemerging = OrderedDict()\n",
    "for key, value in messydatesdict.items():\n",
    "    new_key = key.split('.')[0].replace('(000)(000)', '').replace('(000)', '').replace('(00', ' ').replace('(0', ' ').replace(')', '')\n",
    "    cleandatesdict[new_key] = str(value)\n",
    "    datesforaveragemerging[new_key] = value\n",
    "baselinedatesdict = {}\n",
    "pretreatmentdatesdict = {}\n",
    "acutedatesdict = {}\n",
    "\n",
    "#Create baseline, pretreatment and acute dictionaries and find their indexes within the experiments' time points\n",
    "indexes_bl = []\n",
    "indexes_pre = []\n",
    "indexes_ac = []\n",
    "for k, v in cleandatesdict.items():\n",
    "    if 'bl' in k:\n",
    "        baselinedatesdict[k] = v\n",
    "        indexes_bl.append(list(cleandatesdict.keys()).index(k))\n",
    "    elif 'pretreatment' in k.lower():\n",
    "        pretreatmentdatesdict[k] = v\n",
    "        indexes_pre.append(list(cleandatesdict.keys()).index(k))\n",
    "    elif 'ac' in k:\n",
    "        acutedatesdict[k] = v\n",
    "        indexes_ac.append(list(cleandatesdict.keys()).index(k))\n",
    "    print(k, v)\n",
    "# Dump the timepoints that need to be merged and the ones that not to different dictionaries\n",
    "datestobemergeddict = OrderedDict(baselinedatesdict|pretreatmentdatesdict|acutedatesdict)\n",
    "datesnottobemergeddict = {}\n",
    "for k, v in cleandatesdict.items():\n",
    "    if k not in datestobemergeddict.keys():\n",
    "        datesnottobemergeddict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe7015-f61e-45cf-93b3-e9799e20b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# append desired parameters' data to dataList from the advanced metrics files\n",
    "rowsChulas = np.r_[67,69,71,72,168:186 + 1] - 1\n",
    "dataList = []\n",
    "normal = []\n",
    "duration_in_sec = False\n",
    "durations = []\n",
    "for fl in messydatesdict.keys():\n",
    "    with open(fl, encoding='UTF-8') as csv_file:\n",
    "        temporary = csv.reader(csv_file, delimiter=',')\n",
    "        for i, row_content in enumerate(temporary):\n",
    "            if 'Analysis Duration (s): ' in row_content:\n",
    "                durations.append(float(row_content[1]))\n",
    "                \n",
    "for fl in messydatesdict.keys():\n",
    "    corresponding_index = 'indexes_'+next(subs for subs in ['bl','ac','pre'] if subs in fl) \\\n",
    "                        if any(_ in fl for _ in ['bl','ac','pre']) else 'normal'\n",
    "    with open(fl, encoding='UTF-8') as csv_file:\n",
    "        singleCSV = []\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for i, row_content in enumerate(csv_reader):\n",
    "            if 'Analysis Duration (s): ' in row_content:\n",
    "                duration_in_sec = float(row_content[1])\n",
    "            if i in rowsChulas:                \n",
    "                if len(globals()[corresponding_index]) <= 1: # If the currently open fl is not bl, pre or ac or it is but there is only one of its type in the experiment, do:       \n",
    "                    if row_content[0] == 'Treatment':\n",
    "                        row_content[0] = 'Treatment - Well'\n",
    "                        singleCSV.append(row_content)\n",
    "                    elif (row_content[0] == 'Number of Spikes' or row_content[0] == 'Number of Bursts' or\n",
    "                    row_content[0] == 'Number of Network Bursts' or row_content[0] == 'Inter-Burst Interval - Avg (s)' or\n",
    "                    row_content[0] == 'Inter-Burst Interval - Std (s)'):\n",
    "                        if duration_in_sec >= mode(durations):\n",
    "                            singleCSV.append([round(float(x) / (duration_in_sec/mode(durations))) if x != '' else '' for x in row_content[1:]]) # Divide all values in row[1:] by mode(time span) of the recordings\n",
    "                            singleCSV[-1].insert(0, row_content[0])\n",
    "                        else:\n",
    "                            singleCSV.append(row_content)\n",
    "                    elif row_content[0] == 'Measurement':\n",
    "                        row_content[0] = 'Electrode'\n",
    "                        singleCSV.append(row_content)\n",
    "                    else:\n",
    "                        singleCSV.append(row_content)                    \n",
    "                    \n",
    "                elif len(globals()[corresponding_index]) > 1: # Elif the currently open fl is bl, pre or ac and there are more than one of its type in the experiment, do:\n",
    "                    if row_content[0] == 'Treatment':\n",
    "                        row_content[0] = 'Treatment - Well'\n",
    "                        singleCSV.append(row_content)\n",
    "                    elif row_content[0] == 'Measurement':\n",
    "                        row_content[0] = 'Electrode'\n",
    "                        singleCSV.append(row_content)\n",
    "                    else:\n",
    "                        singleCSV.append(row_content)\n",
    "        dataList.append(singleCSV)\n",
    "\n",
    "for i, row_content in enumerate(dataList):\n",
    "    dataList[i][2][1:] = [c + ' ' + t if c != '' else t for t, c in zip(dataList[i][2][1:], dataList[i][3][1:])]\n",
    "    dataList[i].insert(3, ['Treatment - Electrode'] + [dataList[i][2][a+1] for a, w in enumerate(dataList[i][0][1:]) for e in dataList[i][4][1:] if e[:2] == w])\n",
    "    dataList[i].insert(2, ['Electrode Coloring'] + [dataList[i][1][a+1] for a, w in enumerate(dataList[i][0][1:]) for e in dataList[i][5][1:] if e[:2] == w])\n",
    "    \n",
    "'''\n",
    "In the future, concentration values will be included in the name of the treatment\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f6f48-b45e-4a5a-a132-c0bca0f6b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to calculate metric averages per hour and their new respective timestamps\n",
    "dataListWholeAvgbl, dataListperHourbl, defdatesbl = averageMetricsAndDates(indexes_bl,dataList,cleandatesdict,datesforaveragemerging,'bl')\n",
    "dataListWholeAvgpre, dataListperHourpre, defdatespre = averageMetricsAndDates(indexes_pre,dataList,cleandatesdict,datesforaveragemerging,'pretreatment')\n",
    "dataListWholeAvgac, dataListperHourac, defdatesac = averageMetricsAndDates(indexes_ac,dataList,cleandatesdict,datesforaveragemerging,'ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75506d7-17cd-4c42-82fc-7dd15e477aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to reconstruct 'dataList' and 'cleandatesdict' IF NECESSARY\n",
    "dataList, cleandatesdict = data_and_date_reconstruction(dataList, messydatesdict, cleandatesdict,\n",
    "                                                        datesnottobemergeddict,\n",
    "                                                        dataListWholeAvgbl, dataListperHourbl, defdatesbl,\n",
    "                                                        dataListWholeAvgpre, dataListperHourpre, defdatespre,\n",
    "                                                        dataListWholeAvgac, dataListperHourac, defdatesac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e12d9-916d-493c-8b3a-0ab657c1b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually define a dictionary of shortened parameter names for the excel sheet names\n",
    "\n",
    "excel_friendly = {\n",
    "                'Well':'Well',\n",
    "                'Well Coloring':'Well Coloring',\n",
    "                'Electrode Coloring':'Electrode Coloring',\n",
    "                'Treatment - Well':'Treatment - Well',\n",
    "                'Treatment - Electrode':'Treatment - Electrode',\n",
    "                'Concentration':'Concentration',\n",
    "                'Electrode':'Electrode',\n",
    "                'Number of Spikes':'Number_of_Spikes',\n",
    "                'Mean Firing Rate (Hz)':'Mean_Firing_Rate_Hz',\n",
    "                'ISI Coefficient of Variation':'ISI_Coefficient_of_Variation',\n",
    "                'Number of Bursts':'Number_of_Bursts',\n",
    "                'Burst Duration - Avg (s)':'Burst_Duration_Avg_s',\n",
    "                'Burst Duration - Std (s)':'Burst_Duration_Std_s',\n",
    "                'Number of Spikes per Burst - Avg':'Spikes_per_Burst_Avg',\n",
    "                'Number of Spikes per Burst - Std':'Spikes_per_Burst_Std',\n",
    "                'Mean ISI within Burst - Avg':'Mean_ISI_within_Burst_Avg',\n",
    "                'Mean ISI within Burst - Std':'Mean_ISI_within_Burst_Std',\n",
    "                'Median ISI within Burst - Avg':'Median_ISI_within_Burst_Avg',\n",
    "                'Median ISI within Burst - Std':'Median_ISI_within_Burst_Std',\n",
    "                'Inter-Burst Interval - Avg (s)':'Inter_Burst_Interval_Avg_s',\n",
    "                'Inter-Burst Interval - Std (s)':'Inter_Burst_Interval_Std_s',\n",
    "                'Burst Frequency (Hz)':'Burst_Frequency_Hz',\n",
    "                'IBI Coefficient of Variation':'IBI_Coefficient_of_Variation',\n",
    "                'Normalized Duration IQR':'Normalized Duration IQR',\n",
    "                'Burst Percentage':'Burst_Percentage',\n",
    "                }\n",
    "\n",
    "# Check all names are <=31 chars long\n",
    "for i in excel_friendly.values():\n",
    "    if len(i) > 31:\n",
    "        print(i+ ' is too long of an excel worksheet name')\n",
    "print('Number of electrode advanced metrics =', len(excel_friendly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc5a3d-726b-4e54-89a9-2d02b8539130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign each day's metrics in dataList to their corresponding day in cleandatesdict and dump the info into the \"data\" dictionary\n",
    "data = defaultdict(dict)\n",
    "\n",
    "for day, ds in zip(cleandatesdict.keys(), dataList):\n",
    "    for row in ds:\n",
    "        key = excel_friendly[row[0]]\n",
    "        data[day][key] = row[1:]\n",
    "\n",
    "if any(\"bl\" in k.lower() for k in data.keys()) or any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "    # If there is a baseline or pretreatment, idx is the sorted order of the electrode treatments in the last timepoint\n",
    "    idx = np.argsort(data[day]['Treatment - Electrode'])#merging treatment and concentration values within Treatment key\n",
    "else:\n",
    "    # Else, idx is the sorted order of the electrode coloring of the last timepoint (as coloring must follow some criteria)\n",
    "    idx = np.argsort(data[day]['Electrode Coloring'])   \n",
    "print('\\nIndex of sorted electrode treatments is: ', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2d035-0881-4d42-943f-da1e23357525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of treatment from the name of the first 'ac' time point that received a treatment\n",
    "day_of_treatment = False\n",
    "\n",
    "if any(\"ac\" in k.lower() for k in data.keys()):\n",
    "    acute = [k for k in list(cleandatesdict.keys()) if 'ac' in k[:3]][0]\n",
    "    day_of_treatment = acute.split(' ')[1].replace('div','')\n",
    "    print('Treatment was administered in time point:', acute)\n",
    "print('Day of treatment is: DIV',day_of_treatment)\n",
    "\n",
    "# Calculate relative time from each time point to the acute treatment (time 0) if present\n",
    "def_labels = {}\n",
    "realistic_x_axis = {}\n",
    "if any(\"ac\" in k.lower() for k in data.keys()):\n",
    "    act = datetime.strptime(cleandatesdict[acute], '%Y-%m-%d %H:%M:%S')\n",
    "else:\n",
    "    act = datetime.strptime(list(cleandatesdict.values())[0], '%Y-%m-%d %H:%M:%S')\n",
    "for key in cleandatesdict.keys():\n",
    "    now = datetime.strptime(cleandatesdict[key], '%Y-%m-%d %H:%M:%S')\n",
    "    diff = now - act\n",
    "    hours = diff.days*24 + diff.seconds/3600\n",
    "    minutes = abs(hours*60) % 60 # abs not working the same for pos and neg values, that is why abs(hours). No negative sign in the pre-acute time points for this reason\n",
    "    seconds = abs(hours*3600) % 60\n",
    "    def_labels[key] = [\"{}: {}\".format(key, \"%d%s%02d%s%02d%s\" % (hours,'h ',minutes,'min ',seconds,'sec'))]\n",
    "    realistic_x_axis[key] = hours\n",
    "\n",
    "print('\\ndef_labels:\\n')\n",
    "for k, v in def_labels.items():\n",
    "    print(k+':\\t', \",\".join(v))\n",
    "\n",
    "print('\\nrealistic_x_axis:\\n')\n",
    "for k, v in realistic_x_axis.items():\n",
    "    print(k+':\\t', v, 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7e217-4d05-45b0-aa15-8b4ea93dc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'norms' dict and 'data' dict. Norms contains either the 'Whole averaged bl' metrics or the 'Whole averaged pre' metrics, depending on the experiment's structure\n",
    "if any(\"pretreatment\" in k.lower() for k in cleandatesdict.keys()):\n",
    "    baseline = [key for key in cleandatesdict.keys() if 'pretreatment' in key.lower()][0]\n",
    "    datafornorms = defaultdict(dict)\n",
    "    for row in dataListWholeAvgpre:\n",
    "        key = excel_friendly[row[0]]\n",
    "        datafornorms[baseline][key] = row[1:]\n",
    "    norms = dict()\n",
    "    for parameter, quantity in datafornorms[baseline].items(): \n",
    "        if parameter not in ['Well', 'Well Coloring', 'Electrode Coloring', 'Treatment - Well', 'Treatment - Electrode', 'Concentration', 'Electrode']:\n",
    "            norms[parameter] = []\n",
    "            for val in quantity:\n",
    "                try:\n",
    "                    norms[parameter].append(float(val))\n",
    "                except ValueError as e:\n",
    "                    # print(f'{e} in {parameter} of {baseline}, appending None to norms')\n",
    "                    norms[parameter].append(None)\n",
    "\n",
    "elif any(\"bl\" in k.lower() for k in cleandatesdict.keys()) and not any('pretreatment' in k.lower() for k in cleandatesdict.keys()):\n",
    "    baseline = [key for key in cleandatesdict.keys() if 'bl' in key.lower()][0]\n",
    "    datafornorms = defaultdict(dict)\n",
    "    for row in dataListWholeAvgbl:\n",
    "        key = excel_friendly[row[0]]\n",
    "        datafornorms[baseline][key] = row[1:]\n",
    "    norms = dict()\n",
    "    for parameter, quantity in data[baseline].items(): \n",
    "        if parameter not in ['Well', 'Well Coloring', 'Electrode Coloring', 'Treatment - Well', 'Treatment - Electrode', 'Concentration', 'Electrode']:\n",
    "            norms[parameter] = []\n",
    "            for val in quantity:\n",
    "                try:\n",
    "                    norms[parameter].append(float(val))\n",
    "                except ValueError as e:\n",
    "                    # print(f'{e} in {parameter} of {baseline}, appending None to norms')\n",
    "                    norms[parameter].append(None)\n",
    "print(f'Baseline time point is {baseline}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6213e3-ab96-441c-9190-3ae202a4dc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create excel workbook\n",
    "excelName = os.path.join(output_dir, 'Tables_elec_multi.xlsx')\n",
    "with xlsxwriter.Workbook(excelName, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(total=len(data.values()), desc=\"Writing advanced metrics workbook rows on \" + str(time.ctime(time.time()))) as pbar:\n",
    "        styles = dict()\n",
    "        params = data[list(cleandatesdict.keys())[0]]\n",
    "        sheets_names = []\n",
    "        for key in params.keys():\n",
    "            if key not in ['Well', 'Well Coloring', 'Electrode Coloring', 'Treatment - Well', 'Treatment - Electrode', 'Concentration', 'Electrode']:\n",
    "                sheets_names.append(key)\n",
    "        for name in sheets_names:\n",
    "            workbook.add_worksheet(name)\n",
    "            workbook.add_worksheet(name='n{}'.format(name))\n",
    "        sheets = [sh for sh in workbook.worksheets() if sh.name in sheets_names]\n",
    "        norm_sheets = [sh for sh in workbook.worksheets() if sh.name not in sheets_names]\n",
    "        for i, day_data in enumerate(data.values()):\n",
    "            pbar.update(1)\n",
    "            for color, treat in zip(day_data['Electrode Coloring'], day_data['Treatment - Electrode']):\n",
    "                if not treat in styles:\n",
    "                    styles[treat] = workbook.add_format({'bg_color': color})\n",
    "                # styles[''] = workbook.add_format({'bg_color': '#00FF00'}) # define not-treated condition's color\n",
    "            for sh in sheets:\n",
    "                sh.write(i + 2, 0, list(cleandatesdict.keys())[i])\n",
    "                sh.write(1, 0, 'Treatment')\n",
    "                sh.write(0, 0, 'Electrode')\n",
    "                for j in range(len(day_data['Electrode'])):\n",
    "                    sh.write(0, j + 1, day_data['Electrode'][idx[j]])\n",
    "                    treatment = day_data['Treatment - Electrode'][idx[j]]\n",
    "                    sh.write(1, j + 1, treatment)\n",
    "                    raw = day_data[sh.name][idx[j]]\n",
    "                    try:\n",
    "                        val = float(raw)\n",
    "                        if val == 0:\n",
    "                            val = 0\n",
    "                    except ValueError as e:\n",
    "                        # print(str(e).replace(':', ' in') + \" {}, {}, {}\".format(list(cleandatesdict.keys())[i],\n",
    "                        #                                     sh.name,\n",
    "                        #                                     day_data['Well'][idx[j]]))\n",
    "                        val = 0\n",
    "                    sh.write(i + 2, j + 1, val, styles[day_data['Treatment - Electrode'][idx[j]]])\n",
    "            \n",
    "            # write normalized values\n",
    "            for sh in norm_sheets:\n",
    "                sh.write(i + 2, 0, list(cleandatesdict.keys())[i])\n",
    "                sh.write(1, 0, 'Treatment')\n",
    "                sh.write(0, 0, 'Electrode')\n",
    "                for j in range(len(day_data['Electrode'])):\n",
    "                    sh.write(0, j + 1, day_data['Electrode'][idx[j]])\n",
    "                    treatment = day_data['Treatment - Electrode'][idx[j]]\n",
    "                    sh.write(1, j + 1, treatment)\n",
    "                    raw = day_data[sh.name[1:]][idx[j]]\n",
    "                    try:\n",
    "                        if \"pretreatment\" in baseline.lower() and 'bl' not in list(cleandatesdict.keys())[i]:\n",
    "                            val = float(raw) / norms[sh.name[1:]][idx[j]]\n",
    "                        elif \"pretreatment\" in baseline.lower() and 'bl' in list(cleandatesdict.keys())[i]:\n",
    "                            val = float(raw)\n",
    "                        else: # if no pretreatment present\n",
    "                            val = float(raw) / norms[sh.name[1:]][idx[j]]                            \n",
    "                        if val == 0 or val > 4:\n",
    "                            val = 0\n",
    "                    except ValueError:\n",
    "                        val = 0\n",
    "                    except TypeError:\n",
    "                        val = 0\n",
    "                    except ZeroDivisionError: # if a well has active electrodes\n",
    "#                          in the baseline, a ZeroDivisioError will prompt when\n",
    "#                          trying to normalize (will try to divide float by 0)\n",
    "#                          print(\"{} {} {} {}\".format('Cannot divide',str(raw),'by',str(norms[sh.name[1:]][idx[j]])))\n",
    "                        val = 0\n",
    "                    sh.write(i + 2, j + 1, val, styles[day_data['Treatment - Electrode'][idx[j]]])\n",
    "print(f'Created {excelName}')\n",
    "\"\"\"A FileNotFound error prompting here means the name is too long for an excel workbook and it could not be initialized\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47701f8f-f7d8-4dc2-98bd-004b9dde89a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Now working on the spike amplitude files...'''\n",
    "\n",
    "# Sort full spike_files by date of creation\n",
    "filenames = []\n",
    "for n, d in messydatesdict.items():\n",
    "    filenames.append(f\"{n.split('.csv')[0]}{'_spike_list.csv'}\")\n",
    "    print(filenames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2933fd8-f1cc-41bb-8a08-78c00de3b710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create data structures\n",
    "nombresListAmps = []\n",
    "ampdata = dict()\n",
    "electrode_info = dict()\n",
    "electrodes = dataList[0][6][1:]\n",
    "\n",
    "# add ampdata to structures\n",
    "for fname in tqdm_notebook(filenames, desc = \"Coupling amplitude values with their day, electrode, treatment and treatment concentration on \" + str(time.ctime(time.time()))):\n",
    "    # append short file names to \"nombresListAmps\"\n",
    "    if 'on' in fname and not bool(re.search('\\(00[1-9]\\)|\\([0-9][1-9][0-9]\\)|\\([1-9][0-9][0-9]\\)', fname)):\n",
    "        day = fname.split('.')[0].replace('(000)(000)', '').replace('(000)', '').replace('_spike_list', '')\n",
    "    else:    \n",
    "        day = fname.split('.')[0].replace('(000)', '').replace('_spike_list', '').replace('(00', ' ').replace('(0', ' ').replace(')', '')\n",
    "    print(day)\n",
    "    nombresListAmps.append(day)\n",
    "    with open(fname, encoding='UTF-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',') # execution time in this step is limited by disk data transfer speed,\n",
    "            # specially by the size of the bl and ac spike_list files\n",
    "        ampdata[day] = dict()\n",
    "        ampdata[day]['Electrode'] = []\n",
    "        ampdata[day]['Amplitude'] = []\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i > 0:\n",
    "                try:\n",
    "                    ampdata[day]['Electrode'].append(row[3]) # retrieve full electrode names\n",
    "                    ampdata[day]['Amplitude'].append(float(row[4]))\n",
    "                except IndexError:\n",
    "                    break\n",
    "\n",
    "        wells, coloring, treatment, concentration = None, None, None, None # well info is at the bottom of the table, below the last amplitude values\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            try:\n",
    "                if row[0] == 'Well':\n",
    "                    wells = row[1:]\n",
    "                if row[0] == 'Well Coloring':\n",
    "                    well_coloring = row[1:]\n",
    "                if row[0] == 'Treatment':\n",
    "                    well_treatments = row[1:]\n",
    "                if row[0] == 'Concentration':\n",
    "                    well_concentrations = row[1:]\n",
    "            except IndexError:\n",
    "                break\n",
    "                       \n",
    "        for i, row in enumerate(csv_reader): # try again, necessary in files with very few amplitude values (headers are longer than values list)\n",
    "            try:\n",
    "                if row[0] == 'Well':\n",
    "                    wells = row[1:]\n",
    "                if row[0] == 'Well Coloring':\n",
    "                    well_coloring = row[1:]\n",
    "                if row[0] == 'Treatment':\n",
    "                    well_treatments = row[1:]\n",
    "                if row[0] == 'Concentration':\n",
    "                    well_concentrations = row[1:]\n",
    "            except IndexError:\n",
    "                break\n",
    "                \n",
    "        coloring = [well_coloring[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        treatment = [well_treatments[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        concentration = [well_concentrations[a] for a, w in enumerate(wells) for e in electrodes if e[:2] == w]\n",
    "        \n",
    "        electrode_info[day] = dict(zip(electrodes, zip(coloring, treatment, concentration)))\n",
    "print('\\nCreated \"ampdata\" and \"electrode_info\" dictionaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06d343-6c7f-4168-8751-10cd001fdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert amplitude values from mV to μV and assign them to their electrode of origin\n",
    "amps_per_electrode_in_uV = dict()\n",
    "for day, amp_day_data in tqdm_notebook(ampdata.items(), desc = \"Converting each day\\'s amplitude values to μV and assigning them to each electrode on \" + str(time.ctime(time.time()))):\n",
    "    amps_per_electrode_in_uV[day] = defaultdict(list, {e:[] for e in electrodes})\n",
    "    for el, amp in zip(amp_day_data['Electrode'], amp_day_data['Amplitude']):\n",
    "        amps_per_electrode_in_uV[day][el].append(amp*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d31b0-90aa-40bd-a8e8-f592018208d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort columns by treatment and concentration\n",
    "if any(\"ac\" in k.lower() for k in amps_per_electrode_in_uV.keys()): # if acute treatment is present in days list\n",
    "    treatments = np.array([(e, conc+' '+t) if conc != '' else (e, t) for e, (color, t, conc) in electrode_info[nombresListAmps[-1]].items()], dtype=str).T # retrieve info from the last recording\n",
    "    idx = np.argsort(treatments)[1]\n",
    "else:\n",
    "    print('This experiment does not have an acute treatment point, must be a development experiment or have faulty names')\n",
    "    treatments = np.array([(e, t) for e, (color, t, conc) in electrode_info[list(electrode_info.items())[0][0]].items()], dtype=str).T\n",
    "    idx = np.argsort(treatments)[1]\n",
    "\n",
    "print('\\nEach electrode\\'s unsorted treatments are: ',  treatments[1])\n",
    "print('\\nFinal order of electrodes, sorted by treatment is: ', idx)\n",
    "print('\\nEach electrode\\'s sorted treatments are: ', treatments[1][idx])\n",
    "\n",
    "col_numbers = dict(zip(treatments[0][idx], range(len(idx))))\n",
    "print('\\nElectrode:index key:value couples, sorted by treatment: ', col_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fc363-b4b0-432a-b9c7-8a0789658367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call averageAmps function and perform the averaging on the desired special time points (bl, pre, ac)\n",
    "amps_per_electrode_in_uV_mergedbl, amps_per_electrode_in_uV_merged_per_hourbl = averageAmps(amps_per_electrode_in_uV, electrodes, 'bl')\n",
    "amps_per_electrode_in_uV_mergedpre, amps_per_electrode_in_uV_merged_per_hourpre = averageAmps(amps_per_electrode_in_uV, electrodes, 'pretreatment')\n",
    "amps_per_electrode_in_uV_mergedac, amps_per_electrode_in_uV_merged_per_hourac = averageAmps(amps_per_electrode_in_uV, electrodes, 'ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91c88c-a3a1-4f3e-a06f-1a5c047deabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake name|date dictionary using the new averaged time points and\n",
    "# Merge the averaged results in the adequate order within the remaining non-averaged data\n",
    "defampsdict = dict()\n",
    "averagedtimepointsdicts = amps_per_electrode_in_uV_merged_per_hourbl|amps_per_electrode_in_uV_merged_per_hourpre|amps_per_electrode_in_uV_merged_per_hourac\n",
    "\n",
    "for k, v in averagedtimepointsdicts.items():\n",
    "    defampsdict[k] = averagedtimepointsdicts[k]\n",
    "for k in datesnottobemergeddict.keys():\n",
    "    defampsdict[k] = amps_per_electrode_in_uV[k]\n",
    "\n",
    "# If the newly created 'defampsdict' is not empty and is different from the original 'amps_per_electrode_in_uV'\n",
    "# (so there was an actual remapping of the timepoints and data), rename it like the original 'amps_per_electrode_in_uV'\n",
    "\n",
    "if defampsdict and defampsdict != amps_per_electrode_in_uV:\n",
    "    amps_per_electrode_in_uV = defampsdict\n",
    "if list(cleandatesdict.keys()) != nombresListAmps:\n",
    "    nombresListAmps = list(cleandatesdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ba046-a3d5-4369-bb2a-134eb2bb1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define excel workbook name and directory\n",
    "excelNameAmps = os.path.join(output_dir, 'amps.xlsx')\n",
    "counter = 0\n",
    "# Create excel workbook\n",
    "start = time.time()\n",
    "with xlsxwriter.Workbook(excelNameAmps, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(ncols=800, desc=\"Writing amplitude cells to a workbook on \" + str(time.ctime(time.time()))) as pbar:\n",
    "    # Write raw worksheets\n",
    "        sh = workbook.add_worksheet('Mean_Amplitude_μV')\n",
    "        sh.write(1, 0, 'Treatment')\n",
    "        sh.write(0, 0, 'Electrode')\n",
    "        styles = dict()\n",
    "        for electrode, (color, treatment, concentration) in electrode_info[nombresListAmps[-1]].items():\n",
    "            styles[treatment] = workbook.add_format({'bg_color': color}) ## pick treatments and colors from last time point\n",
    "        styles[''] = workbook.add_format({'bg_color': '#00FF00'})## define not-treated condition and color\n",
    "        for electrode, j in col_numbers.items():\n",
    "            sh.write(0, j + 1, electrode)\n",
    "            t,c = electrode_info[nombresListAmps[-1]][electrode][1:]\n",
    "            sh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "        for i, amp_day_data in enumerate(amps_per_electrode_in_uV.values()):\n",
    "            sh.write(i + 2, 0, nombresListAmps[i])\n",
    "            for electrode, amps in amp_day_data.items():\n",
    "                pbar.update(0.5)      # execution time in this step is limited by disk data transfer speed,\n",
    "                counter += 0.5        # specially the size of the 'bl', 'pretreatment' and 'ac' spike_list files\n",
    "                if amps != []:\n",
    "                    sh.write(i + 2, col_numbers[electrode] + 1, np.nanmean(amps),\n",
    "                         styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                else:\n",
    "                    sh.write(i + 2, col_numbers[electrode] + 1, 0,\n",
    "                         styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "    # Write normalized worksheets\n",
    "        nsh = workbook.add_worksheet('nMean_Amplitude_μV')\n",
    "        nsh.write(1, 0, 'Treatment')\n",
    "        nsh.write(0, 0, 'Electrode')\n",
    "        for electrode, j in col_numbers.items():\n",
    "            nsh.write(0, j + 1, electrode)\n",
    "            t,c = electrode_info[nombresListAmps[-1]][electrode][1:]\n",
    "            nsh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "        for i, amp_day_data in enumerate(amps_per_electrode_in_uV.values()):\n",
    "            nsh.write(i + 2, 0, nombresListAmps[i])\n",
    "            for electrode, amps in amp_day_data.items():\n",
    "                pbar.update(0.5)\n",
    "                counter += 0.5\n",
    "                # if numerator AND denominator ARE NOT empty...\n",
    "                if amps != [] and amps_per_electrode_in_uV[baseline][electrode] != []:\n",
    "                    if \"pretreatment\" in baseline.lower() and 'bl' in list(cleandatesdict.keys())[i]:\n",
    "                        nsh.write(i + 2, col_numbers[electrode] + 1, np.nanmean(amps),\n",
    "                                styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                    else:\n",
    "                        if np.nanmean(amps)/np.nanmean(amps_per_electrode_in_uV[baseline][electrode]) > 4:\n",
    "                            nsh.write(i + 2, col_numbers[electrode] + 1, 0,\n",
    "                                styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                        else:\n",
    "                            nsh.write(i + 2, col_numbers[electrode] + 1, np.nanmean(amps)/np.nanmean(amps_per_electrode_in_uV[baseline][electrode]),\n",
    "                                styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "                        \n",
    "                # else if numerador AND denominador ARE empty...\n",
    "                else:   \n",
    "                    nsh.write(i + 2, col_numbers[electrode] + 1, 0,\n",
    "                         styles[electrode_info[nombresListAmps[i]][electrode][1]])\n",
    "end = time.time()\n",
    "print('Created \"amps.xlsx\" Workbook,',f\"Runtime of the program was {end - start} seconds\")\n",
    "## No RuntimeWarnings means no division of zero or by zero has been performed and excel file will be clean from formula errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0d7a1-f77a-490f-88f1-914cea57c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move amps.xlsx sheets to final Tables_elec_multi.xlsx\n",
    "\n",
    "wb1 = xl.load_workbook(filename=excelNameAmps)\n",
    "meanamps = wb1.worksheets[0]\n",
    "if any(\"bl\" in k.lower() for k in amps_per_electrode_in_uV.keys()) or any(\"pretreatment\" in k.lower() for k in amps_per_electrode_in_uV.keys()):\n",
    "    nmeanamps = wb1.worksheets[1]\n",
    "\n",
    "merged = xl.load_workbook(filename=excelName)\n",
    "copymeanamps = merged.create_sheet(meanamps.title, index=0)\n",
    "if any(\"bl\" in k.lower() for k in amps_per_electrode_in_uV.keys()) or any(\"pretreatment\" in k.lower() for k in amps_per_electrode_in_uV.keys()):\n",
    "    copyNmeanamps = merged.create_sheet(nmeanamps.title, index=1)\n",
    "\n",
    "with tqdm_notebook(ncols=800, total = counter,\n",
    "                   desc=\"Moving amplitude cells to final excel \" + str(time.ctime(time.time()))) as pbar:\n",
    "    for row in meanamps:\n",
    "        for cell in row:\n",
    "            new_cell = copymeanamps.cell(row=cell.row, column=cell.col_idx, value= cell.value)\n",
    "            if cell.has_style:\n",
    "                new_cell.font = copy(cell.font)\n",
    "                new_cell.border = copy(cell.border)\n",
    "                new_cell.fill = copy(cell.fill)\n",
    "                new_cell.number_format = copy(cell.number_format)\n",
    "                new_cell.protection = copy(cell.protection)\n",
    "                new_cell.alignment = copy(cell.alignment)\n",
    "                pbar.update(0.5)\n",
    "                \n",
    "    if any(\"bl\" in k.lower() for k in amps_per_electrode_in_uV.keys()) or any(\"pretreatment\" in k.lower() for k in amps_per_electrode_in_uV.keys()):\n",
    "        for row in nmeanamps:\n",
    "            for cell in row:\n",
    "                new_cell = copyNmeanamps.cell(row=cell.row, column=cell.col_idx, value= cell.value)\n",
    "                if cell.has_style:\n",
    "                    new_cell.font = copy(cell.font)\n",
    "                    new_cell.border = copy(cell.border)\n",
    "                    new_cell.fill = copy(cell.fill)\n",
    "                    new_cell.number_format = copy(cell.number_format)\n",
    "                    new_cell.protection = copy(cell.protection)\n",
    "                    new_cell.alignment = copy(cell.alignment)\n",
    "                    pbar.update(0.5)\n",
    "                    \n",
    "for sheet in merged:\n",
    "    sheet.sheet_view.tabSelected = False\n",
    "merged.active = 0\n",
    "merged.save(excelName)\n",
    "os.remove(excelNameAmps)\n",
    "print('Moved amplitude worksheets to {} workbook and deleted {} workbook'.format(excelName, 'amps.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545af20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"\\nPlease, check the Test.xlsx file in the output folder before continuing. Press Enter to continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dcd1c-105f-4af8-9572-d013911d1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel file as pandas dataframe\n",
    "df = pd.read_excel(excelName, index_col=0, header=1, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb28c0-08da-4de5-a5d4-d1324a323004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Match treatments with their color using the last time point's color-treatment key-value pair.\n",
    "# Check the result is correct and use next code cell after manually correcting the excel workbook info if not\n",
    "try:\n",
    "    co = {}\n",
    "    for color, treatm in zip(day_data['Well Coloring'], day_data['Treatment']):\n",
    "        co[treatm] = color\n",
    "    temp = {v : k for k, v in co.items()}\n",
    "    res = {v : k for k, v in temp.items()}\n",
    "    print(res,'\\n\\nNumber of treatments is: {}\\n'.format(len(res)))\n",
    "    \n",
    "# Solution for when the treatment names are wrong in the .csv files but correct in the DataFrame after correction\n",
    "# and the treatment-color pairs must be gotten from the dataframe (using 'Number of Spikes' sheet)\n",
    "except KeyError:\n",
    "    \n",
    "    tem = list(df['Number_of_Spikes'].columns)\n",
    "    col_headers = []\n",
    "    # ran = [format(x, '0') for x in range(0,10)]\n",
    "    for e in tem:\n",
    "        if e.count('.') > 1:\n",
    "            col_headers.append(e.split(' ', maxsplit=1)[0]+ ' ' + e.split(' ', maxsplit=1)[1].split('.')[0])\n",
    "        elif '.' not in e:\n",
    "            col_headers.append(e)\n",
    "        elif '.' in e[:3] and '.' not in e[-1:-3]:\n",
    "            col_headers.append(e)\n",
    "        else:\n",
    "            col_headers.append(e.split('.')[0])\n",
    "\n",
    "    print(col_headers)\n",
    "    wob = xl.load_workbook(excelName, data_only = True)\n",
    "    sh = wob['Number_of_Spikes']\n",
    "    hex_colors = []\n",
    "    for column in range(2, sh.max_column+1):\n",
    "        hex_colors.append(sh.cell(3,column).fill.start_color.index.replace('FF','#',1))\n",
    "    co = {}\n",
    "    for color, treatm in zip(hex_colors, col_headers):\n",
    "        co[treatm] = color\n",
    "    temp = {v : k for k, v in co.items()}\n",
    "    res = {v : k for k, v in temp.items()}\n",
    "    print(res)\n",
    "    print ('HEX =', hex_colors) \n",
    "    #print('RGB =', tuple(int(hex_colors[i:i+2], 16) for i in (0, 2, 4))) # Color in RGB\n",
    "\n",
    "# Create parameter data and SEM averages for each treatment directly from pandas dataframe content\n",
    "'''These calculations are also performed within the descriptive plots' loop'''\n",
    "parameters = dict()\n",
    "sem = dict()\n",
    "for metric_name, df_metric in tqdm_notebook(df.items(), desc = 'Creating \"parameters\" and \"sem\" data dictionaries on ' + str(time.ctime(time.time()))):\n",
    "    parameters[metric_name] = {k: [] for k in res}\n",
    "    sem[metric_name] = {k: [] for k in res}\n",
    "    for day in df_metric.index:\n",
    "        for trt in parameters[metric_name]:\n",
    "            vals_over_trt = np.array([df_metric.loc[day][key] for key in df_metric.keys() if key.startswith(trt)])\n",
    "            with np.errstate(invalid='ignore'):\n",
    "                parameters[metric_name][trt].append(np.nanmean(vals_over_trt))\n",
    "                sem[metric_name][trt].append(stats.sem(vals_over_trt, axis=0, ddof=1, nan_policy='omit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfbf174-e862-4e28-9b76-f89467e6368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to create a composition with all the charts using add_gridspec and add_subplot\n",
    "# statistics_chart(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a16af-ab16-42c7-a403-026a390f3343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify plot presets\n",
    "\n",
    "titles_and_axes_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=48)\n",
    "legends_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=42)\n",
    "\n",
    "devel = []\n",
    "overnight = []\n",
    "after = []\n",
    "post_on = []\n",
    "for k in realistic_x_axis.keys():\n",
    "    if 'bl' in k:\n",
    "        break\n",
    "    devel.append(k)\n",
    "\n",
    "last_overnight = None\n",
    "first_div = None\n",
    "for i, (k, v) in enumerate(realistic_x_axis.items()):\n",
    "    if 'bl' in k or 'ac' in k or 'on' in k or 'pretreatment' in k or 'Pretreatment' in k:\n",
    "        overnight.append(k)\n",
    "    else:\n",
    "        last_overnight = list(realistic_x_axis.values())[i-1]\n",
    "        first_div = v\n",
    "        break\n",
    "\n",
    "if first_div is not None and last_overnight is not None and abs(last_overnight - first_div) < 3:\n",
    "    overnight.append(df['Number_of_Spikes'].index.values[len(overnight)]) # this line applies to the last timepoint of the on whose name is changed to 'div' something.\n",
    "devel = np.array(devel)                                         # Sometimes a separate 11 min segment is recorded a few hours after the last 11 min of overnight recording\n",
    "after = df['Number_of_Spikes'].index.values[len(devel):]\n",
    "post_on = df['Number_of_Spikes'].index.values[len(devel) + len(overnight):]\n",
    "print('Stablished line plot presets on ' + str(time.ctime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492cd74-0ebe-4e7f-9da7-37d745ed0e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot post-treatment values with SEM whiskers and a broken (split) x-axis\n",
    "workbook = xl.load_workbook(excelName)\n",
    "prs = Presentation()\n",
    "blank_slide_layout = prs.slide_layouts[6]\n",
    "r=0\n",
    "with plt.ion():\n",
    "    for name, mean_metr in tqdm_notebook(parameters.items(), total=((len(parameters.keys()))) , desc='Plotting and saving graphs on ' + str(time.ctime(time.time()))):\n",
    "        if post_on.size == 0:\n",
    "            fig1, axs1 = plt.subplots(1, 1, figsize=(40, 15), gridspec_kw=dict(hspace=0, wspace=0), sharex='none', sharey=True)\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                if name[0] != 'n':\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after]\n",
    "                elif name[0] == 'n' and any('pretreatment' in ind.lower() for ind in after):\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after if 'bl' not in ind.lower()]\n",
    "                realistic_x = [list(realistic_x_axis.values())[i] for i in x]\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, capthick=2, markeredgecolor='black',\n",
    "                              markerfacecolor=res[trt], fmt = ' ', capsize=18, ecolor=res[trt])\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False), label=trt,\n",
    "                              elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, capthick=2.8, markeredgecolor='black',\n",
    "                              markerfacecolor=res[trt], capsize=18.5, ecolor='black')\n",
    "                handles, labels = axs1.get_legend_handles_labels()\n",
    "                handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "                by_label = dict(zip(labels, handles))\n",
    "                axs1.legend(by_label.values(), by_label.keys(), ncol=len(res), labelspacing=1, columnspacing=2,\n",
    "                            bbox_to_anchor=(0.20, 1.05), loc=\"lower left\", framealpha=0, prop=legends_on)\n",
    "        else:\n",
    "            custom_fig_size=(40, 15)\n",
    "            fig1, (axs1,axs2) = plt.subplots(1, 2, figsize=custom_fig_size, gridspec_kw=dict({'width_ratios': [len(overnight), len(post_on)]}, hspace=0, wspace=0.03),\n",
    "                                             sharex='none', sharey=True, facecolor='w')\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            axs2.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                if name[0] != 'n':\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after]\n",
    "                elif name[0] == 'n' and any('pretreatment' in ind.lower() for ind in after):\n",
    "                    x = [df[name].index.get_loc(ind) for ind in after if 'bl' not in ind.lower()]\n",
    "                x2 = [df[name].index.get_loc(ind) for ind in post_on]\n",
    "                realistic_x = [list(realistic_x_axis.values())[i] for i in x]\n",
    "                realistic_x2 = [list(realistic_x_axis.values())[i] for i in x2]\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt], fmt = ' ',\n",
    "                              capsize=18.4, capthick=1.2, ecolor=res[trt])\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=18.8, capthick=2.6, ecolor='black')\n",
    "                axs2.errorbar(realistic_x2, np.nan_to_num(np.array(row)[x2], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x2], copy=False),\n",
    "                              label=trt, elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt], fmt = ' ',\n",
    "                              capsize=18.4, capthick=1.2, ecolor=res[trt])\n",
    "                axs2.errorbar(realistic_x2, np.nan_to_num(np.array(row)[x2], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x2], copy=False),\n",
    "                              label=trt, elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=18.8, capthick=2.6, ecolor='black')\n",
    "            \n",
    "            axs2.set_xlim(realistic_x_axis[post_on[0]]-6, realistic_x_axis[post_on[-1]] + 1)\n",
    "            plt.setp(axs2, xticks=np.arange(round(realistic_x_axis[post_on[0]]), round(realistic_x_axis[post_on[-1]] + 50), 50))\n",
    "            axs2.tick_params(axis='x', length=25, width=3.5, pad=12)\n",
    "            plt.setp(axs2.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "            axs2.spines['left'].set_visible(False)\n",
    "            axs2.spines['bottom'].set_linewidth(3.5)\n",
    "            axs2.spines['top'].set_visible(False)\n",
    "            axs2.spines['right'].set_visible(False)\n",
    "            axs2.yaxis.set_visible(False)\n",
    "            axs2.grid(False)\n",
    "            d = .025 # how big to make the diagonal lines in axes coordinates\n",
    "            kwargs = dict(transform=axs1.transAxes, color='k', clip_on=False, linewidth=3.5)\n",
    "            axs1.plot((1,1), (-d,+d), **kwargs)\n",
    "            kwargs.update(transform=axs2.transAxes)\n",
    "            axs2.plot((0,0), (-d,+d), **kwargs)\n",
    "            handles, labels = axs1.get_legend_handles_labels()\n",
    "            handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            axs1.legend(by_label.values(), by_label.keys(), ncol=len(res), labelspacing=1, columnspacing=2,\n",
    "                        bbox_to_anchor=(0.6, 1.05), loc=\"lower center\", framealpha=0, prop=legends_on)\n",
    "\n",
    "        # Settings common to both cases\n",
    "        axs1.set_xlim(realistic_x_axis[overnight[0]]-0.5, realistic_x_axis[overnight[-1]]+0.5)\n",
    "        plt.setp(axs1, xticks=np.arange(0, round(realistic_x_axis[overnight[-1]]+1.51), 1))\n",
    "        plt.setp(axs1.yaxis.get_majorticklabels(), fontproperties=titles_and_axes_on)\n",
    "        plt.setp(axs1.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "        axs1.tick_params(axis='both', labelright='off', length=25, width=3.5, pad=12)\n",
    "        axs1.spines['left'].set_linewidth(3.5)\n",
    "        axs1.spines['bottom'].set_linewidth(3.5)\n",
    "        axs1.spines['top'].set_visible(False)\n",
    "        axs1.spines['right'].set_visible(False)\n",
    "        axs1.yaxis.tick_left()\n",
    "        axs1.grid(False)\n",
    "        oldname = name\n",
    "        if name[0] == 'n':\n",
    "            name = re.sub('^n', 'Normalized ', name) # use '^n' to replace the first lowercase n by 'Normalized',there is no length limit unlike for excel sheet names\n",
    "            name = re.sub(r' \\([^()]*\\)', '', name) # use ' (*' as a marker to delete units (all units in names are inside parentheses)\n",
    "        fig1.text(0.5, -0.02, 'Time (hours)', ha='center', fontproperties=titles_and_axes_on)\n",
    "        axs1.set_ylabel(name, labelpad=35, fontproperties=titles_and_axes_on)\n",
    "        # Set bottom ylimit to 0 if ylim[0] > 0\n",
    "        ymin, ymax = axs1.get_ylim()\n",
    "        if ymin > 0:\n",
    "            axs1.set_ylim(ymin=0, ymax=ymax*1.1)\n",
    "        else:\n",
    "            pass\n",
    "        axs1.annotate('', xy=(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[0]),\n",
    "                      xytext=(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[1]), rotation=0,\n",
    "                      ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                      arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "        axs1.text(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]]+0.15, axs1.get_ylim()[1]*0.89,\n",
    "                  'DIV'+str(day_of_treatment), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline') # if text separates more than 0.2 from the line is because of autoshrink\n",
    "        if post_on.size > 0:\n",
    "            axs2.annotate('', xy=(realistic_x_axis[post_on[0]], axs2.get_ylim()[0]),\n",
    "                          xytext=(realistic_x_axis[post_on[0]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(realistic_x_axis[post_on[0]]+0.3, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[0].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "            axs2.annotate('', xy=(realistic_x_axis[post_on[-1]], axs2.get_ylim()[0]),\n",
    "                          xytext=(realistic_x_axis[post_on[-1]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(realistic_x_axis[post_on[-1]]+0.3, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[-1].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "\n",
    "        # Save the figures as .png to folder within the loop\n",
    "        if os.path.isdir(os.path.join(output_dir,'Graphs per electrode')) == False:\n",
    "            os.mkdir(os.path.join(output_dir,'Graphs per electrode'))\n",
    "        fig1.savefig(os.path.join(output_dir,'Graphs per electrode\\\\') + name, edgecolor='none', transparent=False)\n",
    "        \n",
    "        # Insert figures on the fly on their corresponding worksheet\n",
    "        sheet = workbook[oldname]\n",
    "        img = xl.drawing.image.Image(os.path.join(output_dir,'Graphs per electrode\\\\') + name + '.png')\n",
    "        img.width = 23*custom_fig_size[0]\n",
    "        img.height = 30*custom_fig_size[1]\n",
    "        sheet.add_image(img, '{}{}'.format('A', len(cleandatesdict.keys())+4))\n",
    "        \n",
    "        # Save figures to a .pptx within the loop\n",
    "        left=Inches(2.9)\n",
    "        top=Inches(0.05)\n",
    "        width=Inches(6.75)\n",
    "        img_path = os.path.join(output_dir,'Graphs per electrode\\\\') + name + '.png'\n",
    "        if r == 0:\n",
    "            slide = prs.slides.add_slide(blank_slide_layout)\n",
    "            pic = slide.shapes.add_picture(img_path, left, top, width)\n",
    "            txBox = slide.shapes.add_textbox(Inches(0.1), Inches(0.1), Inches(3), Inches(0.5))\n",
    "            tf = txBox.text_frame\n",
    "            tf.text = name\n",
    "            tf.paragraphs[0].font.bold = True\n",
    "            tf.paragraphs[0].font.size = Pt(20)\n",
    "            r=1\n",
    "        elif r==1:\n",
    "            top=Inches(3.75)\n",
    "            pic = slide.shapes.add_picture(img_path, left, top, width)\n",
    "            r=0\n",
    "        \n",
    "#         plt.show(close=None, block=None) # Inline plotting consumes many resources and hugely increases running time\n",
    "#         prs.save('Presentation.pptx') # Save after each iteration so I can see the live evolution of the file\n",
    "\n",
    "plt.close('all') # Close all figures and do not display any inline after running the loop (inline plotting is time-consuming)\n",
    "prs.save(os.path.join(output_dir,'Graphs per electrode.pptx')) # Save pptx after the loop is done to save time\n",
    "workbook.save(excelName)\n",
    "\n",
    "# Move all .csv files to \"analysis files\" folder\n",
    "# if os.path.isdir('Unsorted analysis files') == False:\n",
    "#     os.mkdir('Unsorted analysis files')\n",
    "# for fl in input_files:\n",
    "#     if '.csv' in fl or '.txt' in fl:\n",
    "#         shutil.move(fl, os.path.join('Unsorted analysis files', fl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
