{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b24c-9882-492e-9af8-69d824226db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import shutil\n",
    "import warnings\n",
    "# import sys\n",
    "import glob\n",
    "import re\n",
    "# import math\n",
    "from datetime import datetime\n",
    "# from datetime import timedelta\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "# from scipy import stats\n",
    "# import scipy.io\n",
    "import csv\n",
    "import xlsxwriter\n",
    "import time\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import openpyxl as xl\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "# from openpyxl import load_workbook\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# from matplotlib.font_manager import FontProperties\n",
    "# import statsmodels.api as sm\n",
    "# from bioinfokit.analys import stat\n",
    "# from statsmodels.formula.api import ols\n",
    "# import pingouin as pg\n",
    "# from statsmodels.graphics.factorplots import interaction_plot\n",
    "# from brokenaxes import brokenaxes\n",
    "import matplotlib.patheffects as pe\n",
    "# from matplotlib import container\n",
    "# import concurrent.futures\n",
    "# from itertools import starmap\n",
    "# import multiprocessing as mp\n",
    "# from brokenaxes import brokenaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ff2b7-04fe-43a5-b7ea-1eaecb9543ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### hide these types of warning\n",
    "warnings.filterwarnings(action='default', message='.*converting a masked element to nan.*')\n",
    "\n",
    "### Display all columns in pandas dataframe by default\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "### set path\n",
    "path =  os.getcwd()\n",
    "print('path is:', path, '\\n')\n",
    "\n",
    "foldername = os.path.basename(path)\n",
    "excelName = os.path.join(path, foldername + '.xlsx')\n",
    "print('foldername is:', foldername,'\\n')\n",
    "print('excelName is:', excelName)\n",
    "os.chdir(path)\n",
    "\n",
    "### Some plotting presets\n",
    "plt.rc(\"errorbar\", capsize=3)\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "plt.rc(\"savefig\", dpi=220, facecolor=\"white\", bbox=\"tight\") # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb53c6e-8805-4ab3-8ba5-466a00028318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain treatments, concentrations and well coloring from ac spike_list file\n",
    "ac_spike_file = [i for i in os.listdir() if 'spike' in i and 'ac' in i][0]\n",
    "with open(ac_spike_file, 'r', encoding='UTF-8') as csv_file:\n",
    "    row = list(csv.reader(csv_file, delimiter=','))\n",
    "    for lin in tqdm_notebook(row[-7:-1], desc = \"Obtaining treatments, concentrations and well coloring from spike_list files on \" + str(time.ctime(time.time()))):\n",
    "        if lin[0] == 'Well':\n",
    "            wells = [str(i) for i in lin] # string list of treatments\n",
    "        elif lin[0] == 'Treatment':\n",
    "            treats = [str(i) for i in lin] # string list of treatments\n",
    "        elif lin[0] == 'Concentration':\n",
    "            concs = [str(i) for i in lin] # string list of concentrations\n",
    "        elif lin[0] == 'Well Coloring':\n",
    "            colorings = [str(i) for i in lin] # list of well colors\n",
    "print(wells,treats,concs,colorings, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff75de-2519-4769-8d7b-97a46f8e7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete 'sorted ' from file names\n",
    "for file in glob.glob('*supp.csv'):\n",
    "    os.rename(file,file.replace('sorted ', '').replace('__','_'))\n",
    "###  Add this obtained well info to supp files\n",
    "supp_files = [i for i in os.listdir() if '_supp' in i]\n",
    "reader = []\n",
    "cont = 0\n",
    "for fname in glob.glob('*supp.csv'):\n",
    "    with open(fname, \"r\", encoding='UTF-8') as infile:\n",
    "        reader = list(csv.reader(infile, delimiter=','))\n",
    "        if reader[29][0] != 'Treatment':\n",
    "                reader.insert(29, colorings)\n",
    "                reader.insert(29, concs)\n",
    "                reader.insert(29, treats)\n",
    "                with open(fname, \"w\", newline='', encoding='UTF-8') as outfile:\n",
    "                    writer = csv.writer(outfile)\n",
    "                    for line in reader:\n",
    "                        writer.writerow(line)\n",
    "        else:\n",
    "            print('{} already contains well info'.format(fname))\n",
    "            cont += 1\n",
    "\n",
    "print('\\n{} supp files out of {} found contain well info'.format(cont, len(glob.glob('*supp.csv'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146aca5-530a-478a-9939-dc9a1c31b462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Couple each file with its date of creation, retrieved from each of their corresponding _spike_file(s)\n",
    "spikedatesdict = OrderedDict()\n",
    "suppdatesdict = OrderedDict()\n",
    "wfdatesdict = OrderedDict()\n",
    "with tqdm_notebook(total=len([f for f in os.listdir() if f.split('.')[-1] == 'csv' and '_spike_list' in f]),\n",
    "                               desc=\"Coupling each file with its date of creation, retrieved from _spike_file(s) on \" + str(time.ctime(time.time()))) as pbar:\n",
    "    for fl in os.listdir():\n",
    "        if fl.split('.')[-1] == 'csv' and '_spike_list' in fl:\n",
    "            with open(fl, encoding='UTF-8') as csv_file:\n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                row = list(csv_reader)\n",
    "                for lin in row[4:6]:\n",
    "                    if lin[0] == '   Original File Time':\n",
    "                        pbar.update(1)\n",
    "                        spikedatesdict[fl] = lin[1]\n",
    "                        suppdatesdict[fl.replace('_spike_list','_neuralMetrics_supp')] = lin[1]\n",
    "                        wfdatesdict[fl.replace('_spike_list','.wfexp').replace('csv','txt')] = lin[1]\n",
    "                    \n",
    "print('spikedatesdict is:\\n') \n",
    "for k, v in spikedatesdict.items():\n",
    "    print(k, v)\n",
    "print('\\nsuppdatesdict is:\\n') \n",
    "for k, v in suppdatesdict.items():\n",
    "    print(k, v)\n",
    "print('\\nwfdatesdict is:\\n') \n",
    "for k, v in wfdatesdict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f43af8-1f2a-4e5e-83d7-ae17c3611a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort file names by their date of creation\n",
    "spikedatesdict = dict(sorted(spikedatesdict.items(), key=lambda x: datetime.strptime(x[1], '%m/%d/%Y %H:%M:%S')))\n",
    "suppdatesdict = dict(sorted(suppdatesdict.items(), key=lambda x: datetime.strptime(x[1], '%m/%d/%Y %H:%M:%S')))\n",
    "wfdatesdict = dict(sorted(wfdatesdict.items(), key=lambda x: datetime.strptime(x[1], '%m/%d/%Y %H:%M:%S'))) # got to get the order from spike_list files because only there figures the timestamp\n",
    "# Clean file names for the excel workbook and graphs\n",
    "newdatesdict = OrderedDict()\n",
    "for key, value in suppdatesdict.items():\n",
    "    if 'on' in key and '(000)(000)' in key:\n",
    "        new_key = key.split('.')[0].replace('(000)(000)', ' 0').replace('_neuralMetrics_supp','')\n",
    "        newdatesdict[new_key] = value\n",
    "    else:\n",
    "        new_key = key.split('.')[0].replace('(000)', '').replace('(00', ' ').replace('(0', ' ').replace(')', '').replace('_neuralMetrics_supp','')\n",
    "        newdatesdict[new_key] = value\n",
    "for i, (k, v) in enumerate(newdatesdict.items()):\n",
    "    print('newdatesdict {} is: {} {}'.format(i, k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1f578-ac89-43e7-9d54-8b88e916f85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### append desired parameters' data to dataList from the advanced metrics files\n",
    "rowsChulas = np.r_[28,29,30,31,33:39 + 1,41:68 + 1,70:98 + 1,100:105 + 1]\n",
    "dataList = []\n",
    "duration_in_sec = False\n",
    "number_of_wells = None\n",
    "\n",
    "for fl in suppdatesdict.keys():\n",
    "#     if fl.split('.')[-1] == 'csv' and '_' not in fl:            \n",
    "    with open(fl, encoding='UTF-8') as csv_file:\n",
    "        singleCSV = []\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for i, row_content in enumerate(csv_reader):\n",
    "            if i == 67:\n",
    "                number_of_wells = len(row_content)\n",
    "            if row_content != [] and row_content[0] == 'Analysis End':\n",
    "                duration_in_sec = float(row_content[1])\n",
    "            if i in rowsChulas:\n",
    "                row_content[0] = row_content[0].strip()\n",
    "                if row_content[0] == 'Well Averages':\n",
    "                    row_content.pop(-1) ## this line ends with a \",\" making it one position longer as a CSV file\n",
    "                if (row_content[0] == 'Number of Spikes' or row_content[0] == 'Number of Bursts' or\n",
    "                row_content[0] == 'Number of Network Bursts' or row_content[0] == 'Inter-Burst Interval - Avg (s)' or\n",
    "                row_content[0] == 'Inter-Burst Interval - Std (s)'):\n",
    "                    singleCSV.append([round(float(x) / (duration_in_sec/660)) if x != '' else '' for x in row_content[1:]]) # pseudo-normalize by time span of the recording\n",
    "                    singleCSV[-1].insert(0, row_content[0])\n",
    "                elif row_content[0] == 'Mean Firing Rate (Hz)':\n",
    "                    if len(number_of_wells) > 7:\n",
    "                        singleCSV.append([str(float(a)*16) for a in row_content[1:]])\n",
    "                    elif len(number_of_wells) == 7:\n",
    "                        singleCSV.append([str(float(a)*16) for a in row_content[1:]])\n",
    "                    singleCSV[5].insert(0, 'Mean Firing Rate (Hz) - well')\n",
    "                    singleCSV.append(row_content)\n",
    "                    singleCSV[6][0] = 'Mean Firing Rate (Hz) - elec'\n",
    "                elif row_content[0] == 'Weighted Mean Firing Rate (Hz)':\n",
    "                    singleCSV.append([str(float(a)*16) if a != ' ' else ' ' for a in row_content[1:]])\n",
    "                    singleCSV[11].insert(0, 'Weighted Mean Firing Rate (Hz) - unit')\n",
    "                    singleCSV.append(row_content)\n",
    "                    singleCSV[12][0] = 'Weighted Mean Firing Rate (Hz) - elec'\n",
    "                elif row_content[0] == 'Number of Bursting Units':\n",
    "                    singleCSV.append(row_content)\n",
    "                    b = row_content[1:]                    \n",
    "                elif row_content[0] == 'Burst Frequency - Avg (Hz)':\n",
    "                    singleCSV.append([str(float(a)/float(b)) if a != ' ' and b != ' ' else 'NaN' for a, b in zip(row_content[1:],b)])\n",
    "                    singleCSV[33].insert(0, 'Burst Frequency per unit - Avg (Hz)')\n",
    "                elif row_content[0] == 'Burst Frequency - Std (Hz)':\n",
    "                    singleCSV.append([str(float(a)*16) if a != ' ' else ' ' for a in row_content[1:]])\n",
    "                    singleCSV[34].insert(0, 'Burst Frequency per unit - Std (Hz)')\n",
    "                else:\n",
    "                    singleCSV.append(row_content)\n",
    "        dataList.append(singleCSV)\n",
    "for i, row_content in enumerate(dataList):\n",
    "    dataList[i][1][1:] = [j + ' ' + i if j != '' else i for i, j in zip(dataList[i][1][1:], dataList[i][2][1:])]\n",
    "    \n",
    "'''In the future extract the concentration of the second compound used in the well from the name too\n",
    "(use \"after the + symbol\" regular expression to find it) and add it to the \"concentration\" row in dataList'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e12d9-916d-493c-8b3a-0ab657c1b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### manually define a dictionary of shortened parameter names for the excel sheet names\n",
    "\n",
    "excel_friendly = {\n",
    "'Well Averages':'Well Averages',\n",
    "'Treatment':'Treatment',\n",
    "'Concentration':'Concentration',\n",
    "'Well Coloring':'Well Coloring',\n",
    "'Number of Spikes':'Number_of_Spikes',\n",
    "'Mean Firing Rate (Hz) - well':'Mean_Firing_Rate_Hz_well',\n",
    "'Mean Firing Rate (Hz) - elec':'Mean_Firing_Rate_Hz_elec',\n",
    "'ISI Coefficient of Variation - Avg':'ISI_Coeff_of_Variation_Avg',\n",
    "'Network ISI Coefficient of Variation':'Network_ISI_Coeff_Variation',\n",
    "'Fano Factor': 'Fano_Factor',\n",
    "'Number of Active Units':'Number_of_Active_Units',\n",
    "'Weighted Mean Firing Rate (Hz) - unit':'Weighted_Mean_Spk_Rate_Hz_unit',\n",
    "'Weighted Mean Firing Rate (Hz) - elec':'Weighted_Mean_Spk_Rate_Hz_elec',\n",
    "'Number of Bursts':'Number_of_Bursts',\n",
    "'Number of Bursting Units':'Number_of_Bursting_Units',\n",
    "'Burst Duration - Avg (sec)':'Burst_Duration_Avg_sec',\n",
    "'Burst Duration - Std (sec)':'Burst_Duration_Std_sec',\n",
    "'Burst Duration (Median) - Avg (sec)':'Burst_Duration_Med_Avg_s',\n",
    "'Burst Duration (Median) - Std (sec)':'Burst_Duration_Med_Std_s',\n",
    "'Number of Spikes per Burst - Avg':'Spikes_per_Burst_Avg',\n",
    "'Number of Spikes per Burst - Std':'Spikes_per_Burst_Std',\n",
    "'Number of Spikes per Burst (Median) - Avg':'Spikes_per_Burst_Med_Avg',\n",
    "'Number of Spikes per Burst (Median) - Std':'Spikes_per_Burst_Med_Std',\n",
    "'Mean ISI within Burst - Avg (sec)':'Mean_ISI_within_Burst_Avg_s',\n",
    "'Mean ISI within Burst - Std (sec)':'Mean_ISI_within_Burst_Std_s',\n",
    "'Median ISI within Burst - Avg (sec)':'Median_ISI_within_Burst_Avg_s',\n",
    "'Median ISI within Burst - Std (sec)':'Median_ISI_within_Burst_Std_s',\n",
    "'Median/Mean ISI within Burst - Avg':'x̃÷x̅_ISI_within_Burst_Avg',\n",
    "'Median/Mean ISI within Burst - Std':'x̃÷x̅_ISI_within_Burst_Std',\n",
    "'Inter-Burst Interval - Avg (sec)':'Inter_Burst_Interval_Avg_s',\n",
    "'Inter-Burst Interval - Std (sec)':'Inter_Burst_Interval_Std_s',\n",
    "'Inter-Burst Interval (Median) - Avg (sec)':'Inter_Burst_Interv_x̃_Avg_s',\n",
    "'Inter-Burst Interval (Median) - Std (sec)':'Inter_Burst_Interv_x̃_Std_s',\n",
    "'Burst Frequency per unit - Avg (Hz)':'Burst_Frequency_unit_Avg_Hz',\n",
    "'Burst Frequency per unit - Std (Hz)':'Burst_Frequency_unit_Std_Hz',\n",
    "'Normalized Duration IQR - Avg':'Normalized_Duration_IQR_Avg',\n",
    "'Normalized Duration IQR - Std':'Normalized_Duration_IQR_Std',\n",
    "'IBI Coefficient of Variation - Avg':'IBI_Coeff_of_Variation_Avg',\n",
    "'IBI Coefficient of Variation - Std':'IBI_Coeff_of_Variation_Std',\n",
    "'Burst Percentage - Avg':'Burst_Percentage_Avg',\n",
    "'Burst Percentage - Std':'Burst_Percentage_Std',\n",
    "'Network Bursts Ignored Flag':'Network_Bursts_Ignored_Flag',\n",
    "'Number of Network Bursts':'Number_of_Network_Bursts',\n",
    "'Network Burst Frequency':'Network_Burst_Frequency_Hz',\n",
    "'Network Burst Duration - Avg (sec)':'Network_Burst_Duration_Avg_s',\n",
    "'Network Burst Duration - Std (sec)':'Network_Burst_Duration_Std_s',\n",
    "'Network Burst Duration - Median (sec)':'Network_Burst_Duration_Med_s',\n",
    "'Network Burst Duration - MAD (sec)':'Network_Burst_Duration_MAD_s',\n",
    "'Number of Spikes per Network Burst - Avg':'Spikes_per_Netw_Burst_Avg',\n",
    "'Number of Spikes per Network Burst - Std':'Spikes_per_Netw_Burst_Std',\n",
    "'Number of Spikes per Network Burst - Median':'Spikes_per_Netw_Burst_Med',\n",
    "'Number of Spikes per Network Burst - MAD':'Spikes_per_Netw_Burst_MAD',\n",
    "'Mean ISI within Network Burst - Avg (sec)':'x̅_ISI_within_Netw_Burst_Avg_s',\n",
    "'Mean ISI within Network Burst - Std (sec)':'x̅_ISI_within_Netw_Burst_Std_s',\n",
    "'Median ISI within Network Burst - Avg (sec)':'x̃_ISI_within_Netw_Burst_Avg_s',\n",
    "'Median ISI within Network Burst - Std (sec)':'x̃_ISI_within_Netw_Burst_Std_s',\n",
    "'Median/Mean ISI within Network Burst - Avg':'x̃÷x̅_ISI_within_Netw_Brst_Avg',\n",
    "'Median/Mean ISI within Network Burst - Std':'x̃÷x̅_ISI_within_Netw_Brst_Std',\n",
    "'ISI CoV within Network Burst - Avg':'ISI_CoV_within_Netw_Burst_Avg',\n",
    "'Number of Units Participating in Burst - Avg':'Units_Particip_in_Brst_Avg',\n",
    "'Number of Units Participating in Burst - Std':'Units_Particip_in_Brst_Std',\n",
    "'Number of Units Participating in Burst - Median':'Units_Particip_in_Brst_Med',\n",
    "'Number of Units Participating in Burst - MAD':'Units_Particip_in_Brst_MAD',\n",
    "'Number of Spikes per Network Burst per Unit - Avg':'Spikes_per_NwBrst_per_Unit_Avg',\n",
    "'Number of Spikes per Network Burst per Unit - Std':'Spikes_per_NwBrst_per_Unit_Std',\n",
    "'Number of Spikes per Network Burst per Unit - Median':'Spikes_per_NwBrst_per_Unit_Med',\n",
    "'Number of Spikes per Network Burst per Unit - MAD':'Spikes_per_NwBrst_per_Unit_MAD',\n",
    "'Network Burst Percentage':'Network_Burst_Percentage',\n",
    "'Network IBI Coefficient of Variation':'Network_IBI_Coeff_Variation',\n",
    "'Network Normalized Duration IQR':'Network_Norm_Duration_IQR',\n",
    "'Area Under Normalized Cross-Correlation':'Area_Under_Norm_Cross_Corr',\n",
    "'Area Under Cross-Correlation':'Area_Under_Cross_Correlation',\n",
    "'Full Width at Half Height of Normalized Cross-Correlation':'Width_Half_Height_N_Cross_Corr',\n",
    "'Full Width at Half Height of Cross-Correlation':'Width_Half_Height_Cross_Corr',\n",
    "'Synchrony Index':'Synchrony_Index',\n",
    "'Kreuz SPIKE Distance':'Kreuz_SPIKE_Distance'\n",
    "}\n",
    "\n",
    "### Make sure all names are <=31 chars long\n",
    "for i in excel_friendly.values():\n",
    "    if len(i) > 31:\n",
    "        print(i+ ' is too long of an excel worksheet name')\n",
    "print('Number of advanced metrics parameters =', len(excel_friendly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc5a3d-726b-4e54-89a9-2d02b8539130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### assign each row in dataList to its day in newdatesdict, load into data dictionary\n",
    "data = defaultdict(dict)\n",
    "\n",
    "for day, ds in zip(newdatesdict.keys(), dataList):\n",
    "    for row in ds:\n",
    "        key = excel_friendly[row[0]]\n",
    "        data[day][key] = row[1:]\n",
    "\n",
    "for day in newdatesdict.keys():\n",
    "    keys = list(data[day].keys())\n",
    "    vals = list(data[day].values())\n",
    "    idx = np.argsort(data[day]['Treatment'])#merging treatment and concentration values within 'Treatment' key allows to easily sort columns by both at the same time,\n",
    "                                            #useful when different concentrations of the same treatment are present in the same plate\n",
    "print('\\nIndex of sorted well treatments is: ', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebc7d1-375d-4e6d-bdee-9fd55797d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "### find the first time point that received a treatment and the last that did not, and save their names as \"baseline\" and \"acute\", respectively\n",
    "# Use when there is only one baseline and one acute time point\n",
    "baseline = False\n",
    "day_of_treatment = False\n",
    "for i, day in enumerate(data):\n",
    "    for hexcolor in data[day]['Well Coloring']:\n",
    "        if hexcolor != '#00FF00' and 'bl' in day:\n",
    "            baseline = day\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "acute = [k for k in list(newdatesdict.keys()) if 'ac' in k and 'ac(' not in k][0]\n",
    "day_of_treatment = acute.split(' ')[0].replace('div','')\n",
    "print(baseline, acute, '\\Day of the treatment is: ', day_of_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0a4e1-a4bd-462c-b0da-3059abbbd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate relative time from each time point to the acute treatment (time 0)\n",
    "def_labels = {}\n",
    "realistic_x_axis = {}\n",
    "act = datetime.strptime(newdatesdict[acute], '%m/%d/%Y %H:%M:%S')\n",
    "for key, value in newdatesdict.items():\n",
    "    now = datetime.strptime(newdatesdict[key], '%m/%d/%Y %H:%M:%S')\n",
    "    diff = now - act\n",
    "    hours = diff.days*24 + diff.seconds/3600\n",
    "    minutes = abs(hours*60) % 60\n",
    "    seconds = abs(hours*3600) % 60\n",
    "    def_labels[key] = [\"{}: {}\".format(key, \"%d%s%02d%s%02d%s\" % (hours,'h ',minutes,'min ',seconds,'sec'))]\n",
    "    realistic_x_axis[key] = diff.days*24 + diff.seconds/3600\n",
    "\n",
    "print('def_labels:\\n')\n",
    "for k, v in def_labels.items():\n",
    "    print(k+':\\t', \",\".join(v))\n",
    "\n",
    "print('\\nrealistic_x_axis:\\n')\n",
    "for k, v in realistic_x_axis.items():\n",
    "    print(k+':\\t', v, 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bd620-a901-46ef-97a6-65949fc963f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, calculate the average of all baseline timepoints when\n",
    "### needed, then append baseline values for normalization to norms\n",
    "### dictionary if there is at least one 'bl' item in \"data\".\n",
    "baselines = [key for key in data.keys() if 'bl' in key.lower()]\n",
    "for element in baselines: # This would be the place to do the averaging of all the baseline time points\n",
    "    norms = dict()\n",
    "    for parameter, quantity in data[element].items(): \n",
    "        if parameter not in ['Well', 'Well Coloring', 'Treatment', 'Concentration']:\n",
    "            norms[parameter] = [] # As it is written now, \"norms\" will contain the data from\n",
    "                                  # the last baseline timepoint, usually bl(011) in a batch of 12 bl time points\n",
    "            for val in quantity:\n",
    "                try:\n",
    "                    norms[parameter].append(float(val))\n",
    "                except ValueError:\n",
    "                    norms[parameter].append(None)\n",
    "\n",
    "### List of all acute treatment timepoints\n",
    "acutes = [key for key in data.keys() if 'ac' in key.lower()]\n",
    "day_of_treatment = acutes[0].split(' ')[0].replace('div','')\n",
    "print(baselines,acutes,day_of_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6213e3-ab96-441c-9190-3ae202a4dc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### create excel workbook\n",
    "excelName = os.path.join(output_dir, 'Tables_sorted_well_single.xlsx')\n",
    "with xlsxwriter.Workbook(excelName, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(total=len(data.values()), desc=\"Writing advanced metrics workbook rows on \" + str(time.ctime(time.time()))) as pbar:\n",
    "        styles = dict()\n",
    "        params = data[list(newdatesdict.keys())[0]]\n",
    "        sheets_names = []\n",
    "        for key in params.keys():\n",
    "            if key not in ['Well Averages', 'Well Coloring', 'Treatment', 'Concentration']:\n",
    "                sheets_names.append(key)\n",
    "        for name in sheets_names:\n",
    "            workbook.add_worksheet(name)\n",
    "            workbook.add_worksheet('n{}'.format(name))\n",
    "        sheets = [sh for sh in workbook.worksheets() if sh.name in sheets_names]\n",
    "        norm_sheets = [sh for sh in workbook.worksheets() if sh.name not in sheets_names]\n",
    "        for i, day_data in enumerate(data.values()):\n",
    "            pbar.update(1)\n",
    "            for color, treat in zip(day_data['Well Coloring'], day_data['Treatment']):\n",
    "                if not treat in styles:\n",
    "                    styles[treat] = workbook.add_format({'bg_color': color})\n",
    "                #styles[''] = workbook.add_format({'bg_color': '#00FF00'})#define absent treatment condition and its color\n",
    "            for sh in sheets:\n",
    "                sh.write(i + 2, 0, list(newdatesdict.keys())[i])\n",
    "                sh.write(1, 0, 'Time')\n",
    "                sh.write(0, 0, 'Well')\n",
    "                for j in range(len(day_data['Well Averages'])):\n",
    "                    sh.write(0, j + 1, day_data['Well Averages'][idx[j]])\n",
    "                    treatment = day_data['Treatment'][idx[j]]\n",
    "                    sh.write(1, j + 1, treatment)\n",
    "                    raw = day_data[sh.name][idx[j]]\n",
    "                    try:\n",
    "                        val = float(raw)\n",
    "                        if val == 0:\n",
    "                            val = None\n",
    "                    except ValueError as e:\n",
    "    #                   print(str(e).replace(':', ' in') + \" {}, {}, {}\".format(list(newdatesdict.keys())[i],\n",
    "                                                            #sh.name,\n",
    "                                                            #day_data['Well'][idx[j]]))\n",
    "                        val = None\n",
    "                    sh.write(i + 2, j + 1, val, styles[day_data['Treatment'][idx[j]]])\n",
    "            for sh in norm_sheets:\n",
    "                sh.write(i + 2, 0, list(newdatesdict.keys())[i])\n",
    "                sh.write(1, 0, 'Time')\n",
    "                sh.write(0, 0, 'Well')\n",
    "                for j in range(len(day_data['Well Averages'])):\n",
    "                    sh.write(0, j + 1, day_data['Well Averages'][idx[j]])\n",
    "                    treatment = day_data['Treatment'][idx[j]]\n",
    "                    sh.write(1, j + 1, treatment)\n",
    "                    raw = day_data[sh.name[1:]][idx[j]]\n",
    "                    try:\n",
    "                        val = float(raw) / norms[sh.name[1:]][idx[j]]\n",
    "                        if val == 0:\n",
    "                            val = None\n",
    "                    except ValueError:\n",
    "                        val = None\n",
    "                    except TypeError:\n",
    "                        val = None\n",
    "                    except ZeroDivisionError: # if a well has active electrodes\n",
    "#                          in the baseline, a ZeroDivisioError will prompt when\n",
    "#                          trying to normalize (will try to divide float by 0)\n",
    "#                          print(\"{} {} {} {}\".format('Cannot divide',str(raw),'by',str(norms[sh.name[1:]][idx[j]])))\n",
    "                        val = None\n",
    "                    sh.write(i + 2, j + 1, val, styles[day_data['Treatment'][idx[j]]])\n",
    "print('Created', excelName)\n",
    "# \"\"\"A FileNotFound error prompting here means the name is too long for an excel workbook and it could not be initialized\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b0d27-7630-4c24-8dd6-838c708a3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Opening amplitude files...'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2933fd8-f1cc-41bb-8a08-78c00de3b710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### create data structures\n",
    "nombresListAmps = []\n",
    "ampdata = dict()\n",
    "well_info = dict()\n",
    "\n",
    "### add ampdata to structures\n",
    "for fname in tqdm_notebook(wfdatesdict.keys(),\n",
    "                           desc = \"Coupling amplitude values with their day, electrode, treatment and treatment concentration on \" + str(time.ctime(time.time()))):\n",
    "    # append short file names to \"nombresListAmps\"\n",
    "    day = fname.split('.')[0].replace('on(000)(000)', 'on 0').replace('(000)', '').replace('.wfexp', '').replace('(00', ' ').replace('(0', ' ').replace(')', '')\n",
    "    print('Processing:',day)\n",
    "    nombresListAmps.append(day)\n",
    "    with open(fname, encoding='UTF-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',') # execution time in this step is limited by disk data transfer speed,\n",
    "            # specially by the size of the bl and ac spike_list files\n",
    "        ampdata[day] = dict()\n",
    "        ampdata[day]['Electrode'] = []\n",
    "        ampdata[day]['Amplitude'] = []\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i > 0:\n",
    "                ampdata[day]['Electrode'].append(row[2])\n",
    "                ampdata[day]['Amplitude'].append(float(row[11]))\n",
    "        well, coloring, treatment, concentration = None, None, None, None # well info is at the bottom of the table, below the last amplitude values\n",
    "        well = wells[1:]\n",
    "        coloring = colorings[1:]\n",
    "        treatment = treats[1:]\n",
    "        concentration = concs[1:]\n",
    "        well_info[day] = dict(zip(well, zip(coloring, treatment, concentration)))\n",
    "print('\\nCreated \"ampdata\" and \"Well_info\" dictionaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06d343-6c7f-4168-8751-10cd001fdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting amplitude values to μV and assigning them to their well of origin\n",
    "well_data = dict()\n",
    "for day, amp_day_data in tqdm_notebook(ampdata.items(),\n",
    "                                       desc = \"Converting each day\\'s amplitude values to μV and assigning them to their well of origin on \" + str(time.ctime(time.time()))):\n",
    "    well_data[day] = defaultdict(list)\n",
    "    for el, amp in zip(amp_day_data['Electrode'], amp_day_data['Amplitude']):\n",
    "        well_data[day][el[:2]].append(amp/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d31b0-90aa-40bd-a8e8-f592018208d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sorting columns by treatment and concentration\n",
    "#if 'ac' in well_data.keys(): # if treatment present in days list\n",
    "treatments = np.array([(w, conc+' '+t) if conc != '' else (w, t) for w, (color, t, conc) in well_info[nombresListAmps[-1]].items()], dtype=str).T\n",
    "idx = np.argsort(treatments)[1]\n",
    "# else:\n",
    "#     print('This is a development excel table')\n",
    "#     treatments = np.array([(w, t) for w, (c, t) in well_info['div1'].items()], dtype=str).T\n",
    "#     idx = np.argsort(treatments)[0]\n",
    "\n",
    "print('\\nEach well\\'s treatments are: ',  treatments[1])\n",
    "print('\\nIndex of sorted well treatments is: ', idx)\n",
    "print('\\nWell treatments sorted by final table arrangement: ', treatments[1][idx])\n",
    "\n",
    "col_numbers = dict(zip(treatments[0][idx], range(len(idx))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ba046-a3d5-4369-bb2a-134eb2bb1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create excel workbook\n",
    "excelNameAmps = os.path.join(path, 'amps.xlsx')\n",
    "start = time.time()\n",
    "with xlsxwriter.Workbook(excelNameAmps, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(ncols=900, desc=\"Writing amplitude cells to workbook on \" + str(time.ctime(time.time()))) as pbar:\n",
    "    # Write raw worksheets\n",
    "        sh = workbook.add_worksheet('Mean_Amplitude_μV')\n",
    "        sh.write(1, 0, 'Time')\n",
    "        sh.write(0, 0, 'Well')\n",
    "        styles = dict()\n",
    "        for well, (color, treatment, concentration) in well_info[nombresListAmps[-1]].items():\n",
    "            styles[treatment] = workbook.add_format({'bg_color': color}) ## pick treatments and colors from last time point\n",
    "        styles[''] = workbook.add_format({'bg_color': '#00FF00'})## define not-treated condition and color\n",
    "        for well, j in col_numbers.items():\n",
    "            sh.write(0, j + 1, well)\n",
    "            t,c = well_info[nombresListAmps[-1]][well][1:]\n",
    "            sh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "        for i, amp_day_data in enumerate(well_data.values()):\n",
    "            sh.write(i + 2, 0, nombresListAmps[i])\n",
    "            for well, amps in amp_day_data.items():\n",
    "                pbar.update(0.5)      # execution time in this step is limited by disk data transfer speed,\n",
    "                                      # especially the size of the bl and ac waveform.txt files\n",
    "                if amps != []:\n",
    "                    sh.write(i + 2, col_numbers[well] + 1, np.nanmean(amps),\n",
    "                         styles[well_info[nombresListAmps[i]][well][1]])\n",
    "                else:\n",
    "                    sh.write(i + 2, col_numbers[well] + 1, None,\n",
    "                         styles[well_info[nombresListAmps[i]][well][1]])\n",
    "    # Write normalized worksheets\n",
    "        nsh = workbook.add_worksheet('Normalized_Mean_Amplitude')\n",
    "        nsh.write(1, 0, 'Time')\n",
    "        nsh.write(0, 0, 'Well')\n",
    "        for well, j in col_numbers.items():\n",
    "            nsh.write(0, j + 1, well)\n",
    "            t,c = well_info[nombresListAmps[-1]][well][1:]\n",
    "            nsh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "        for i, amp_day_data in enumerate(well_data.values()):\n",
    "            nsh.write(i + 2, 0, nombresListAmps[i])\n",
    "            for well, amps in amp_day_data.items():\n",
    "                pbar.update(0.5)\n",
    "                if amps != [] and well_data[baselines[0]][well] != []:\n",
    "                    nsh.write(i + 2, col_numbers[well] + 1, np.nanmean(amps)/np.nanmean(well_data[baselines[0]][well]),\n",
    "                         styles[well_info[nombresListAmps[i]][well][1]])\n",
    "                else:\n",
    "                    nsh.write(i + 2, col_numbers[well] + 1, None,\n",
    "                         styles[well_info[nombresListAmps[i]][well][1]])\n",
    "end = time.time()\n",
    "print('Created \"mean amplitude.xlsx\" Workbook,',f\"Runtime of the program was {end - start} seconds\")\n",
    "# No RuntimeWarnings means no division of zero or by zero has been performed and excel file will be clean from formula errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93334527-5ec8-45f0-b964-72990a5a2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move mean amplitude.xlsx sheets to the two first positions of final tables.xlsx\n",
    "\n",
    "wb1 = xl.load_workbook(filename='amps.xlsx')\n",
    "meanamps = wb1.worksheets[0]\n",
    "nmeanamps = wb1.worksheets[1]\n",
    "\n",
    "merged = xl.load_workbook(filename=excelName)\n",
    "copymeanamps = merged.create_sheet(meanamps.title, index=0)\n",
    "copyNmeanamps = merged.create_sheet(nmeanamps.title, index=1)\n",
    "\n",
    "with tqdm_notebook(ncols=900, desc=\"Moving amplitudes cells to final excel \" + str(time.ctime(time.time()))) as pbar:\n",
    "    for row in meanamps:\n",
    "        for cell in row:\n",
    "            new_cell = copymeanamps.cell(row=cell.row, column=cell.col_idx,\n",
    "                    value= cell.value)\n",
    "            if cell.has_style:\n",
    "                new_cell.font = copy(cell.font)\n",
    "                new_cell.border = copy(cell.border)\n",
    "                new_cell.fill = copy(cell.fill)\n",
    "                new_cell.number_format = copy(cell.number_format)\n",
    "                new_cell.protection = copy(cell.protection)\n",
    "                new_cell.alignment = copy(cell.alignment)\n",
    "                pbar.update(0.5)\n",
    "    for row in nmeanamps:\n",
    "        for cell in row:\n",
    "            new_cell = copyNmeanamps.cell(row=cell.row, column=cell.col_idx,\n",
    "                    value= cell.value)\n",
    "            if cell.has_style:\n",
    "                new_cell.font = copy(cell.font)\n",
    "                new_cell.border = copy(cell.border)\n",
    "                new_cell.fill = copy(cell.fill)\n",
    "                new_cell.number_format = copy(cell.number_format)\n",
    "                new_cell.protection = copy(cell.protection)\n",
    "                new_cell.alignment = copy(cell.alignment)\n",
    "                pbar.update(0.5)\n",
    "    ### Rename normalized sheet names\n",
    "    for i in merged:\n",
    "        if i.title[0] == 'n':\n",
    "            i.title = re.sub(r' \\([^()]*\\)', '', i.title)\n",
    "\n",
    "merged.active = 0\n",
    "merged.save(excelName)\n",
    "os.remove('amps.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f1dbf-5de0-48a6-9f83-6da32e0444dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match treatments with their color using the last time point's color-treatment key-value pair.\n",
    "# Check the result is correct and use next code cell after manually correcting the excel workbook info if not\n",
    "co = {}\n",
    "for color, treatm in zip(day_data['Well Coloring'], day_data['Treatment']):\n",
    "    co[treatm] = color\n",
    "temp = {v : k for k, v in co.items()}\n",
    "res = {v : k for k, v in temp.items()}\n",
    "print(res)\n",
    "#res['Control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66899b-3871-4662-a907-f17d509e8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel workbook to a pandas dataframe\n",
    "df = pd.read_excel(excelName, index_col=0, header=1, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5e17f-e4a0-4df6-97c5-eb178e3e04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution for when the treatment names are wrong in the .csv files but correct in the DataFrame after correction\n",
    "### and the treatment-color pairs must be gotten from the dataframe (using 'Number of Spikes' sheet)\n",
    "\n",
    "tem = list(df['Number_of_Spikes'].columns)\n",
    "col_headers = []\n",
    "# ran = [format(x, '0') for x in range(0,10)]\n",
    "for e in tem:\n",
    "    if e.count('.') > 1:\n",
    "        col_headers.append(e.split(' ', maxsplit=1)[0]+ ' ' + e.split(' ', maxsplit=1)[1].split('.')[0])\n",
    "    elif '.' not in e:\n",
    "        col_headers.append(e)\n",
    "    elif '.' in e[:3] and '.' not in e[-1:-3]:\n",
    "        col_headers.append(e)\n",
    "    else:\n",
    "        col_headers.append(e.split('.')[0])\n",
    "        \n",
    "print(col_headers)\n",
    "wob = xl.load_workbook(excelName, data_only = True)\n",
    "sh = wob['Number_of_Spikes']\n",
    "hex_colors = []\n",
    "for column in range(2, sh.max_column+1):\n",
    "    hex_colors.append(sh.cell(3,column).fill.start_color.index.replace('FF','#',1))\n",
    "co = {}\n",
    "for color, treatm in zip(hex_colors, col_headers):\n",
    "    co[treatm] = color\n",
    "temp = {v : k for k, v in co.items()}\n",
    "res = {v : k for k, v in temp.items()}\n",
    "print(res)\n",
    "print ('HEX =', hex_colors) \n",
    "#print('RGB =', tuple(int(hex_colors[i:i+2], 16) for i in (0, 2, 4))) # Color in RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98a659-8def-49ac-8b2d-43f392eb750d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create parameter data and SEM averages for each treatment directly from pandas dataframe content\n",
    "parameters = dict()\n",
    "sem = dict()\n",
    "\n",
    "for metric_name, df_metric in tqdm_notebook(df.items(), desc = 'Creating \"parameters\" and \"sem\" data dictionaries on ' + str(time.ctime(time.time()))):\n",
    "    parameters[metric_name] = {k: [] for k in res}\n",
    "    sem[metric_name] = {k: [] for k in res}\n",
    "    for day in df_metric.index:\n",
    "        for trt in parameters[metric_name]:\n",
    "            vals_over_trt = np.array([df_metric.loc[day][key] for key in df_metric.keys() if key.startswith(trt)])\n",
    "            with np.errstate(invalid='ignore'):\n",
    "                parameters[metric_name][trt].append(np.nanmean(vals_over_trt))\n",
    "                sem[metric_name][trt].append(stats.sem(vals_over_trt, axis=0, ddof=1, nan_policy='omit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d836df-9d66-409d-96d9-291aa55a959f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter multiple instances of bl and ac if extant, keep only the first one of each and discard those with a number in the third position (ac1, ac2...)\n",
    "\n",
    "new_realistic_x_axis = {}\n",
    "for k, v in realistic_x_axis.items():\n",
    "    if 'bl' in k and not bool(re.search(r'\\d', k[2])):\n",
    "        new_realistic_x_axis[k] = realistic_x_axis[k]\n",
    "    elif 'ac' in k and not bool(re.search(r'\\d', k[2])):\n",
    "        new_realistic_x_axis[k] = realistic_x_axis[k]\n",
    "    elif 'bl' in k and bool(re.search(r'\\d', k[2])):\n",
    "        pass\n",
    "    elif 'ac' in k and bool(re.search(r'\\d', k[2])):\n",
    "        pass\n",
    "    else:\n",
    "        new_realistic_x_axis[k] = realistic_x_axis[k]\n",
    "print('\\n'.join(list(new_realistic_x_axis.keys())[:6]), '\\nEtc...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7deefe9-15d2-45fc-8595-ecae2dad4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a composition with all the charts using add_gridspec and add_subplot\n",
    "# plt.style.use('fivethirtyeight') \n",
    "\n",
    "# # Make a deepcopy of df to avoid modifying the original df\n",
    "# local_df = deepcopy(df)\n",
    "\n",
    "# # Ignore these warnings:\n",
    "# warnings.filterwarnings(action='default',\n",
    "#                         message='(.*divide by zero.*|.*Mean of empty slice.*|.*Degrees of freedom.*|.*invalid value.*|.*Epsilon values.*)')\n",
    "\n",
    "# # Create parameter data and SEM averages for each treatment directly from pandas dataframe content\n",
    "# two_way_rm_anovas = {}\n",
    "# ancovas = {}\n",
    "# two_way_unb_anova = {}\n",
    "# # three_way_anova = {}\n",
    "# tukeys_test = {}\n",
    "# levenes_test = {}\n",
    "# parameters = dict()\n",
    "# sem = dict()\n",
    "        \n",
    "# # with interactive plotting (refreshes itself and shows individual plots after every iteration of the loop)\n",
    "# with plt.ion():\n",
    "#     for metric_name, df_metric in tqdm_notebook(local_df.items(), desc = 'Plotting metrics on ' + str(time.ctime(time.time()))):\n",
    "#     ### Extract numbers and calculate Average and SEM for each treatment group and time point before the local_df DataFrame gets altered\n",
    "# #         metric_name = 'Network_Bursts_Ignored_Flag'\n",
    "# #         df_metric = local_df['Network_Bursts_Ignored_Flag']\n",
    "#         print(metric_name)\n",
    "#         parameters[metric_name] = {k: [] for k in res}\n",
    "#         sem[metric_name] = {k: [] for k in res}\n",
    "#         for day in df_metric.index:\n",
    "#             for trt in parameters[metric_name]:\n",
    "#                 vals_over_trt = np.array([df_metric.loc[day][key] for key in df_metric.keys() if key.startswith(trt)])\n",
    "#                 with np.errstate(invalid='ignore'):\n",
    "#                     parameters[metric_name][trt].append(np.nanmean(vals_over_trt))\n",
    "#                     sem[metric_name][trt].append(stats.sem(vals_over_trt, axis=0, ddof=1, nan_policy='omit'))\n",
    "                    \n",
    "#         # Create grid\n",
    "#         slide = plt.figure(constrained_layout=False, figsize=(25,35), dpi=100, facecolor = 'white')\n",
    "#         #         gs1 = slide.add_gridspec(nrows=1, ncols=1, left=0, right=1, wspace=0.1, hspace=0.1) # To place the average + SEM line broken axes plots\n",
    "#         gs2 = slide.add_gridspec(nrows=7, ncols=3, left=0, right=1, top=1, bottom=0,\n",
    "#                                  height_ratios=[1.5, 0.5, 0.5, 0.5, 1, 1, 1], width_ratios=[1,1,1], wspace=0.2, hspace=0.3) # For the statistical test and data assessment plots\n",
    "#         #         slide_ax0 = slide.add_subplot(gs1[:, :])\n",
    "#         slide_ax1 = slide.add_subplot(gs2[0, :2])\n",
    "#         slide_ax1_1 = slide.add_subplot(gs2[0, 2])\n",
    "#         slide_ax2 = slide.add_subplot(gs2[1, :])\n",
    "#         slide_ax3 = slide.add_subplot(gs2[2, :])\n",
    "#         slide_ax9 = slide.add_subplot(gs2[3, :])\n",
    "#         slide_ax4 = slide.add_subplot(gs2[4, :])\n",
    "#         slide_ax5 = slide.add_subplot(gs2[5, 0])\n",
    "#         slide_ax5_1 = slide.add_subplot(gs2[5, 1:])\n",
    "#         slide_ax6 = slide.add_subplot(gs2[6, 0])\n",
    "#         slide_ax7 = slide.add_subplot(gs2[6, 1])\n",
    "#         slide_ax8 = slide.add_subplot(gs2[6, 2])\n",
    "\n",
    "#         allaxes = slide.get_axes()\n",
    "        \n",
    "#         # Create a long-format table for statistical calculations\n",
    "#         cc = pd.DataFrame(local_df[metric_name])\n",
    "#         cc.columns.name = 'Treatment'\n",
    "#         cc.index.name = 'Hours'\n",
    "#         cc = cc.rename({c: c + '.0' for c in local_df[metric_name].columns if c in res}, axis=1, inplace=False)\n",
    "#         cc.columns = metric_name + '_' + cc.columns\n",
    "        \n",
    "#         cc_lineplot = deepcopy(cc)\n",
    "#         cc_lineplot.index.name = 'Time'\n",
    "#         cc_lineplot['Time'] = cc_lineplot.index\n",
    "#         #     cc = cc.drop(cc.columns[[cc.columns.get_loc(c) for c in cc.columns if 'Inactive' in c or 'Dead' in c or 'Untreated' in c]], axis=1, inplace=False)\n",
    "#         cc_lineplot = pd.wide_to_long(cc_lineplot, stubnames = metric_name, i = 'Time', j = 'Treatment.Well', sep='_',\n",
    "#                              suffix='({}.\\\\d{{1,2}})'.format('.\\\\d{1,2}|'.join(res.keys())).replace('/', '\\\\'+'/').replace('+','\\\\'+'+').replace('%', '\\\\'+'%'))\n",
    "#         #     cc.to_csv(path_or_buf=r'C:\\Users\\lab\\Desktop\\DataFrame.csv', encoding='UTF-8-sig') # export active DataFrame to a .csv file on the Desktop\n",
    "#         cc_lineplot = cc_lineplot.reset_index(inplace=False)\n",
    "#         cc_lineplot[['Treatment','Well']] = cc_lineplot['Treatment.Well'].str.split(\".\", n=1, expand=True)\n",
    "#         del cc_lineplot['Treatment.Well']\n",
    "        \n",
    "#         cc['Hours'] = cc.index\n",
    "#         cc['Hours'] = list(realistic_x_axis.values())\n",
    "#     #     cc = cc.drop(cc.columns[[cc.columns.get_loc(c) for c in cc.columns if 'Inactive' in c or 'Dead' in c or 'Untreated' in c]], axis=1, inplace=False)\n",
    "#         cc = pd.wide_to_long(cc, stubnames = metric_name, i = 'Hours', j = 'Treatment.Well', sep='_',\n",
    "#                              suffix='({}.\\\\d{{1,2}})'.format('.\\\\d{1,2}|'.join(res.keys())).replace('/', '\\\\'+'/').replace('+','\\\\'+'+').replace('%', '\\\\'+'%'))\n",
    "#     #     cc.to_csv(path_or_buf=r'C:\\Users\\lab\\Desktop\\DataFrame.csv', encoding='UTF-8-sig') # export active DataFrame to a .csv file on the Desktop\n",
    "#         cc = cc.reset_index(inplace=False)\n",
    "#         cc[['Treatment','Well']] = cc['Treatment.Well'].str.split(\".\", n=1, expand=True)\n",
    "#         del cc['Treatment.Well']\n",
    "#     #     cc.columns = cc.columns.str.replace(\" \", \"_\")\n",
    "#     #     cc.columns.str.replace(\"[_(μV)]\", \"\",regex='False')\n",
    "#     #     cc = cc.replace(np.nan, 0, regex=True) # replace all NaN values with 0 in the long-format DataFrame\n",
    "\n",
    "# # End of DataFrame formatting\n",
    "\n",
    "# # Perform statistical tests for each metric/worksheet in the excel\n",
    "\n",
    "#     #     cc_full = cc.replace(np.nan, 0, regex=True)\n",
    "#         try:            \n",
    "#             two_way_rm_anovas[metric_name] = pg.rm_anova(cc, dv=metric_name, within=['Hours', 'Treatment'], subject='Well',\n",
    "#                                                          detailed=True, correction=True, effsize='n2').round(6)\n",
    "\n",
    "# #             three_way_anova[metric_name] = pg.anova(cc, dv=metric_name, between=['Hours', 'Treatment', 'Well'],\n",
    "# #                                                     detailed=True, ss_type=3, effsize=\"n2\").round(6) # We do not usually have the appropiate data for a three-way ANOVA\n",
    "#         except KeyError as e:\n",
    "#             two_way_rm_anovas[metric_name] = pd.DataFrame([['NaN', 'NaN'], ['NaN', 'NaN'], ['NaN', 'NaN']], columns = ['no_data0', 'no_data1'])\n",
    "#             print('Cannot perform two-way repeated measures Anova on {}: {}'.format(metric_name.replace(' ', '_'), e))\n",
    "        \n",
    "#         try:\n",
    "#             two_way_unb_anova[metric_name] = pg.anova(cc, dv=metric_name, between=['Hours', 'Treatment'],\n",
    "#                                                       detailed=True, effsize=\"n2\").round(6)\n",
    "#         except (KeyError, AssertionError) as e:\n",
    "#             two_way_unb_anova[metric_name] = pd.DataFrame([['NaN', 'NaN'], ['NaN', 'NaN'], ['NaN', 'NaN']], columns = ['no_data0', 'no_data1'])\n",
    "#             print('Cannot perform unbalanced two-way Anova on {}: {}'.format(metric_name.replace(' ', '_'), e))\n",
    "            \n",
    "#         try:\n",
    "#             ancovas[metric_name] = pg.ancova(cc, dv=metric_name, between='Treatment', covar = 'Hours',\n",
    "#                                              effsize='n2').round(6)\n",
    "#         except (KeyError, ValueError) as e:\n",
    "#             ancovas[metric_name] = pd.DataFrame([['NaN', 'NaN'], ['NaN', 'NaN'], ['NaN', 'NaN']], columns = ['no_data0', 'no_data1'])\n",
    "#             print('Cannot perform ANCOVA on {}: {}'.format(metric_name.replace(' ', '_'), e))\n",
    "        \n",
    "#         try:\n",
    "#             tukeys_test[metric_name] = pg.pairwise_tukey(cc, dv=metric_name, between='Treatment', effsize='eta-square')\n",
    "#         except (KeyError, ValueError) as e:\n",
    "#             tukeys_test[metric_name] = pd.DataFrame([['NaN', 'NaN'], ['NaN', 'NaN'], ['NaN', 'NaN']], columns = ['no_data0', 'no_data1'])\n",
    "#             print('Cannot perform Tukey\\'s test on {}: {}'.format(metric_name.replace(' ', '_'), e))\n",
    "\n",
    "#         strip_plot = sns.stripplot(x='Treatment', y=metric_name, hue='Well', \n",
    "#                                    data=cc, ax=slide_ax1, size=9, jitter=True, edgecolor='black', linewidth=0.5, dodge=True)\n",
    "#         box_plot = sns.boxplot(x='Treatment', y=metric_name, hue='Treatment', data=cc,\n",
    "#                                ax=slide_ax1, dodge=False, palette = [v for k, v in sorted(res.items())])\n",
    "#     #     box_plot.legend(bbox_to_anchor=(1.03, 1), borderaxespad=0., labelspacing = 1, fontsize=17)\n",
    "#         bar_plot = sns.barplot(x='Treatment', y=metric_name, hue='Treatment', units='Well', data=cc, ax=slide_ax1_1,\n",
    "#                                dodge=False, ci=95, errcolor='black', errwidth=1.5, capsize=0.25, palette = [v for k, v in sorted(res.items())])\n",
    "#         plt.setp(slide_ax1_1.xaxis.get_majorticklabels(), rotation=-10, ha='left', rotation_mode='anchor')\n",
    "#         slide_ax1.set_xlabel('Treatment', loc='center', labelpad=20, fontsize=18)\n",
    "#         slide_ax1.set_title(metric_name, pad=20, fontsize=20)\n",
    "#         slide_ax1.tick_params(axis='both', which='major', labelsize=17)\n",
    "#         slide_ax1.set_ylabel(metric_name, labelpad=25, fontsize=20)\n",
    "#         slide_ax1_1.set_xlabel('Treatment', loc='center', labelpad=20, fontsize=18)\n",
    "#         slide_ax1_1.set_title(metric_name, pad=20, fontsize=20)\n",
    "#         slide_ax1_1.tick_params(axis='both', which='major', labelsize=17)\n",
    "#         slide_ax1_1.set_ylabel(metric_name, labelpad=25, fontsize=20)\n",
    "\n",
    "#         # Add repeated measures two-way ANOVA results table to the graph\n",
    "#         two_way_rm_anovas_text = []\n",
    "#         for row in range(len(two_way_rm_anovas[metric_name])):\n",
    "#             two_way_rm_anovas_text.append(two_way_rm_anovas[metric_name].iloc[row])\n",
    "#         table1 = slide_ax2.table(cellText=two_way_rm_anovas_text, colLabels=two_way_rm_anovas[metric_name].columns, loc='bottom', bbox=[0, -0.1, 1, 1])\n",
    "#         slide_ax2.set_title('Two-way repeated measures Anova test within time and treatment groups for {}'.format(metric_name),\n",
    "#                            fontsize=18, pad=0)\n",
    "#         slide_ax2.axis('off')\n",
    "# #         table1.auto_set_font_size(False)\n",
    "# #         table1.set_fontsize(12)\n",
    "\n",
    "#         # Add unbalanced two-way ANOVA table to the graph\n",
    "#         two_way_unb_anova_text = []\n",
    "#         for row in range(len(two_way_unb_anova[metric_name])):\n",
    "#             two_way_unb_anova_text.append(two_way_unb_anova[metric_name].iloc[row])\n",
    "#         table2 = slide_ax3.table(cellText=two_way_unb_anova_text, colLabels=two_way_unb_anova[metric_name].columns, loc='bottom', bbox=[0, 0, 1, 1])\n",
    "#         slide_ax3.set_title('Two-way unbalanced samples Anova test within time and treatment groups for {}'.format(metric_name),\n",
    "#                            fontsize=18, pad=20)\n",
    "#         slide_ax3.axis('off')\n",
    "# #         table2.auto_set_font_size(False)\n",
    "# #         table2.set_fontsize(11)\n",
    "\n",
    "#         # Add unbalanced two-way ANOVA table to the graph\n",
    "#         ancova_text = []\n",
    "#         for row in range(len(ancovas[metric_name])):\n",
    "#             ancova_text.append(ancovas[metric_name].iloc[row])\n",
    "#         table5 = slide_ax9.table(cellText=ancova_text, colLabels=ancovas[metric_name].columns, loc='bottom', bbox=[0, 0, 1, 1])\n",
    "#         slide_ax9.set_title('Analisis of covariance between treatments ~ Time + Replicates for {}'.format(metric_name),\n",
    "#                            fontsize=18, pad=20)\n",
    "#         slide_ax9.axis('off')\n",
    "# #         table5.auto_set_font_size(False)\n",
    "# #         table5.set_fontsize(11)\n",
    "\n",
    "#            # Add Tukey's test table to the graph\n",
    "#         tukeys_text = []\n",
    "#         for row in range(len(tukeys_test[metric_name])):\n",
    "#             tukeys_text.append(tukeys_test[metric_name].iloc[row])\n",
    "#         table3 = slide_ax4.table(cellText=tukeys_text, colLabels=tukeys_test[metric_name].columns, loc='bottom', bbox=[0, 0, 1, 1])\n",
    "# #         table3.auto_set_font_size(False)\n",
    "#         slide_ax4.set_title('Pairwise Tukey\\'s post-hoc test (table) within treatment groups for {}'.format(metric_name),\n",
    "#                            fontsize=18, pad=20)\n",
    "#         slide_ax4.axis('off')\n",
    "# #         table3.set_fontsize(11)\n",
    "\n",
    "#         interaction_plot(x=cc['Well'], trace=cc['Treatment'], response=cc[metric_name], ax=slide_ax5, plottype='b', colors=[v for k, v in sorted(res.items())])\n",
    "#         slide_ax5.set_title('{} on each replicate (well)'.format(metric_name),\n",
    "#                            fontsize=18, pad=20)\n",
    "        \n",
    "# #         interaction_plot(x=cc['Hours'], trace=cc['Treatment'], response=cc[metric_name], ax=slide_ax5_1, plottype='b', colors=[v for k, v in sorted(res.items())])\n",
    "#         sns.lineplot(x='Time', y=metric_name, data=cc_lineplot, hue='Treatment', palette=[v for k, v in sorted(res.items())],\n",
    "#                              estimator='mean', ax=slide_ax5_1, err_style='bars', n_boot=1000, sort=True, ci=95)\n",
    "#         #         fancy_lineplot(name, mean_metr, sem, res, new_realistic_x_axis, df)\n",
    "#         slide_ax5_1.set_title('Treatment effect on {} over time (95% CI)'.format(metric_name),\n",
    "#                              fontsize=18, pad=20)\n",
    "#         plt.setp(slide_ax5_1.xaxis.get_majorticklabels(), rotation=-25, ha='left', rotation_mode='anchor')\n",
    "# #         field = \"Hours\"\n",
    "# #         time_order = [list(df[metric_name].index)]\n",
    "# #         cc.set_index(field).loc[time_order].plot\n",
    "\n",
    "#         try:\n",
    "#             residuals = stat()\n",
    "#             residuals.anova_stat(df=cc, res_var='Q(\\'{}\\')'.format(metric_name),\n",
    "#                              anova_model='{}~C(Treatment)+C(Hours)+C(Treatment):C(Hours)'.format('Q(\\'{}\\')'.format(metric_name)))\n",
    "#             pg.qqplot(residuals.anova_std_residuals, dist='norm', confidence=False, ax = slide_ax6)\n",
    "#             pg.qqplot(residuals.anova_std_residuals, dist='expon', ax=slide_ax7)\n",
    "#             # histogram\n",
    "#             slide_ax8 = plt.hist(residuals.anova_model_out.resid, bins='auto', histtype='bar', ec='k', zorder=1) \n",
    "#             plt.xlabel('Residuals')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title('Histogram of residuals\\' distribution')\n",
    "\n",
    "#             # Shapiro-Wilkins test for normal distribution of residuals \n",
    "#             # (Null hypothesis: data were drawn from a normal distribution)\n",
    "#             w, pvalue = stats.shapiro(residuals.anova_model_out.resid)\n",
    "#             textstr1 = '\\n'.join((r'$\\bf{Shapiro-Wilkins test}$',\n",
    "#                                   r'$\\mathrm{w}=%.30f$' % (w, ),\n",
    "#                                   r'$\\mathrm{p-value}=%.30f$' % (pvalue, )))\n",
    "#             # these are matplotlib.patch.Patch properties for the text box\n",
    "#             hist_props = dict(boxstyle='round', facecolor='wheat', alpha=.5)\n",
    "#             # add text box with the data to the plot\n",
    "#             plt.text(0.555, 0.99, textstr1, transform=allaxes[10].transAxes, fontsize=9,\n",
    "#                     verticalalignment='top', bbox=hist_props)\n",
    "\n",
    "#             # Levene's test for homogeneity of variances\n",
    "#             residuals.levene(df=cc, res_var=metric_name, xfac_var=['Treatment', 'Hours'])   \n",
    "#             lev_text = []\n",
    "#             levene_cell_colors = [[\"wheat\",\"wheat\"],[\"wheat\",\"wheat\"],[ \"wheat\",\"wheat\"]]\n",
    "#             for row in range(len(residuals.levene_summary)):\n",
    "#                 lev_text.append(residuals.levene_summary.iloc[row])\n",
    "#             table4 = plt.table(cellText=lev_text, colLabels=residuals.levene_summary.columns,\n",
    "#                               transform=allaxes[10].transAxes, loc='left', edges='closed',\n",
    "#                               cellColours=levene_cell_colors, colColours=[\"gainsboro\",\"gainsboro\"],\n",
    "#                                bbox=[0,0.75,0.25,0.25], zorder=2, alpha=.5)\n",
    "#             # Make table4 semitransparent\n",
    "#             for cell in table4._cells:\n",
    "#                 table4._cells[cell].set_alpha(.5)\n",
    "#             plt.text(0.01, 1.035, 'Levene\\'s test summary', transform=allaxes[10].transAxes,\n",
    "#                     verticalalignment='top', fontsize=9, weight='bold')\n",
    "#         except ValueError as e:\n",
    "#             print('Cannot calculate residuals of {} distribution: {} array is zero-size'.format(metric_name.replace(' ', '_'), e))\n",
    "\n",
    "#         # Save the figure within the loop\n",
    "#         if os.path.isdir('Descriptive statistics and plots') == False:\n",
    "#             os.mkdir('Descriptive statistics and plots')\n",
    "#         slide.savefig('Descriptive statistics and plots\\\\' + metric_name, edgecolor='none', transparent=False)\n",
    "        \n",
    "#         plt.show(close=None, block=None)\n",
    "# #         plt.close(slide)\n",
    "\n",
    "plt.close(\"all\") # Close all figures and do not display any inline after running the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a73733-6123-4296-b079-ddf86e87a9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some plotting presets\n",
    "\n",
    "titles_and_axes_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=48)\n",
    "legends_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=48)\n",
    "devel = []\n",
    "overnight = []\n",
    "after = []\n",
    "post_on = []\n",
    "for k, val in new_realistic_x_axis.items():\n",
    "    if 'bl' in k:\n",
    "        break\n",
    "    devel.append(k)\n",
    "\n",
    "for k, val in new_realistic_x_axis.items():\n",
    "    if 'bl' in k or 'ac' in k or 'on' in k or 'Pretreatment' in k:\n",
    "        overnight.append(k)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "overnight.append(df[name].index.values[len(overnight)])\n",
    "after = df[name].index.values[len(devel):]\n",
    "post_on = df[name].index.values[len(devel)+len(overnight):]\n",
    "print('Stablished plot presets on ' + str(time.ctime(time.time())))\n",
    "    \n",
    "# Plots post-treatment values with SEM whiskers in a broken (split) x-axis\n",
    "\n",
    "with plt.ion():\n",
    "    for name, mean_metr in tqdm_notebook(parameters.items(), total=len(parameters.keys()),\n",
    "                                         desc='Plotting and saving graphs on ' + str(time.ctime(time.time()))):\n",
    "        if post_on.size == 0:\n",
    "            fig1, axs1 = plt.subplots(1, 1, figsize=(35, 15), gridspec_kw=dict(hspace=0, wspace=0), sharex='none', sharey=True)\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                x = [df[name].index.get_loc(ind) for ind in after]\n",
    "                realistic_x = [list(new_realistic_x_axis.values())[i] for i in x]\n",
    "                axs1.errorbar(realistic_x, np.array(row)[x], yerr = np.array(sem[name][trt])[x], label=trt,\n",
    "                              elinewidth=2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=True, markeredgewidth='2.5', markeredgecolor='black',\n",
    "                              markerfacecolor=res[trt],capsize=10, capthick=2, ecolor=res[trt])\n",
    "                handles, labels = axs1.get_legend_handles_labels()\n",
    "                handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "                axs1.legend(handles, labels, bbox_to_anchor=(0.75, 1.1), loc=\"lower left\", framealpha=0, prop=legends_on)\n",
    "        else:\n",
    "            fig1, (axs1,axs2) = plt.subplots(1, 2, figsize=(35, 15),\n",
    "                                             gridspec_kw=dict({'width_ratios': [7, 3]},hspace=0, wspace=0.04),\n",
    "                                             sharex='none', sharey=True, facecolor='w')\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            axs2.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                x = [df[name].index.get_loc(ind) for ind in after]\n",
    "                realistic_x = [list(new_realistic_x_axis.values())[i] for i in x]\n",
    "                axs1.errorbar(realistic_x, np.array(row)[x], yerr = np.array(sem[name][trt])[x], label=trt,\n",
    "                              elinewidth=2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=True, markeredgewidth='2.5', markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=10, capthick=2, ecolor=res[trt])\n",
    "                axs2.errorbar(realistic_x, np.array(row)[x], yerr = np.array(sem[name][trt])[x], label=trt,\n",
    "                              elinewidth=2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=True, markeredgewidth='2.5', markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=10, capthick=2, ecolor=res[trt])\n",
    "            axs2.set_xlim(new_realistic_x_axis[post_on[0]] - 5, new_realistic_x_axis[post_on[-1]] + 8)\n",
    "            plt.setp(axs2, xticks=np.arange(round(new_realistic_x_axis[post_on[0]]), round(new_realistic_x_axis[post_on[-1]] + 50), 50))\n",
    "            axs2.tick_params(axis='x', length=25, width=3.5, pad=12)\n",
    "            plt.setp(axs2.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "            axs2.spines['left'].set_visible(False)\n",
    "            axs2.spines['bottom'].set_linewidth(3.5)\n",
    "            axs2.spines['top'].set_visible(False)\n",
    "            axs2.spines['right'].set_visible(False)\n",
    "            axs2.yaxis.set_visible(False)\n",
    "            axs2.grid(False)\n",
    "            d = .025 # how big to make the diagonal lines in axes coordinates\n",
    "            kwargs = dict(transform=axs1.transAxes, color='k', clip_on=False, linewidth=3.5)\n",
    "            axs1.plot((1,1), (-d,+d), **kwargs)\n",
    "            kwargs.update(transform=axs2.transAxes)\n",
    "            axs2.plot((0,0), (-d,+d), **kwargs)\n",
    "            handles, labels = axs1.get_legend_handles_labels()\n",
    "            handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "            axs1.legend(handles, labels, ncol=2, labelspacing=1.5, columnspacing=8, bbox_to_anchor=(0.75, 1.05), loc=\"lower center\", framealpha=0, prop=legends_on)\n",
    "\n",
    "        # Settings common to both cases\n",
    "        axs1.set_xlim(new_realistic_x_axis[overnight[0]]-0.5, new_realistic_x_axis[overnight[-1]] + 0.25)\n",
    "        plt.setp(axs1, xticks=np.arange(0, round(new_realistic_x_axis[overnight[-1]]), 1))\n",
    "        plt.setp(axs1.yaxis.get_majorticklabels(), fontproperties=titles_and_axes_on)\n",
    "        plt.setp(axs1.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "        axs1.tick_params(axis='both', labelright='off', length=25, width=3.5, pad=12)\n",
    "        axs1.spines['left'].set_linewidth(3.5)\n",
    "        axs1.spines['bottom'].set_linewidth(3.5)\n",
    "        axs1.spines['top'].set_visible(False)\n",
    "        axs1.spines['right'].set_visible(False)\n",
    "        axs1.yaxis.tick_left()\n",
    "        axs1.grid(False)\n",
    "        if name[0] == 'n':\n",
    "            name = re.sub('^n', 'Normalized ', name) ### use '^n' to replace the first lowercase n by 'Normalized',there is no length limit unlike for excel sheet names\n",
    "            name = re.sub(r' \\([^()]*\\)', '', name) ### use ' (*' as a marker to delete units (all units in names are inside parentheses)\n",
    "        fig1.text(0.5, -0.02, 'Time (hours)', ha='center', fontproperties=titles_and_axes_on)\n",
    "        axs1.set_ylabel(name, labelpad=35, fontproperties=titles_and_axes_on)\n",
    "        # Set bottom ylimit to 0 if ylim[0] > 0\n",
    "        ymin, ymax = axs1.get_ylim()\n",
    "        if ymin > 0:\n",
    "            axs1.set_ylim(ymin=0, ymax=ymax*1.1)\n",
    "        else:\n",
    "            pass\n",
    "        axs1.annotate('', xy=(new_realistic_x_axis[[k for k in new_realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[0]),\n",
    "                      xytext=(new_realistic_x_axis[[k for k in new_realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[1]), rotation=0,\n",
    "                      ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                      arrowprops={\"color\" : \"purple\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "        axs1.text(new_realistic_x_axis[[k for k in new_realistic_x_axis.keys() if 'ac' in k][0]]+0.15, axs1.get_ylim()[1]*0.89,\n",
    "                  'DIV'+str(day_of_treatment), rotation=90, fontname='Arial', color='black', fontsize=40,\n",
    "                  ha='left', va='baseline') # if text separates more than 0.2 from the line is because of autoshrink\n",
    "        if post_on.size > 0:\n",
    "            axs2.annotate('', xy=(new_realistic_x_axis[post_on[0]], axs2.get_ylim()[0]),\n",
    "                          xytext=(new_realistic_x_axis[post_on[0]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"purple\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(new_realistic_x_axis[post_on[0]]+5, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[0].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "            axs2.annotate('', xy=(new_realistic_x_axis[post_on[-1]], axs2.get_ylim()[0]),\n",
    "                          xytext=(new_realistic_x_axis[post_on[-1]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"purple\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(new_realistic_x_axis[post_on[-1]]+5, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[-1].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "\n",
    "            if os.path.isdir('Figures') == False:\n",
    "                os.mkdir('Figures')\n",
    "            fig1.savefig('Figures\\\\' + name, edgecolor='none', transparent=False)\n",
    "\n",
    "            plt.show(close=None, block=None)\n",
    "\n",
    "    plt.close(\"all\") # Close all figures and do not display any inline after running the loop\n",
    "\n",
    "    # Move all .csv files to \"Sorted analysis files\" folder\n",
    "\n",
    "if os.path.isdir('Sorted analysis files') == False:\n",
    "    os.mkdir('Sorted analysis files')\n",
    "for fl in os.listdir():\n",
    "    if fl.endswith('.csv') or fl.endswith('.txt'):\n",
    "        shutil.move(os.path.join(path, fl), os.path.join(path, 'Sorted analysis files', fl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
