{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30b24c-9882-492e-9af8-69d824226db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import shutil\n",
    "import warnings\n",
    "import sys\n",
    "# import glob\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from scipy import stats\n",
    "# import scipy.io\n",
    "import csv\n",
    "import xlsxwriter\n",
    "from statistics import mode\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import openpyxl as xl\n",
    "from copy import copy\n",
    "# from copy import deepcopy\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# import statsmodels.api as sm\n",
    "# from bioinfokit.analys import stat\n",
    "# from statsmodels.formula.api import ols\n",
    "# import pingouin as pg\n",
    "# from statsmodels.graphics.factorplots import interaction_plot\n",
    "# from brokenaxes import brokenaxes\n",
    "# import matplotlib.patheffects as pe\n",
    "from matplotlib import container\n",
    "# import concurrent.futures\n",
    "# from itertools import starmap\n",
    "# import multiprocessing as mp\n",
    "# from brokenaxes import brokenaxes\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "import tkinter as tk\n",
    "import tkinter.filedialog as fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b8ef8-00ad-4735-a59e-412b4a9fe018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide these warnings\n",
    "warnings.filterwarnings(action='ignore', message='.*converting a masked element to nan.*')\n",
    "\n",
    "# Some plotting presets\n",
    "# plt.rc(\"errorbar\", capsize=3)\n",
    "plt.rc(\"figure\", dpi=70)\n",
    "plt.rc(\"savefig\", dpi=30, facecolor=\"white\", bbox=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d0501-d26b-4d93-8cec-f00a1605a1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print python version\n",
    "python_version = sys.version\n",
    "print(python_version)\n",
    "\n",
    "## Choose analysis files\n",
    "initial_dir = \"C:\\\\\"\n",
    "print(\"Please select the files to analyze:\")\n",
    "\n",
    "time.sleep(0.25)\n",
    "root = tk.Tk()\n",
    "root.filenames = fd.askopenfilenames(initialdir=initial_dir,\n",
    "                                     title=\"Please select the files to analyze\",\n",
    "                                     filetypes=((\"csv files\", \"*.csv\"), (\"all files\", \"*.*\")))\n",
    "input_files = [os.path.basename(f) for f in root.filenames]\n",
    "input_dir = os.path.dirname(root.filenames[0])\n",
    "root.destroy()\n",
    "\n",
    "print(\"Please choose the output folder:\")\n",
    "time.sleep(0.25)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.filenames = fd.askdirectory(initialdir=input_dir,\n",
    "                                 title=\"Please select the output folder\")\n",
    "output_dir = root.filenames\n",
    "root.destroy()\n",
    "\n",
    "os.chdir(input_dir)\n",
    "print('Working directory is:', input_dir, '\\n')\n",
    "print('Output folder is:', output_dir,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e357fef-f2e8-4205-ac5b-ebe9d3dfeb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Couple each file with its date of creation\n",
    "messydatesdict = OrderedDict()\n",
    "for fl in input_files:\n",
    "    if '_spike_' not in fl:\n",
    "        with open(fl, encoding='UTF-8') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            row = list(csv_reader)\n",
    "            for lin in row[75:76]:\n",
    "                if any('Analysis Duration (s):' in e.strip() for e in lin):\n",
    "                    duration = round(float(lin[1])/2)\n",
    "            for lin in row[5:7]:\n",
    "                if any('Original File Time' in e.strip() for e in lin):\n",
    "                    stamp = lin[1].split(' ')[0]\n",
    "                    clock = lin[1].split(' ')[1]\n",
    "                    year = int(stamp.split('/')[2])\n",
    "                    month = int(stamp.split('/')[0])\n",
    "                    day = int(stamp.split('/')[1])\n",
    "                    hour = int(clock.split(':')[0])\n",
    "                    minute = int(clock.split(':')[1])\n",
    "                    second = int(clock.split(':')[2])\n",
    "                    messydatesdict[fl] = datetime(year,month,day,hour,minute,second) + timedelta(seconds=duration)\n",
    "\n",
    "# Sort file names by their date of creation\n",
    "messydatesdict = dict(sorted(messydatesdict.items(), key=lambda x: datetime.strptime(str(x[1]), '%Y-%m-%d %H:%M:%S')))\n",
    "for k, v in messydatesdict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed99fc-7f1c-4106-9dc6-15a71a23a336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean file names\n",
    "cleandatesdict = OrderedDict()\n",
    "datesforaveragemerging = OrderedDict()\n",
    "for key, value in messydatesdict.items():\n",
    "    new_key = key.split('.')[0].replace('(000)(000)', '').replace('(000)', '').replace('(00', ' ').replace('(0', ' ').replace(')', '')\n",
    "    cleandatesdict[new_key] = str(value)\n",
    "    datesforaveragemerging[new_key] = value\n",
    "acutedatesdict = {}\n",
    "for k, v in cleandatesdict.items():\n",
    "    if 'bl' in k or 'ac' in k:\n",
    "        acutedatesdict[k] = v \n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe7015-f61e-45cf-93b3-e9799e20b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# append desired parameters' data to dataList from the advanced metrics files\n",
    "rowsChulas = np.r_[67,69,71,72,123:166 + 1] - 1\n",
    "dataList = []\n",
    "duration_in_sec = False\n",
    "durations = []\n",
    "for fl in messydatesdict.keys():\n",
    "    with open(fl, encoding='UTF-8') as csv_file:\n",
    "        temporary = csv.reader(csv_file, delimiter=',')\n",
    "        for i, row_content in enumerate(temporary):\n",
    "            if 'Analysis Duration (s): ' in row_content:\n",
    "                durations.append(float(row_content[1]))\n",
    "print(durations)\n",
    "for fl in messydatesdict.keys():\n",
    "#     if fl.split('.')[-1] == 'csv' and '_' not in fl:            \n",
    "    with open(fl, encoding='UTF-8') as csv_file:\n",
    "        singleCSV = []\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for i, row_content in enumerate(csv_reader):\n",
    "            if i == 67:\n",
    "                number_of_wells = len(row_content[1:])\n",
    "            if 'Analysis Duration (s): ' in row_content:\n",
    "                duration_in_sec = float(row_content[1])\n",
    "            if i in rowsChulas:\n",
    "                if (row_content[0] == 'Number of Spikes' or row_content[0] == 'Number of Bursts' or\n",
    "                row_content[0] == 'Number of Network Bursts' or row_content[0] == 'Inter-Burst Interval - Avg (s)' or\n",
    "                row_content[0] == 'Inter-Burst Interval - Std (s)'):\n",
    "                    if duration_in_sec >= mode(durations):\n",
    "                        # pseudo-normalize by time span of the recording\n",
    "                        singleCSV.append([round(float(x) / (duration_in_sec/mode(durations))) if x != '' else '' for x in row_content[1:]])\n",
    "                        singleCSV[-1].insert(0, row_content[0])\n",
    "                    else:\n",
    "                        singleCSV.append(row_content)\n",
    "                elif row_content[0] == 'Mean Firing Rate (Hz)':\n",
    "                    if number_of_wells == 24:\n",
    "                        singleCSV.append([str(float(a)*16) for a in row_content[1:]])\n",
    "                    elif number_of_wells == 6:\n",
    "                        singleCSV.append([str(float(a)*64) for a in row_content[1:]])\n",
    "                    singleCSV[5].insert(0, 'Mean Firing Rate (Hz) - well')\n",
    "                    singleCSV.append(row_content)\n",
    "                    singleCSV[6][0] = 'Mean Firing Rate (Hz) - elec'\n",
    "                else:\n",
    "                    singleCSV.append(row_content)\n",
    "        dataList.append(singleCSV)\n",
    "for i, row_content in enumerate(dataList):\n",
    "    dataList[i][2][1:] = [j + ' ' + i if j != '' else i for i, j in zip(dataList[i][2][1:], dataList[i][3][1:])]\n",
    "'''\n",
    "In the future extract the concentration of the second compound used in the well from the name too\\n\n",
    "(use \"after the + symbol\" regular expression to find it) and add it to the \"concentration\" row in dataList\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e12d9-916d-493c-8b3a-0ab657c1b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually define a dictionary of shortened parameter names for the excel sheet names\n",
    "\n",
    "excel_friendly = {\n",
    "                  'Well':'Well',\n",
    "                  'Well Coloring':'Well Coloring',\n",
    "                  'Treatment':'Treatment',\n",
    "                  'Concentration':'Concentration',\n",
    "                  'Mean Firing Rate (Hz) - well':'Mean_Firing_Rate_Hz_well',\n",
    "                  'Mean Firing Rate (Hz) - elec':'Mean_Firing_Rate_Hz_elec',\n",
    "                  'Number of Spikes':'Number_of_Spikes', # get normalization and acute treatment values from average for all 11 minutes periods\n",
    "                  'Number of Active Electrodes':'Number_of_Active_Electrodes',\n",
    "                  'Weighted Mean Firing Rate (Hz)':'Weighted_Mean_Firing_Rate_Hz',\n",
    "                  'ISI Coefficient of Variation - Avg':'ISI_Coeff_of_Variation_Avg',\n",
    "                  'Number of Bursts':'Number_of_Bursts', # get normalization and acute treatment values from average for all 11 minutes periods\n",
    "                  'Number of Bursting Electrodes':'Number_of_Bursting_Electrodes',\n",
    "                  'Burst Duration - Avg (s)':'Burst_Duration_Avg_s',\n",
    "                  'Burst Duration - Std (s)':'Burst_Duration_Std_s',\n",
    "                  'Number of Spikes per Burst - Avg':'Spikes_per_Burst_Avg',\n",
    "                  'Number of Spikes per Burst - Std':'Spikes_per_Burst_Std',\n",
    "                  'Mean ISI within Burst - Avg':'Mean_ISI_within_Burst_Avg',\n",
    "                  'Mean ISI within Burst - Std':'Mean_ISI_within_Burst_Std',\n",
    "                  'Median ISI within Burst - Avg':'Median_ISI_within_Burst_Avg',\n",
    "                  'Median ISI within Burst - Std':'Median_ISI_within_Burst_Std',\n",
    "                  'Inter-Burst Interval - Avg (s)':'Inter_Burst_Interval_Avg_s',\n",
    "                  'Inter-Burst Interval - Std (s)':'Inter_Burst_Interval_Std_s',\n",
    "                  'Burst Frequency - Avg (Hz)':'Burst_Frequency_Avg_Hz',\n",
    "                  'Burst Frequency - Std (Hz)':'Burst_Frequency_Std_Hz',\n",
    "                  'Normalized Duration IQR - Avg':'Normalized_Duration_IQR_Avg',\n",
    "                  'Normalized Duration IQR - Std':'Normalized_Duration_IQR_Std',\n",
    "                  'IBI Coefficient of Variation - Avg':'IBI_Coeff_of_Variation_Avg',\n",
    "                  'IBI Coefficient of Variation - Std':'IBI_Coeff_of_Variation_Std',\n",
    "                  'Burst Percentage - Avg':'Burst_Percentage_Avg',\n",
    "                  'Burst Percentage - Std':'Burst_Percentage_Std',\n",
    "                  'Number of Network Bursts':'Number_of_Network_Bursts', # get normalization and acute treatment values from average for all 11 minutes periods\n",
    "                  'Network Burst Frequency (Hz)':'Network_Burst_Frequency_Hz',\n",
    "                  'Network Burst Duration - Avg (sec)':'Network_Burst_Duration_Avg_s',\n",
    "                  'Network Burst Duration - Std (sec)':'Network_Burst_Duration_Std_s',\n",
    "                  'Number of Spikes per Network Burst - Avg':'Spikes_per_Netw_Burst_Avg',\n",
    "                  'Number of Spikes per Network Burst - Std':'Spikes_per_Netw_Burst_Std',\n",
    "                  'Number of Elecs Participating in Burst - Avg':'Elecs_Particip_in_Brst_Avg',\n",
    "                  'Number of Elecs Participating in Burst - Std':'Elecs_Particip_in_Brst_Std',\n",
    "                  'Number of Spikes per Network Burst per Channel - Avg':'Spikes_Net_Brst_Channel_Avg',\n",
    "                  'Number of Spikes per Network Burst per Channel - Std':'Spikes_Net_Brst_Channel_Std',\n",
    "                  'Network Burst Percentage':'Network_Burst_Percentage',\n",
    "                  'Network IBI Coefficient of Variation':'Network_IBI_Coeff_Variation',\n",
    "                  'Network ISI Coefficient of Variation':'Network_ISI_Coeff_Variation',\n",
    "                  'Network Normalized Duration IQR':'Network_Norm_Duration_IQR',\n",
    "                  'Area Under Normalized Cross-Correlation':'Area_Under_Norm_Cross_Corr',\n",
    "                  'Area Under Cross-Correlation':'Area_Under_Cross_Correlation',\n",
    "                  'Width at Half Height of Normalized Cross-Correlation':'Width_Hf_Height_N_Cross_Corr',\n",
    "                  'Width at Half Height of Cross-Correlation':'Width_Hf_Height_Cross_Corr',\n",
    "                  'Synchrony Index':'Synchrony_Index'\n",
    "                 }\n",
    "\n",
    "# Check all names are <=31 chars long\n",
    "for i in excel_friendly.values():\n",
    "    if len(i) > 31:\n",
    "        print(i+ ' is too long of an excel worksheet name')\n",
    "print('Number of advanced metrics parameters =', len(excel_friendly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc5a3d-726b-4e54-89a9-2d02b8539130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign each row in dataList to its day in cleandatesdict, then load it into the \"data\" dictionary\n",
    "data = defaultdict(dict)\n",
    "\n",
    "for day, ds in zip(cleandatesdict.keys(), dataList):\n",
    "    for row in ds:\n",
    "        key = excel_friendly[row[0]]\n",
    "        data[day][key] = row[1:]\n",
    "\n",
    "for day in cleandatesdict.keys():\n",
    "    keys = list(data[day].keys())\n",
    "    vals = list(data[day].values())\n",
    "    if any(\"bl\" in k.lower() for k in data.keys()) or any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "        idx = np.argsort(data[day]['Treatment'])\n",
    "    else:\n",
    "        idx = np.argsort(data[day]['Well Coloring'])\n",
    "print('\\nIndex of sorted well treatments is: ', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebc7d1-375d-4e6d-bdee-9fd55797d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first time point that received a treatment and the last that did not, and save their names as \"baseline\" and \"acute\", respectively\n",
    "# Use when there is only one baseline and one acute time point\n",
    "baseline = False\n",
    "day_of_treatment = False\n",
    "if any(\"bl\" in k.lower() for k in data.keys()) or any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "    for i, day in enumerate(data):\n",
    "        for hexcolor in data[day]['Well Coloring']:\n",
    "            if hexcolor != '#00FF00' and 'bl' in day:\n",
    "                baseline = day\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "if any(\"ac\" in k.lower() for k in data.keys()):\n",
    "    acute = [k for k in list(cleandatesdict.keys()) if 'ac' in k and 'ac(' not in k][0]\n",
    "    day_of_treatment = acute.split(' ')[1].replace('div','')\n",
    "    print('Acute treatment administered in:', acute)\n",
    "print('Baseline is:', baseline, '\\nDay of treatment is:', day_of_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a44208-bd7d-4f8b-94d2-8f9ee023fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative time from each time point to the acute treatment (time 0) if present\n",
    "def_labels = {}\n",
    "realistic_x_axis = {}\n",
    "if any(\"ac\" in k.lower() for k in data.keys()):\n",
    "    act = datetime.strptime(cleandatesdict[acute], '%Y-%m-%d %H:%M:%S')\n",
    "else:\n",
    "    act = datetime.strptime(list(cleandatesdict.values())[0], '%Y-%m-%d %H:%M:%S')\n",
    "for key in cleandatesdict.keys():\n",
    "    now = datetime.strptime(cleandatesdict[key], '%Y-%m-%d %H:%M:%S')\n",
    "    diff = now - act\n",
    "    hours = diff.days*24 + diff.seconds/3600\n",
    "    minutes = abs(hours*60) % 60 # abs not working the same for pos and neg values, that is why abs(hours). No negative sign in the pre-acute time points for this reason\n",
    "    seconds = abs(hours*3600) % 60\n",
    "    def_labels[key] = [\"{}: {}\".format(key, \"%d%s%02d%s%02d%s\" % (hours,'h ',minutes,'min ',seconds,'sec'))]\n",
    "    realistic_x_axis[key] = hours\n",
    "\n",
    "print('def_labels:\\n')\n",
    "for k, v in def_labels.items():\n",
    "    print(k+':\\t', \",\".join(v))\n",
    "\n",
    "print('\\nrealistic_x_axis:\\n')\n",
    "for k, v in realistic_x_axis.items():\n",
    "    print(k+':\\t', v, 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bd620-a901-46ef-97a6-65949fc963f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the average of all baseline timepoints and append baseline values for normalization to norms dictionary if there is at least one 'bl' item in \"data\".\n",
    "if any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "    baselines = [key for key in data.keys() if 'pretreatment' in key.lower()]\n",
    "else:\n",
    "    baselines = [key for key in data.keys() if 'bl' in key.lower()]\n",
    "for element in baselines: # This is the place to do the averaging of all the baseline time points\n",
    "    norms = dict()\n",
    "    for parameter, quantity in data[element].items(): \n",
    "        if parameter not in ['Well', 'Well Coloring', 'Treatment', 'Concentration']:\n",
    "            norms[parameter] = [] # As it is written now, \"norms\" will contain the data from the last baseline timepoint (usually bl(011) in a batch of 12)\n",
    "            for val in quantity:\n",
    "                try:\n",
    "                    norms[parameter].append(float(val))\n",
    "                except ValueError:\n",
    "                    norms[parameter].append(None)\n",
    "\n",
    "# Then calculate the average of all acute treatment timepoints\n",
    "acutes = [key for key in data.keys() if 'ac' in key.lower()]\n",
    "# average and save as a unique \"ac\" point\n",
    "baselines, acutes, day_of_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6213e3-ab96-441c-9190-3ae202a4dc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create excel workbook\n",
    "excelName = os.path.join(output_dir, 'Tables_well_single.xlsx')\n",
    "with xlsxwriter.Workbook(excelName, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(total=len(data.values()), desc=\"Writing advanced metrics workbook rows on \" + str(time.ctime(time.time()))) as pbar:\n",
    "        styles = dict()\n",
    "        params = data[list(cleandatesdict.keys())[0]]\n",
    "        sheets_names = []\n",
    "        for key in params.keys():\n",
    "            if key not in ['Well', 'Well Coloring', 'Treatment', 'Concentration']:\n",
    "                sheets_names.append(key)\n",
    "        for name in sheets_names:\n",
    "            workbook.add_worksheet(name)\n",
    "            if any(\"bl\" in k.lower() for k in data.keys()) or any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "                workbook.add_worksheet(name='n{}'.format(name))\n",
    "        sheets = [sh for sh in workbook.worksheets() if sh.name in sheets_names]\n",
    "        if any(\"bl\" in k.lower() for k in data.keys()) or any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "            norm_sheets = [sh for sh in workbook.worksheets() if sh.name not in sheets_names]\n",
    "        for i, day_data in enumerate(data.values()):\n",
    "            pbar.update(1)\n",
    "            for color, treat in zip(day_data['Well Coloring'], day_data['Treatment']):\n",
    "                if not color in styles:\n",
    "                    styles[color] = workbook.add_format({'bg_color': color})\n",
    "                    styles[''] = workbook.add_format({'bg_color': '#00FF00'})#define not-treated condition and color\n",
    "            for sh in sheets:\n",
    "                sh.write(i + 2, 0, list(cleandatesdict.keys())[i])\n",
    "                sh.write(1, 0, 'Treatment')\n",
    "                sh.write(0, 0, 'Well')\n",
    "                for j in range(len(day_data['Well'])):\n",
    "                    sh.write(0, j + 1, day_data['Well'][idx[j]])\n",
    "                    treatment = day_data['Treatment'][idx[j]]\n",
    "                    sh.write(1, j + 1, treatment)\n",
    "                    raw = day_data[sh.name][idx[j]]\n",
    "                    try:\n",
    "                        val = float(raw)\n",
    "                        if val == 0:\n",
    "                            val = 0\n",
    "                    except ValueError as e:\n",
    "                        # print(str(e).replace(':', ' in') + \" {}, {}, {}\".format(list(cleandatesdict.keys())[i],\n",
    "                        #                                     sh.name,\n",
    "                        #                                     day_data['Well'][idx[j]]))\n",
    "                        val = 0\n",
    "                    sh.write(i + 2, j + 1, val, styles[day_data['Well Coloring'][idx[j]]])\n",
    "            if any(\"bl\" in k.lower() for k in data.keys()) or any(\"pretreatment\" in k.lower() for k in data.keys()):\n",
    "                for sh in norm_sheets:\n",
    "                    sh.write(i + 2, 0, list(cleandatesdict.keys())[i])\n",
    "                    sh.write(1, 0, 'Treatment')\n",
    "                    sh.write(0, 0, 'Well')\n",
    "                    for j in range(len(day_data['Well'])):\n",
    "                        sh.write(0, j + 1, day_data['Well'][idx[j]])\n",
    "                        treatment = day_data['Treatment'][idx[j]]\n",
    "                        sh.write(1, j + 1, treatment)\n",
    "                        raw = day_data[sh.name[1:]][idx[j]]\n",
    "                        try:\n",
    "                            if \"pretreatment\" in baselines[0].lower() and 'bl' not in list(cleandatesdict.keys())[i]:\n",
    "                                val = float(raw) / norms[sh.name[1:]][idx[j]]\n",
    "                            elif \"pretreatment\" in baselines[0].lower() and 'bl' in list(cleandatesdict.keys())[i]:\n",
    "                                val = float(raw)\n",
    "                            else: # If no pretreatment present\n",
    "                                val = float(raw) / norms[sh.name[1:]][idx[j]]                            \n",
    "                            if val == 0 or val > 4:\n",
    "                                val = 0\n",
    "                        except ValueError:\n",
    "                            val = 0\n",
    "                        except TypeError:\n",
    "                            val = 0\n",
    "                        except ZeroDivisionError: # if a well has active electrodes\n",
    "    #                          in the baseline, a ZeroDivisioError will prompt when\n",
    "    #                          trying to normalize (will try to divide float by 0)\n",
    "    #                          print(\"{} {} {} {}\".format('Cannot divide',str(raw),'by',str(norms[sh.name[1:]][idx[j]])))\n",
    "                            val = 0\n",
    "                        sh.write(i + 2, j + 1, val, styles[day_data['Well Coloring'][idx[j]]])\n",
    "print('Created', excelName)\n",
    "# \"\"\"A FileNotFound error prompting here means the name is too long for an excel workbook and it could not be initialized\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47701f8f-f7d8-4dc2-98bd-004b9dde89a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Now working on the spike amplitude files...'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfd673-d4a7-4de9-b123-b3b1513a0e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Couple each spike_list file with its creation date. messydatesdict needs to be a dict in order to sort the file names by date\n",
    "messydatesdict = {}\n",
    "for fl in tqdm_notebook([f for f in input_files if '_spike_list' in f],\n",
    "                        desc = \"Retrieving timestamp from spike_list files on \" + str(time.ctime(time.time()))):\n",
    "        with open(fl) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file) # execution time in this step is limited by disk data transfer speed,\n",
    "            # specially by the size of the bl and ac spike_list files\n",
    "            rows = list(csv_reader)\n",
    "            for lin in rows[3:6]:\n",
    "                if lin[0] == '   Original File Time':\n",
    "                    messydatesdict[fl]= lin[1]\n",
    "for i in list(messydatesdict.items())[:6]:\n",
    "    print(i)\n",
    "print('\\netc...')\n",
    "\n",
    "# Sort full file names by date of creation\n",
    "filenames = []\n",
    "for n, d in sorted(messydatesdict.items(), key=lambda x: datetime.strptime(x[1], '%m/%d/%Y %H:%M:%S')):\n",
    "    filenames.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2933fd8-f1cc-41bb-8a08-78c00de3b710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create data structures\n",
    "nombresListAmps = []\n",
    "ampdata = dict()\n",
    "well_info = dict()\n",
    "\n",
    "# add ampdata to structures\n",
    "for fname in tqdm_notebook(filenames, desc = \"Coupling amplitude values with their day, electrode, treatment and treatment concentration on \" + str(time.ctime(time.time()))):\n",
    "    # append short file names to \"nombresListAmps\"\n",
    "    if 'on' in fname and not bool(re.search('\\(00[1-9]\\)|\\([0-9][1-9][0-9]\\)|\\([1-9][0-9][0-9]\\)', fname)):\n",
    "        day = fname.split('.')[0].replace('(000)', ' 0').replace('_spike_list', '')\n",
    "    else:    \n",
    "        day = fname.split('.')[0].replace('(000)', '').replace('_spike_list', '').replace('(00', ' ').replace('(0', ' ').replace(')', '')\n",
    "    print(day)\n",
    "    nombresListAmps.append(day)\n",
    "    with open(fname, encoding='UTF-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',') # execution time in this step is limited by disk data transfer speed,\n",
    "            # specially by the size of the bl and ac spike_list files\n",
    "        ampdata[day] = dict()\n",
    "        ampdata[day]['Electrode'] = []\n",
    "        ampdata[day]['Amplitude'] = []\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i > 0:\n",
    "                try:\n",
    "                    ampdata[day]['Electrode'].append(row[3]) # electrode data will be useful in the future\n",
    "                    ampdata[day]['Amplitude'].append(float(row[4]))\n",
    "                except IndexError:\n",
    "                    break\n",
    "\n",
    "        wells, coloring, treatment, concentration = None, None, None, None # well info is at the bottom of the table, below the last amplitude values\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            try:\n",
    "                if row[0] == 'Well':\n",
    "                    wells = row[1:]\n",
    "                if row[0] == 'Well Coloring':\n",
    "                    coloring = row[1:]\n",
    "                if row[0] == 'Treatment':\n",
    "                    treatment = row[1:]\n",
    "                if row[0] == 'Concentration':\n",
    "                    concentration = row[1:]\n",
    "            except IndexError:\n",
    "                break\n",
    "                \n",
    "        for i, row in enumerate(csv_reader): # try again, necessary in files with very few amplitude values (headers are longer than values list)\n",
    "            try:\n",
    "                if row[0] == 'Well':\n",
    "                    wells = row[1:]\n",
    "                if row[0] == 'Well Coloring':\n",
    "                    coloring = row[1:]\n",
    "                if row[0] == 'Treatment':\n",
    "                    treatment = row[1:]\n",
    "                if row[0] == 'Concentration':\n",
    "                    concentration = row[1:]\n",
    "            except IndexError:\n",
    "                break\n",
    "        well_info[day] = dict(zip(wells, zip(coloring, treatment, concentration)))\n",
    "print('\\nCreated \"ampdata\" and \"Well_info\" dictionaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06d343-6c7f-4168-8751-10cd001fdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting amplitude values from mV to μV and assigning them to their well of origin\n",
    "well_data = dict()\n",
    "for day, amp_day_data in tqdm_notebook(ampdata.items(), desc = \"Converting each day\\'s amplitude values to μV and assigning them to each well on \" + str(time.ctime(time.time()))):\n",
    "    well_data[day] = defaultdict(list, {w:[] for w in wells})\n",
    "    for el, amp in zip(amp_day_data['Electrode'], amp_day_data['Amplitude']):\n",
    "        well_data[day][el[:2]].append(amp*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d31b0-90aa-40bd-a8e8-f592018208d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting columns by treatment and concentration\n",
    "if any(\"ac\" in k.lower() for k in well_data.keys()): # if acute treatment is present in days list\n",
    "    treatments = np.array([(w, conc+' '+t) if conc != '' else (w, t) for w, (color, t, conc) in well_info[nombresListAmps[-1]].items()], dtype=str).T\n",
    "    idx = np.argsort(treatments)[1]\n",
    "else:\n",
    "    print('This experiment does not have an acute treatment point, must be a development experiment or badly named')\n",
    "    treatments = np.array([(w, color) for w, (color, t, conc) in well_info[list(well_info.items())[0][0]].items()], dtype=str).T\n",
    "    idx = np.argsort(treatments)[1]\n",
    "\n",
    "print('\\nEach well\\'s treatment is: ',  treatments[1])\n",
    "print('\\nIndex of sorted well treatments is: ', idx)\n",
    "print('\\nWell treatments sorted by final table arrangement: ', treatments[1][idx])\n",
    "\n",
    "col_numbers = dict(zip(treatments[0][idx], range(len(idx))))\n",
    "print('Sorted columns by treatment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ba046-a3d5-4369-bb2a-134eb2bb1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define excel workbook name and directory\n",
    "excelNameAmps = os.path.join(output_dir, 'amp.xlsx')\n",
    "# Create excel workbook\n",
    "start = time.time()\n",
    "with xlsxwriter.Workbook(excelNameAmps, options= {'nan_inf_to_errors': True}) as workbook:\n",
    "    with tqdm_notebook(ncols=800, desc=\"Writing amplitude cells to workbook on \" + str(time.ctime(time.time()))) as pbar:\n",
    "    # Write raw worksheets\n",
    "        sh = workbook.add_worksheet('Mean_Amplitude_μV')\n",
    "        sh.write(1, 0, 'Treatment')\n",
    "        sh.write(0, 0, 'Well')\n",
    "        styles = dict()\n",
    "        for well, (color, treat, concentration) in well_info[nombresListAmps[-1]].items(): # pick treatments and colors from last time point\n",
    "            styles[color] = workbook.add_format({'bg_color': color})\n",
    "        styles[''] = workbook.add_format({'bg_color': '#00FF00'}) # define not-treated condition and color\n",
    "        for well, j in col_numbers.items():\n",
    "            sh.write(0, j + 1, well)\n",
    "            t,c = well_info[nombresListAmps[-1]][well][1:]\n",
    "            sh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "        for i, amp_day_data in enumerate(well_data.values()):\n",
    "            sh.write(i + 2, 0, nombresListAmps[i])\n",
    "            for well, amps in amp_day_data.items():\n",
    "                pbar.update(0.5)      # execution time in this step is limited by disk data transfer speed,\n",
    "                                      # specially the size of the 'bl', 'pretreatment' and 'ac' spike_list files\n",
    "                if amps != []:\n",
    "                    sh.write(i + 2, col_numbers[well] + 1, np.nanmean(amps),\n",
    "                         styles[well_info[nombresListAmps[i]][well][0]])\n",
    "                else:\n",
    "                    sh.write(i + 2, col_numbers[well] + 1, 0,\n",
    "                         styles[well_info[nombresListAmps[i]][well][0]])\n",
    "    # Only write normalized worksheets if there is a baseline or pretreatment\n",
    "        if any(\"bl\" in k.lower() for k in well_data.keys()) or any(\"pretreatment\" in k.lower() for k in well_data.keys()):\n",
    "            nsh = workbook.add_worksheet('nMean_Amplitude_μV')\n",
    "            nsh.write(1, 0, 'Treatment')\n",
    "            nsh.write(0, 0, 'Well')\n",
    "            for well, j in col_numbers.items():\n",
    "                nsh.write(0, j + 1, well)\n",
    "                t,c = well_info[nombresListAmps[-1]][well][1:]\n",
    "                nsh.write(1, j + 1, c + ' ' + t if c != '' else t)\n",
    "            for i, amp_day_data in enumerate(well_data.values()):\n",
    "                nsh.write(i + 2, 0, nombresListAmps[i])\n",
    "                for well, amps in amp_day_data.items():\n",
    "                    pbar.update(0.5)\n",
    "                    # if numerator or denominator are NOT empty...   \n",
    "                    if amps != [] and well_data[baselines[0]][well] != []:\n",
    "                        if \"pretreatment\" in baselines[0].lower() and 'bl' in list(cleandatesdict.keys)[i]:\n",
    "                            nsh.write(i + 2, col_numbers[well] + 1, np.nanmean(amps),\n",
    "                                 styles[well_info[nombresListAmps[i]][well][0]])\n",
    "                        else:\n",
    "                            if np.nanmean(amps)/np.nanmean(well_data[baselines[0]][well]) > 4:\n",
    "                                nsh.write(i + 2, col_numbers[well] + 1, 0,\n",
    "                                    styles[well_info[nombresListAmps[i]][well][0]])\n",
    "                            else:\n",
    "                                nsh.write(i + 2, col_numbers[well] + 1, np.nanmean(amps)/np.nanmean(well_data[baselines[0]][well]),\n",
    "                                styles[well_info[nombresListAmps[i]][well][0]])\n",
    "\n",
    "                    # if numerator or denominator ARE empty...\n",
    "                    else:\n",
    "                        nsh.write(i + 2, col_numbers[well] + 1, 0,\n",
    "                             styles[well_info[nombresListAmps[i]][well][0]])\n",
    "end = time.time()\n",
    "print('Created \"amp.xlsx\" Workbook,',f\"Runtime of the program was {end - start} seconds\")\n",
    "## No RuntimeWarnings means no division of zero or by zero has been performed and excel file will be clean from formula errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0d7a1-f77a-490f-88f1-914cea57c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move mean amplitude.xlsx sheets to final Tables_well_single.xlsx\n",
    "\n",
    "wb1 = xl.load_workbook(filename=excelNameAmps)\n",
    "meanamps = wb1.worksheets[0]\n",
    "if any(\"bl\" in k.lower() for k in well_data.keys()) or any(\"pretreatment\" in k.lower() for k in well_data.keys()):\n",
    "    nmeanamps = wb1.worksheets[1]\n",
    "\n",
    "merged = xl.load_workbook(filename=excelName)\n",
    "copymeanamps = merged.create_sheet(meanamps.title, index=0)\n",
    "if any(\"bl\" in k.lower() for k in well_data.keys()) or any(\"pretreatment\" in k.lower() for k in well_data.keys()):\n",
    "    copyNmeanamps = merged.create_sheet(nmeanamps.title, index=1)\n",
    "\n",
    "with tqdm_notebook(ncols=800, desc=\"Moving amplitude cells to final excel \" + str(time.ctime(time.time()))) as pbar:\n",
    "    for row in meanamps:\n",
    "        for cell in row:\n",
    "            new_cell = copymeanamps.cell(row=cell.row, column=cell.col_idx, value= cell.value)\n",
    "            if cell.has_style:\n",
    "                new_cell.font = copy(cell.font)\n",
    "                new_cell.border = copy(cell.border)\n",
    "                new_cell.fill = copy(cell.fill)\n",
    "                new_cell.number_format = copy(cell.number_format)\n",
    "                new_cell.protection = copy(cell.protection)\n",
    "                new_cell.alignment = copy(cell.alignment)\n",
    "                pbar.update(0.5)\n",
    "                \n",
    "    if any(\"bl\" in k.lower() for k in well_data.keys()) or any(\"pretreatment\" in k.lower() for k in well_data.keys()):\n",
    "        for row in nmeanamps:\n",
    "            for cell in row:\n",
    "                new_cell = copyNmeanamps.cell(row=cell.row, column=cell.col_idx, value= cell.value)\n",
    "                if cell.has_style:\n",
    "                    new_cell.font = copy(cell.font)\n",
    "                    new_cell.border = copy(cell.border)\n",
    "                    new_cell.fill = copy(cell.fill)\n",
    "                    new_cell.number_format = copy(cell.number_format)\n",
    "                    new_cell.protection = copy(cell.protection)\n",
    "                    new_cell.alignment = copy(cell.alignment)\n",
    "                    pbar.update(0.5)\n",
    "for sheet in merged:\n",
    "    sheet.sheet_view.tabSelected = False\n",
    "merged.active = 0\n",
    "merged.save(excelName)\n",
    "os.remove(excelNameAmps)\n",
    "print('Moved amplitude worksheets to {} workbook and deleted {} workbook'.format(excelName, 'amps.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15b8b5-17ce-4cec-b2c7-a52eca80a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"\\n\\nPlease, check the Test.xlsx file in the output folder before proceeding with the plots. Press Enter to continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dcd1c-105f-4af8-9572-d013911d1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel file as pandas dataframe\n",
    "df = pd.read_excel(excelName, index_col=0, header=1, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6610a75-cad5-4744-b41a-84ab825ae91d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Match treatments with their color using the last time point's color-treatment key-value pair.\n",
    "# Check the result is correct and use next code cell after manually correcting the excel workbook info if not\n",
    "try:\n",
    "    co = {}\n",
    "    for color, treatm in zip(day_data['Well Coloring'], day_data['Treatment']):\n",
    "        co[treatm] = color\n",
    "    temp = {v : k for k, v in co.items()}\n",
    "    res = {v : k for k, v in temp.items()}\n",
    "    print(res,'\\n\\nNumber of treatments is: {}\\n'.format(len(res)))\n",
    "    \n",
    "# Solution for when the treatment names are wrong in the .csv files but correct in the DataFrame after correction\n",
    "# and the treatment-color pairs must be gotten from the dataframe (using 'Number of Spikes' sheet)\n",
    "except KeyError:\n",
    "    \n",
    "    tem = list(df['Number_of_Spikes'].columns)\n",
    "    col_headers = []\n",
    "    # ran = [format(x, '0') for x in range(0,10)]\n",
    "    for e in tem:\n",
    "        if e.count('.') > 1:\n",
    "            col_headers.append(e.split(' ', maxsplit=1)[0]+ ' ' + e.split(' ', maxsplit=1)[1].split('.')[0])\n",
    "        elif '.' not in e:\n",
    "            col_headers.append(e)\n",
    "        elif '.' in e[:3] and '.' not in e[-1:-3]:\n",
    "            col_headers.append(e)\n",
    "        else:\n",
    "            col_headers.append(e.split('.')[0])\n",
    "\n",
    "    print(col_headers)\n",
    "    wob = xl.load_workbook(excelName, data_only = True)\n",
    "    sh = wob['Number_of_Spikes']\n",
    "    hex_colors = []\n",
    "    for column in range(2, sh.max_column+1):\n",
    "        hex_colors.append(sh.cell(3,column).fill.start_color.index.replace('FF','#',1))\n",
    "    co = {}\n",
    "    for color, treatm in zip(hex_colors, col_headers):\n",
    "        co[treatm] = color\n",
    "    temp = {v : k for k, v in co.items()}\n",
    "    res = {v : k for k, v in temp.items()}\n",
    "    print(res)\n",
    "    print ('HEX =', hex_colors) \n",
    "    #print('RGB =', tuple(int(hex_colors[i:i+2], 16) for i in (0, 2, 4))) # Color in RGB\n",
    "\n",
    "# Create parameter data and SEM averages for each treatment directly from pandas dataframe content\n",
    "'''These calculations are also performed within the descriptive plots' loop'''\n",
    "parameters = dict()\n",
    "sem = dict()\n",
    "for metric_name, df_metric in tqdm_notebook(df.items(), desc = 'Creating \"parameters\" and \"sem\" data dictionaries on ' + str(time.ctime(time.time()))):\n",
    "    parameters[metric_name] = {k: [] for k in res}\n",
    "    sem[metric_name] = {k: [] for k in res}\n",
    "    for day in df_metric.index:\n",
    "        for trt in parameters[metric_name]:\n",
    "            vals_over_trt = np.array([df_metric.loc[day][key] for key in df_metric.keys() if key.startswith(trt)])\n",
    "            with np.errstate(invalid='ignore'):\n",
    "                parameters[metric_name][trt].append(np.nanmean(vals_over_trt))\n",
    "                sem[metric_name][trt].append(stats.sem(vals_over_trt, axis=0, ddof=1, nan_policy='omit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98f6fa-14d2-4b1b-bcc0-7f63f0a5c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a composition with all the charts using add_gridspec and add_subplot\n",
    "# from my_functions import statistics_chart, ideal_labels\n",
    "# statistics_chart(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa795cf-e4fc-47fa-98fd-c6013c348c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some plot presets\n",
    "\n",
    "titles_and_axes_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=48)\n",
    "legends_on = FontProperties(family='Arial', style='normal', weight=250, stretch='normal', size=50)\n",
    "\n",
    "devel = []\n",
    "overnight = []\n",
    "after = []\n",
    "post_on = []\n",
    "for k in realistic_x_axis.keys():\n",
    "    if 'bl' in k:\n",
    "        break\n",
    "    devel.append(k)\n",
    "\n",
    "last_overnight = None\n",
    "first_div = None\n",
    "for i, (k, v) in enumerate(realistic_x_axis.items()):\n",
    "    if 'bl' in k or 'ac' in k or 'on' in k or 'pretreatment' in k or 'Pretreatment' in k:\n",
    "        overnight.append(k)\n",
    "    else:\n",
    "        last_overnight = list(realistic_x_axis.values())[i-1]\n",
    "        first_div = v\n",
    "        break\n",
    "\n",
    "if first_div is not None and last_overnight is not None and abs(last_overnight - first_div) < 3:\n",
    "    # Sometimes, a separate 11 min segment is recorded shortly after overnight[-1]\n",
    "    overnight.append(df['Number_of_Spikes'].index.values[len(overnight)])\n",
    "devel = np.array(devel)\n",
    "after = df['Number_of_Spikes'].index.values[len(devel):]\n",
    "post_on = df['Number_of_Spikes'].index.values[len(devel) + len(overnight):]\n",
    "print('Stablished line plot presets on ' + str(time.ctime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7d42b-c362-4bec-8594-263f9cc3c7e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plots loop\n",
    "workbook = xl.load_workbook(excelName)\n",
    "prs = Presentation()\n",
    "blank_slide_layout = prs.slide_layouts[6]\n",
    "r=0\n",
    "with plt.ion():\n",
    "    for name, mean_metr in tqdm_notebook(parameters.items(), total=((len(parameters.items()))) , desc='Plotting and saving graphs on ' + str(time.ctime(time.time()))):\n",
    "        if post_on.size == 0:\n",
    "            custom_fig_size=(45, 15)\n",
    "            fig1, axs1 = plt.subplots(1, 1, figsize=custom_fig_size, gridspec_kw=dict(hspace=0, wspace=0), sharex='none', sharey=True)\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                if name[0] != 'n':\n",
    "                    x = [df[name].index.get_loc(ind) for ind in np.append(devel,overnight)]\n",
    "                elif name[0] == 'n' and any('pretreatment' in ind.lower() for ind in np.append(devel,overnight)):\n",
    "                    x = [df[name].index.get_loc(ind) for ind in np.append(devel,overnight) if 'bl' not in ind.lower()]\n",
    "                realistic_x = [list(realistic_x_axis.values())[i] for i in x]\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, markeredgewidth='2.5', markeredgecolor='black',\n",
    "                              markerfacecolor=res[trt], fmt = ' ', capsize=17, capthick=2, ecolor=res[trt])\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False),\n",
    "                              label=trt, elinewidth=6, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, capthick=6, markeredgecolor='black',\n",
    "                              markerfacecolor=res[trt], capsize=18.5, ecolor='black')\n",
    "                handles, labels = axs1.get_legend_handles_labels()\n",
    "                handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "                by_label = dict(zip(labels, handles))\n",
    "                axs1.legend(by_label.values(), by_label.keys(), ncol=len(res), labelspacing=1, columnspacing=2,\n",
    "                            bbox_to_anchor=(0.1, 1.05), loc=\"lower left\", framealpha=0, prop=legends_on)\n",
    "        else:\n",
    "            custom_fig_size=(40, 15)\n",
    "            fig1, (axs1,axs2) = plt.subplots(1, 2, figsize=custom_fig_size, gridspec_kw=dict({'width_ratios': [5, 1.5]}, hspace=0, wspace=0.03),\n",
    "                                             sharex='none', sharey=True, facecolor='w')\n",
    "            axs1.patch.set_alpha(0.0)\n",
    "            axs2.patch.set_alpha(0.0)\n",
    "            realistic_x = []\n",
    "            for trt, row in mean_metr.items():\n",
    "                if name[0] != 'n':\n",
    "                    x = [df[name].index.get_loc(ind) for ind in np.append(devel,after)]\n",
    "                elif name[0] == 'n' and any('pretreatment' in ind.lower() for ind in np.append(devel,after)):\n",
    "                    x = [df[name].index.get_loc(ind) for ind in np.append(devel,after) if 'bl' not in ind.lower()]\n",
    "                x2 = [df[name].index.get_loc(ind) for ind in post_on]\n",
    "                realistic_x = [list(realistic_x_axis.values())[i] for i in x]\n",
    "                realistic_x2 = [list(realistic_x_axis.values())[i] for i in x2]\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False), label=trt,\n",
    "                              elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, markeredgewidth='2.5', markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              fmt = ' ', capsize=18.4, capthick=1.2, ecolor=res[trt])\n",
    "                axs1.errorbar(realistic_x, np.nan_to_num(np.array(row)[x], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x], copy=False), label=trt, \n",
    "                              elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=18.8, capthick=2.6, ecolor='black')\n",
    "                axs2.errorbar(realistic_x2, np.nan_to_num(np.array(row)[x2], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x2], copy=False), label=trt,\n",
    "                              elinewidth=1.4, linewidth=2.25, color='black', marker='o', markersize='24', zorder=3,\n",
    "                              barsabove=False, markeredgewidth='2.5', markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              fmt = ' ', capsize=18.4, capthick=1.2, ecolor=res[trt])\n",
    "                axs2.errorbar(realistic_x2, np.nan_to_num(np.array(row)[x2], copy=False),\n",
    "                              yerr = np.nan_to_num(np.array(sem[name][trt])[x2], copy=False),\n",
    "                              label=trt, elinewidth=2.2, linewidth=2.25, color='black', marker='o', markersize='24', zorder=2,\n",
    "                              barsabove=False, markeredgecolor='black', markerfacecolor=res[trt],\n",
    "                              capsize=18.8, capthick=2.6, ecolor='black')\n",
    "\n",
    "            axs2.set_xlim(realistic_x_axis[post_on[0]]-15, realistic_x_axis[post_on[-1]] + 6)\n",
    "            plt.setp(axs2, xticks=np.arange(round(realistic_x_axis[post_on[0]]), round(realistic_x_axis[post_on[-1]] + 50), 50))\n",
    "            axs2.tick_params(axis='x', length=25, width=3.5, pad=12)\n",
    "            plt.setp(axs2.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "            axs2.spines['left'].set_visible(False)\n",
    "            axs2.spines['bottom'].set_linewidth(3.5)\n",
    "            axs2.spines['top'].set_visible(False)\n",
    "            axs2.spines['right'].set_visible(False)\n",
    "            axs2.yaxis.set_visible(False)\n",
    "            axs2.grid(False)\n",
    "            d = .025 # how big to make the diagonal lines in axes coordinates\n",
    "            kwargs = dict(transform=axs1.transAxes, color='k', clip_on=False, linewidth=3.5)\n",
    "            axs1.plot((1,1), (-d,+d), **kwargs)\n",
    "            kwargs.update(transform=axs2.transAxes)\n",
    "            axs2.plot((0,0), (-d,+d), **kwargs)\n",
    "            handles, labels = axs1.get_legend_handles_labels()\n",
    "            handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            axs1.legend(by_label.values(), by_label.keys(), ncol=len(res), labelspacing=1, columnspacing=2,\n",
    "                        bbox_to_anchor=(0.6, 1.1), loc=\"lower center\", framealpha=0, prop=legends_on)\n",
    "\n",
    "        # Settings common to both cases\n",
    "        if devel.size != 0:\n",
    "            axs1.set_xlim(realistic_x_axis[np.append(devel,overnight)[0]]-3, realistic_x_axis[np.append(devel,overnight)[-1]]+3)\n",
    "        else:\n",
    "            axs1.set_xlim(realistic_x_axis[np.append(devel,overnight)[0]]-0.5, realistic_x_axis[np.append(devel,overnight)[-1]]+0.4)\n",
    "        plt.setp(axs1, xticks=np.arange(0, round(realistic_x_axis[np.append(devel,overnight)[-1]]+1), 1))\n",
    "        plt.setp(axs1.yaxis.get_majorticklabels(), fontproperties=titles_and_axes_on)\n",
    "        plt.setp(axs1.xaxis.get_majorticklabels(), rotation=0, fontproperties=titles_and_axes_on)\n",
    "        axs1.tick_params(axis='both', labelright='off', length=25, width=3.5, pad=12)\n",
    "        axs1.spines['left'].set_linewidth(3.5)\n",
    "        axs1.spines['bottom'].set_linewidth(3.5)\n",
    "        axs1.spines['top'].set_visible(False)\n",
    "        axs1.spines['right'].set_visible(False)\n",
    "        axs1.yaxis.tick_left()\n",
    "        axs1.grid(False)\n",
    "        oldname = name\n",
    "        if name[0] == 'n':\n",
    "            name = re.sub('^n', 'Normalized ', name) # use '^n' to replace the first lowercase n by 'Normalized',there is no length limit unlike for excel sheet names\n",
    "            name = re.sub(r' \\([^()]*\\)', '', name) # use ' (*' as a marker to delete units (all units in names are inside parentheses)\n",
    "        fig1.text(0.5, -0.02, 'Time (hours)', ha='center', fontproperties=titles_and_axes_on)\n",
    "        axs1.set_ylabel(name, labelpad=35, fontproperties=titles_and_axes_on)\n",
    "        # Set bottom ylimit to 0 if ylim[0] > 0\n",
    "        ymin, ymax = axs1.get_ylim()\n",
    "        if ymin > 0:\n",
    "            axs1.set_ylim(ymin=0, ymax=ymax*1.1)\n",
    "        else:\n",
    "            pass\n",
    "        if any(\"ac\" in k.lower() for k in cleandatesdict.keys()):\n",
    "            axs1.annotate('', xy=(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[0]),\n",
    "                          xytext=(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]], axs1.get_ylim()[1]), rotation=0,\n",
    "                          ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs1.text(realistic_x_axis[[k for k in realistic_x_axis.keys() if 'ac' in k][0]]+0.15, axs1.get_ylim()[1]*0.89,\n",
    "                      'DIV'+str(day_of_treatment), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline') # if text separates more than 0.2 from the line is because of autoshrink\n",
    "        if post_on.size > 0:\n",
    "            axs2.annotate('', xy=(realistic_x_axis[post_on[0]], axs2.get_ylim()[0]),\n",
    "                          xytext=(realistic_x_axis[post_on[0]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(realistic_x_axis[post_on[0]]+5, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[0].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "            axs2.annotate('', xy=(realistic_x_axis[post_on[-1]], axs2.get_ylim()[0]),\n",
    "                          xytext=(realistic_x_axis[post_on[-1]], axs2.get_ylim()[1]), rotation=0,\n",
    "                          fontsize=37, fontname='Arial', color='black', ha='center', va='center', annotation_clip=False, zorder=1,\n",
    "                          arrowprops={\"color\" : \"black\", \"arrowstyle\" : \"-\", \"linestyle\" : \":\", \"linewidth\" : 2, \"shrinkA\": 0, \"shrinkB\": 0})\n",
    "\n",
    "            axs2.text(realistic_x_axis[post_on[-1]]+5, axs1.get_ylim()[1]*0.89,\n",
    "                      post_on[-1].upper(), rotation=90, fontname='Arial', color='black', fontsize=40, ha='left', va='baseline')\n",
    "\n",
    "        # Save the figures as .png to folder within the loop\n",
    "        if os.path.isdir(os.path.join(output_dir,'Graphs per well')) == False:\n",
    "            os.mkdir(os.path.join(output_dir,'Graphs per well'))\n",
    "        fig1.savefig(os.path.join(output_dir,'Graphs per well\\\\') + name, edgecolor='none', transparent=False)\n",
    "        \n",
    "        # Insert figures on the fly on their corresponding worksheet\n",
    "        sheet = workbook[oldname]\n",
    "        img = xl.drawing.image.Image(os.path.join(output_dir,'Graphs per well\\\\') + name + '.png')\n",
    "        img.width = 23*custom_fig_size[0]\n",
    "        img.height = 30*custom_fig_size[1]\n",
    "        sheet.add_image(img, '{}{}'.format('A', len(cleandatesdict.keys())+4))\n",
    "        \n",
    "        # Save figures to a .pptx within the loop\n",
    "        left=Inches(2.9)\n",
    "        top=Inches(0.05)\n",
    "        width=Inches(6.75)\n",
    "        img_path = os.path.join(output_dir,'Graphs per well\\\\') + name + '.png'\n",
    "        if r == 0:\n",
    "            slide = prs.slides.add_slide(blank_slide_layout)\n",
    "            pic = slide.shapes.add_picture(img_path, left, top, width)\n",
    "            txBox = slide.shapes.add_textbox(Inches(0.1), Inches(0.1), Inches(3), Inches(0.5))\n",
    "            tf = txBox.text_frame\n",
    "            tf.text = name\n",
    "            tf.paragraphs[0].font.bold = True\n",
    "            tf.paragraphs[0].font.size = Pt(20)\n",
    "            r=1\n",
    "        elif r==1:\n",
    "            top=Inches(3.75)\n",
    "            pic = slide.shapes.add_picture(img_path, left, top, width)\n",
    "            r=0\n",
    "        \n",
    "#         plt.show(close=None, block=None) # Inline plotting consumes many resources and hugely increases running time\n",
    "#         prs.save(foldername + '.pptx') # Save after each iteration so I can see the live evolution of the file\n",
    "\n",
    "plt.close('all') # Close all figures and do not display any inline after running the loop (inline plotting is time-consuming)\n",
    "prs.save(os.path.join(output_dir,'Graphs per well.pptx')) # Save pptx after the loop is done to save time\n",
    "workbook.save(excelName)\n",
    "\n",
    "# Move all .csv files to \"analysis files\" folder\n",
    "# if os.path.isdir('Unsorted analysis files') == False:\n",
    "#     os.mkdir('Unsorted analysis files')\n",
    "# for fl in input_files:\n",
    "#     if fl.endswith('.csv') or fl.endswith('.txt'):\n",
    "#         shutil.move(os.path.join(path, fl), os.path.join(path, 'Unsorted analysis files', fl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
